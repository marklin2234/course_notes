---
title: "STAT 341: Assignment 1"
subtitle: "DUE: Friday, January 26, 2024 by 5:00pm EST"
output:
  pdf_document:
    keep_tex: yes
    number_sections: no
  html_document:
    toc: yes
  word_document: default
urlcolor: blue
editor_options: 
  markdown: 
    wrap: 72
---

```{r setup, include = FALSE}
knitr::opts_chunk$set(echo = TRUE, warning = FALSE)
```

```{r, include = FALSE}
library(knitr)
opts_chunk$set(tidy.opts=list(width.cutoff=70),tidy=TRUE)
```

$\;$ $\;$ $\;$ $\;$

## NOTES

Your assignment must be submitted by the due date listed at the top of
this document, and it must be submitted electronically in .pdf format
via Crowdmark. This means that your responses for different questions
should begin on separate pages of your .pdf file. Note that your .pdf
solution file must have been generated by R Markdown. Additionally:

-   For mathematical questions: your solutions must be produced by LaTeX
    (from within R Markdown). Neither screenshots nor
    scanned/photographed handwritten solutions will be accepted -- these
    will receive zero points.

-   For computational questions: R code should always be included in
    your solution (via code chunks in R Markdown). If code is required
    and you provide none, you will receive zero points.

-   For interpretation questions: plain text (within R Markdown) is
    required. Text responses embedded as comments within code chunks
    will not be accepted.

Organization and comprehensibility is part of a full solution.
Consequently, points will be deducted for solutions that are not
organized and incomprehensible. Furthermore, if you submit your
assignment to Crowdmark, but you do so incorrectly in any way (e.g., you
upload your Question 2 solution in the Question 1 box), you will receive
a 5% deduction (i.e., 5% of the assignmentâ€™s point total will be
deducted from your point total).

\newpage

## QUESTION 1: Basic R Calculations [8 points]

The first lecture introduced `apply` functions as an alternative to
using `for` loops in R. Implement the following tasks in R by using an
appropriate `apply` function instead of a `for` loop. You may use *any*
functions in `base` R to complete these tasks.

(a) [2 points] Add 10 to each of the odd numbers in the following
    vector.

```{r}
part_a <- c(4, 9, 22, 25, 30, 34, 41, 53, 67, 98)

add_ten_to_odd <- function(num) {
  if (num %% 2 == 1) {
    return(num + 10)
  } else {
    return(num);
  }
}

ret <- apply(matrix(part_a), MARGIN=1, FUN=add_ten_to_odd)
```

(b) [2 points] Determine how many entries in each row of the following
    matrix begin with the lowercase letter j.

```{r}
part_b <- matrix(c(letters, tolower(month.name[1:10])), nrow = 6)

find_num_start_j <- function(row) {
  sum(substr(row, 1, 1) == "j")
}

ret <- apply(part_b, MARGIN=1, FUN=find_num_start_j)
```

Later in the course, you will use more sophisticated techniques to
minimize functions $g(x)$ or find solutions to the equation $g(x) = 0$.
For the next two parts, solve these types of questions by naively
evaluating $g(x)$ at various $x$ values. Get the `x` values for each
question using the `seq(..., by=dx)` function. Notes:

-   Using an extremely small `dx` will use a lot of computational
    resources, so press escape if a computation is taking too long and
    try something more moderate.
-   You can check your work with calculus. Your answers should be within
    0.001 of the correct answers.

(c) [2 points] Approximate the value of $x$ that minimizes
    $g(x) = x^4 + x^3 + 2x^2 + 3x + 4$ on $-2 \le x \le 2$. Return the
    $(x, g(x))$ combination for this approximate solution.

```{r}
g <- function(x) {
  return (x^4 + x^3 + 2*x^2+3*x+4)
}
x <- seq(from=-2,to=2,by=0.1)
ret <- sapply(x, g)
ans <- c(min(ret), x[which.min(ret)])
print(ans)
```

The approximate minimum (x, g(x)) is (-0.7, 2.7771)

(d) [2 points] Approximate the value of $x$ that solves
    $g(x) = \dfrac{x^2 + x - 1}{x^2 + 2} = 0$ on $0 \le x \le 4$. Return
    the $(x, g(x))$ combination for this approximate solution.

```{r}
g <- function(x) {
  return((x^2+x-1) / (x^2+2))
}

x <- seq(from=0,to=4,by=0.1)

ret <- sapply(x, g)

ans <- c(min(abs(ret)), x[which.min(abs(ret))])

# (0.618, 0)
print(ans)
```

The approximate solution is (0.6, 0.01694915)

\newpage

## QUESTION 2: Attributes with Kernel Density Estimators [11 points]

The beginning of this course emphasizes population attributes with a
single variate: $\mathcal{P} = \{y_1, y_2, \ldots, y_N\}$. We often
analyze this data under the assumption that $y_1, y_2, \ldots, y_N$ are
generated independently and identically from some univariate
distribution with density function $f$. [Kernel density
estimation](https://en.wikipedia.org/wiki/Kernel_density_estimation)
provides a flexible method to estimate this function $f$. Let's suppose
we want to estimate the value of $f$ at a given point $y_0$. The *kernel
density estimator* (KDE) is
$$\hat{f}_h(y_0) = \dfrac{1}{N}\sum_{i = 1}^NK_h(y_0 - y_i),$$ where the
non-negative function $K_h(\cdot)$ is called a *kernel* function that
depends on a smoothing parameter $h > 0$ called the *bandwidth*.
Essentially, the kernel function estimates $f$ by weighting
contributions corresponding to each population unit such that units with
variate values close to $y_0$ have larger $K_h(\cdot)$ values. We can
therefore think of the KDE as a population attribute:
$\hat{f}_h(y_0) = a(\mathcal{P}; y_0)$, where $a(\cdot)$ is a function
of the population variates and the fixed point $y_0$.

We now consider the *Gaussian* kernel. For the Gaussian kernel, the
function $K_h(\cdot)$ is the density function of a normal distribution
with mean 0 and standard deviation corresponding to the bandwidth $h$:
$$K_h(v) = \dfrac{1}{\sqrt{2\pi}h}\text{exp}\left(-\dfrac{1}{2}\left(\dfrac{v}{h}\right)^2\right).$$

The figure below illustrates how to compute the kernel density estimate
using a Gaussian kernel for a population of size 3:
$\{y_1 = 3.5, y_2 = 5.5, y_3 = 6\}$. The black curve visualizes the
kernel density estimate for $0 \le y_0 \le 10$. Each variate value is
depicted by a coloured point on the horizontal axis, and its
corresponding Gaussian kernel function with $h = 1$ is given by the
dotted curve of the same colour. As shown in the figure, $\hat{f}_1(4)$
is computed by taking the average of the vertical coordinates for the
three open circles.

Below, you will investigate the location, scale, and replication
properties of the KDE with a Gaussian kernel. To be clear, you will
consider how the KDE at a *fixed* point $y_0$ is impacted when the
population $\mathcal{P}$ is modified.

(a) [2 points] Determine whether the kernel density estimator
    $\hat{f}_h(y_0)$ with a Gaussian kernel is location invariant,
    location equivariant, or neither.

It is evident that the KDF with a Gaussian kernel is neither.
Modifying $v$ causes an exponential change.

$$
a(\mathcal P) = a(y_1, \dots,y_n) = \hat f_h(y_0) = \frac{1}{N}\sum_{i=1}^NK_h(y_0 - y_i)
$$
$$
a(\mathcal P + b) = a(y_1,\dots,y_n) = \hat f_h(y_0) = \frac{1}{N}\sum_{i=1}^NK_h(y_0 - (y_i + b))
$$
$$
= \frac{1}{N}\sum_{i=1}^N\frac{1}{\sqrt{2\pi}h}exp\Bigg(-\frac{1}{2}\Big(\frac{y_0 - y_i - b}{h}\Big)^2\Bigg) \\
= \frac{1}{N}\sum_{i=1}^N\frac{1}{\sqrt{2\pi}h}exp\Bigg(-\frac{1}{2}\Big(y\frac{y_0-y_i}{h}\Big)^2\Bigg)exp\Bigg(\frac{b}{2h}\Bigg)
$$
$$
=\frac{1}{N}\sum_{i=1}^NK_h(y_0-y_i)exp\Big(\frac{b}{2h}\Big) = \frac{1}{N}exp\Big(\frac{b}{2h}\Big)\sum_{i=1}^NK_{h}(y_0-y_i) \neq a(\mathcal P + b)
$$
(b) [2 points] Determine whether the kernel density estimator
    $\hat{f}_h(y_0)$ with a Gaussian kernel is scale invariant, scale
    equivariant, or neither.

The KDF with a Gaussian kernel is neither because $v$ is a factor in
the exponent of the KDF. Thus, the function reacts accordingly by
amplifying the exponent of the function, rather than scaling it by a
constant.
The proof follows from part a.

(c) [2 points] Determine whether the kernel density estimator
    $\hat{f}_h(y_0)$ with a Gaussian kernel is replication invariant,
    replication equivariant, or neither.

The KDF with a Gaussian kernel should be replication invariant, but
not replication equivariant. This is because $\mathcal P^k$ does not
change the average of the gaussian kernels.

$$
\mathcal P^k = \{x_1, \dots, x_{kn}\}
$$
$$
\hat f_{h}(y_0) = \frac{1}{KN}\sum_{i=1}^{KN}K_n(y_0 - x_i)
$$
$$
=\frac{1}{N} \sum_{i=1}^{N}K_n(y_0 - y_i)
$$

For the remainder of this question, we consider the *uniform* kernel
instead of the Gaussian one. The uniform kernel prompts the following
$K_h(\cdot)$ function:
$$K_h(v) = \dfrac{1}{2h}, ~~ \text{with support}~~ \lvert v \rvert \le h.$$
That is, $K_h(v) = 0$ if $\lvert v \rvert > h$. Given
$\{y_1 = 3.5, y_2 = 5.5, y_3 = 6\}$ from earlier and a bandwidth of
$h = 1$, the figure below demonstrates that the kernel density estimate
with the uniform kernel is not smooth function of the fixed point $y_0$.
For instance, there is a spike at $y_0 = 4.5$ since it is the only point
such that $\lvert y_0 - y_1 \rvert \le 1$ *and*
$\lvert y_0 - y_2 \rvert \le 1$.

We will investigate the location, scale, and replication properties of
the KDE with a uniform kernel using a framework that is slightly
different than the one introduced in class. In the course notes,
location properties are defined with respect to all $b \in \mathbb{R}$.
When $b = 0$, it is trivial that $a(\mathcal{P}) = a(\mathcal{P} + b)$
for any attribute. If $a(\mathcal{P})$ is location invariant, this
equality holds for all $b \in \mathbb{R}$. Even if $a(\mathcal{P})$ is
not location invariant, $a(\mathcal{P}) = a(\mathcal{P} + b)$ *might* be
true for some $b \in \mathbb{R}$.

For the next parts of the question, you will consider the KDE at
$y_0 = 5.25$ with the uniform kernel defined using $h = 1$. You will
also assume that the population variates are natural numbers:
$y_1, ..., y_N \in \mathbb{N}$.

(d) [2 points] Determine the values of $b \in \mathbb{R}$ for which
    $a(\mathcal{P}) = a(\mathcal{P} + b)$ when the attribute is the KDE
    $\hat{f}_1(5.25)$ with the uniform kernel.

Since $h = 1$, we want $|5.25 - y_{i} - b| \leq 1$. We can think of this
question as asking how much we can scale the natural numbers such that
there are 2 $y_i$ within our $y_0 = 5.25$. We can shift it by $b \leq 4.25$.

(e) [2 points] For scale transformations, it is trivial that
    $a(\mathcal{P}) = a(m\times \mathcal{P})$ for any attribute when
    $m = 1$. Even if $a(\mathcal{P})$ is not scale invariant,
    $a(\mathcal{P}) =a(m\times \mathcal{P})$ might be true for some
    $m > 0$. Determine the values of $m > 0$ for which
    $a(\mathcal{P}) =a(m\times \mathcal{P})$ when the attribute is the
    KDE $\hat{f}_1(5.25)$ with the uniform kernel.

$a(\mathcal P) = a(\mathcal P \cdot m)$ holds true for $m\in\mathbb{N}$.

(f) [1 point] For replication, it is trivial that
    $a(\mathcal{P}) = a(\mathcal{P}^k)$ for any attribute with $k = 1$
    repetition. Even if $a(\mathcal{P})$ is not replication invariant,
    $a(\mathcal{P}) =a(\mathcal{P}^k)$ might be true for some
    $k \in \mathbb{N}$. Determine the values of $k \in \mathbb{N}$ for
    which $a(\mathcal{P}) =a(\mathcal{P}^k)$ when the attribute is the
    KDE $\hat{f}_1(5.25)$ with the uniform kernel.

Replication should not affect the KDE. Thus, $k \in \mathbb{N}$.

\newpage

## QUESTION 3: Overlaying Histograms and Density Estimates [16 points]

The kernel density estimator at a fixed point $y_0$ as introduced in
Question 2 is a numerical attribute. We can synthesize kernel density
estimates across a range of $y_0$ values by plotting them. This process
allows us estimate the unknown density function $f$, and this plot of
the estimated density function is a graphical attribute. The red curve
in the plot below is one such estimated density curve. Histograms are
another popular graphical attribute. The black boxes in the plot below
illustrate the notion of a histogram. In this question, you will create
plots that compare these two graphical attributes for the same data.

(a) [2 points] You will first create a function `kde_gaussian()` that
    takes three inputs: a vector `y` of numeric data, a single point
    `y0`, and a non-negative bandwidth `h`. Using these inputs and the
    formula from Question 2, your function should return the kernel
    density estimate $\hat{f}(y_0)$ based on data `y` and the Gaussian
    kernel. You should not need to use a `for` loop or `apply` function
    for this part of the question.
    
```{r}
gaussian_k <- function(v, h) {
  return (1/(sqrt(2*pi)*h)*exp(-(1/2)*(v/h)^2))
}
kde_gaussian <- function(y, y0, h) {
  return (1/length(y) * sum(gaussian_k(y0 - y, h)))
}
```

(b) [11 points] You will next make a function called
    `hist_density_plot()` that can take in any vector `y` of numeric
    data and overlay a histogram and kernel density estimate in the same
    plot. This function should take the four additional inputs: the
    bandwidth `h` for the density estimate, a vector `xrange` of length
    two that contains the minimum and maximum $x$-values for the plot,
    and inputs `xlabel` and `title` used to customized the plot. The
    plot your function produces should have

-   A histogram plotted on the relative scale, where the bins for the
    histogram are not shaded. You will find the `hist()` function useful
    here. The bins for the histogram should be created using
    `breaks = "fd"`. The limits for the $x$-axis of this plot should
    coincide with `xrange`. [4 points]

-   A density estimate computed using your `kde_gaussian()` function
    with bandwidth `h` at 1000 evenly spaced $y_0$ points in the
    interval `xrange`. You may find `seq()`, `lines()` and `apply()`
    type functions useful here. The density estimate should be plotted
    using a solid line that is a different colour than the histogram. [4
    points]

-   *Both* the histogram and density estimate should fit entirely on the
    $y$-axis of the plot. You should ensure this occurs for any numeric
    vector `y` of data. [1 point]

-   A customizable $x$-axis label and plot title through the inputs
    `xlabel` and `title`. [2 points]

Note: You must create the graph using functions available in `base` R
(all that you need has been laid out above). You may not use the
`density()` function to obtain the kernel density estimate, but you can
use it to check your work.

```{r}
hist_density_plot <- function(y, h, xrange, xlabel, title) {
  histogram <- hist(y,xlim=xrange,breaks="fd",xlab=xlabel,main=title,prob="TRUE",col="white")
  y0 <- seq(from=xrange[1],to=xrange[2], length.out=1000)
  estimate <- sapply(y0, kde_gaussian,y=y,h=h)
  lines(y0, estimate, col='red',lwd=2)
}
```

(c) [3 points] In the last part of this question, you will test your
    `hist_density_plot()` function on the data below. You should use the
    bandwidth determined by Silverman's "rule of thumb":
    $$h = 0.9~ n^{-1/5}\min \left\{SD_{\mathcal{S}}(y), \dfrac{IQR_{\mathcal{S}}(y)}{1.34} \right\},$$
    where $SD_{\mathcal{S}}(y)$ and $IQR_{\mathcal{S}}(y)$ are the
    *sample* standard deviation and interquartile range of the data $y$
    with $n$ observations. These attributes can be computed using
    standard functions in `base` R. The input `xrange` should be the
    vector `c(-4,4)`. Note the plot you produce should look similar to
    the one above.

```{r}
set.seed(341)
z <- rnorm(n = 100)

xrange <- c(-4,4)
h <- 0.9*100^(-1/5)*min(sd(z),IQR(z)/1.34)

hist_density_plot(z,h,xrange,'Range of Data', 'Histogram of Data')
```

\newpage

## QUESTION 4: R Analysis [21 points]

The *Billboard* Hot 100 is the standard chart used by the American music
industry to assess the popularity of songs. The chart has been published
weekly by *Billboard* Magazine since August 4, 1958. Each week, the
chart ranks the 100 most popular songs based on data from the
corresponding tracking period. The chart rankings are based on radio
play, online streaming, and physical and digital sales in the United
States.

The archive of songs that reach #1 on the chart is well documented (see
e.g., [this
list](https://en.wikipedia.org/wiki/List_of_Billboard_Hot_100_number_ones_of_2023)
for the year 2023). You will work with data from songs that went #1 on
the *Billboard* Hot 100 during the $N = 209$ weeks between January 1,
2020 and December 31, 2023. These data are available in the `bh100.csv`
file. Each row of this file corresponds to a specific week and the
columns and their contents are described below.

| Column   | Description                                                                                                                                                                                                         |
|------------|------------------------------------------------------------|
| `Year`   | An integer between 2020 and 2023 signifying a year.                                                                                                                                                                 |
| `Week`   | An integer between 1 and 53 denoting the week of the year in which the *Billboard* Hot 100 chart for that row was published.                                                                                        |
| `Title`  | A character string corresponding to the title of the #1 song on the chart that week. Each song is uniquely identified by its title since no *distinct* songs that went #1 between 2020 and 2023 had the same title. |
| `Artist` | A character string signifying the artist(s) credited on the #1 song.                                                                                                                                                |

(a) [2 points] Using R, read in the data found in `bh100.csv` and create
    a data frame with a row for each song that indicates how many weeks
    it spent at #1 on the *Billboard* Hot 100 between January 1, 2020
    and December 31, 2023. Then, use the `summary()` function on this
    variable for the number of weeks and output the results.
    
```{r}
data <- read.csv("bh100.csv")
titles = data["Title"]
df <- as.data.frame(table(titles))

summary(df["Freq"])
```

(b) [4 points] Let the *population* variance of the number of weeks
    spent at #1 be the attribute of interest so that
    $a(\mathcal{P})=\sum_{i = 1}^N(y_i - \bar{y})^2/N$. The influence of
    song $u$ on $a(\mathcal{P})$ is $\Delta(a,u)$ from the course notes.
    Construct an influence plot of $\Delta$ vs. the observation number.
    Identify the song(s) with the largest influence on the population
    variance attribute and determine their title(s). Based on the data
    frame you created in part (a), describe why the song(s) have such a
    large influence.
    
```{r}
z <- df[["Freq"]]
N = length(z)
pop_var <- function(p) {
  return((sum((p - mean(p))^2))/N)
}

with_u <- pop_var(z)

delta = rep(0, length(z))
for (i in 1:length(z)) {
  delta[i] = pop_var(z) - (1/(N-1)*sum((z - mean(z))^2) - z[i])
}

plot(z, delta, main="Influence Plot", pch=19,col=adjustcolor("black", alpha = 0.2),
     xlab="Number of Days (y)", ylab = bquote(Delta))
```

The song Last Night, appears 16 times at the #1 spot. It has such a large influence because it is observed very frequently, but most songs appear only once or twice at the #1 spot. As an outlier, this causes it to have a larger influence on the variance.

(c) [4 points] Using the `hist_density_plot()` function you created in
    Question 3, construct a plot that visualizes the histogram and
    density estimate for the the number of weeks spent at #1 variate.
    The bandwidth `h` should be determined using Silverman's "rule of
    thumb" introduced in Question 3 and `xrange` should be `c(-1, 18)`.
    Be sure to label the axes and titles informatively. Given your plot,
    does the histogram or density estimate better summarize the data,
    and why?
    
```{r}
xrange <- c(-1, 18)
h <- 0.9*100^(-1/5)*min(sd(z),IQR(z)/1.34)
hist_density_plot(z,h,xrange,title="Billboard #1 Song Frequency",
                  xlabel="Weeks at #1")
```

In the remainder of this question, we will explore the density estimate
for this data set. We will do so by generating a sample from the
distribution with density function $f = \hat{f}$, the estimated density
function produced in the previous plot. We will use rejection sampling
to obtain this sample.

[Rejection sampling](https://en.wikipedia.org/wiki/Rejection_sampling)
transforms a sample from a distribution with density function $g(y)$
into a sample from a distribution with density function $f(y)$.
Rejection sampling is useful when it is easy to sample from $g(y)$ but
difficult to sample from $f(y)$. Rejection sampling uses a constant
$M > 0$ such that $f(y) \le Mg(y)$ for all $y$ to repeatedly implement
the following steps:

-   Generate $y^* \sim g(y)$ and $u^*$ from the uniform
    $\mathcal{U}(0,1)$ distribution

-   If $u^* \le \dfrac{f(y^*)}{Mg(y^*)}$, keep $y^*$. Otherwise, reject
    $y^*$.

It can be shown that the sample of the retained $y^*$ values is indeed a
sample from $f(y)$. We illustrate this fact in the below figure with a
simple example, where $g(y) = 1$ for $0 < y < 1$ and $f(y) = 2y$ for
$0 < y < 1$. Here, we used $M = 2$. The left plot shows the $(y^*, u^*)$
combinations that were retained in green (below the solid line) and
those that were rejected in orange (above the solid line). When
considering the orange and green points together, we can see that the
$y^*$ values were originally generated from a uniform $\mathcal{U}(0,1)$
distribution that corresponds to $g(y) = 1$ for $0 < y < 1$. The right
plot shows the histogram of the $y^*$ values that were retained. This
histogram aligns nicely with the density function $f(y) = 2y$ for
$0 < y < 1$ given by the solid blue line in the left plot.

You will implement rejection sampling in stages over the next parts of
this question.

(d) [2 points] First, you will generate 50000 $y^*$ values from the
    uniform $\mathcal{U}(-1,18)$ distribution. This can be done using
    built-in functions in `base` R. Then, compute $\hat{f}(y^*)$ for
    each of these values using the `kde_gaussian()` function you created
    in Question 3 with the bandwidth `h` you computed in part (c) of
    this question.

```{r}
ystar <- runif(50000, -1, 18)

f <- sapply(ystar, kde_gaussian,y=z,h=h)
```

(e) [3 points] Next, you will generate 50000 $u^*$ values. You will then
    compute the ratios $f(y^*) \div Mg(y^*)$ from the second bullet
    point earlier to decide whether to reject or retain the $y^*$ value.
    You should use $M = 7.25$ and discern $f$ and $g$ based on
    information provided earlier in this question.
    
```{r}
ustar <- runif(50000, 0, 1)
M <- 7.25
g <- sapply(ystar,dunif,min=-1,max=18)
data <- c()
for (i in 1:50000) {
  if (ustar[i] <= (f[i]/(M*g[i]))) {
    data <- c(data, ystar[i])
  }
}
```

(f) [2 points] You will now plot a histogram of your retained $y^*$
    values on the relative scale, where the bins for the histogram are
    unshaded and created using `breaks = "fd"`. The limits for the
    $x$-axis of this plot should coincide with `xrange` used in part (c)
    of this question. Remember to include informative titles and axis
    labels.
    
```{r}
hist(data, breaks="fd",xlim=xrange,main="Rejection Sample", xlab="Number of Weeks at #1",prob=TRUE)
```

(g) [3 points] Lastly, you will use the sample of your retained $y^*$
    values to estimate $Pr(Y < 1)$, where $Y$ is the random variable
    with density function $\hat{f}$ from part (c). Based on the formula
    for the kernel density estimator from Question 2, why is this
    probability greater than 0 when the Gaussian kernel is used?
    Briefly, what do the results from this question suggest about using
    kernel density estimation to summarize data from distributions with
    bounded support.

```{r}
pr_y <- sum(data < 3)/50000
print(pr_y)
```
The probabilty is 0.09606.

Based on the formula for the KDE in Question 2, the probability is greater
than 0 when the Gaussian kernel is used because the the kernel is simply
the gaussian pdf with a mean of 0, and due to the symmetry of the
distribution, it follows that there are values where $Y$ < 1.

The results from this question suggests that using kernel density
estimation to summarize data from distributions with bounded support is
an alternative to sampling from a difficult distribution and instead
we can use a simpler distribution and use rejection sampling instead to
achieve similar results.