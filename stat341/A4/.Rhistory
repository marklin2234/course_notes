dis <- abs(mean(p$pop1$Order.Value) - mean(p$pop2$Order.Value))
x <- c(x, dis)
}
hist(x, xlab="Idx", ylab="Discrepancy Measures", main="Histogram of Discrepancy Measures",prob=TRUE)
abline(v=D,col="RED",lwd=2)
p <- mean(x >= D)
p <- mean(x >= D)
p
data$Version <- ifelse(data$Version == "T", 1, 0)
fit <- lm(data$Order.Value ~ data$Version)
coeffs <- coefficients(fit)
alpha <- coeffs[[1]]; beta <- coeffs[[2]]
ATE <- beta
lift <- beta/alpha
print(paste0("ATE: ", ATE))
print(paste0("lift: ", lift))
bootstrap_t_interval_new <- function(S, a, confidence, B, D, ...) {
##    S = an n row data frame containing the variate values in the sample
##    a = a scalar-valued function that calculates the attribute a() of interest
##    confidence = a value in (0,1) indicating the confidence level
##    B = a numeric value representing the outer bootstrap count of
##    replicates (used to calculate the lower and upper limits)
##    D = a numeric value representing the inner bootstrap count of replicates
##    (used to estimate the standard deviation of the sample attribute for
##    each (outer) bootstrap sample)
aS <- a(S)
sampleSize <- nrow(S)
## get (outer) bootstrap values
bVals <- sapply(1:B, FUN = function(b) {
Sstar.idx <- sample(1:sampleSize, sampleSize, replace = TRUE)
aSstar <- a(S[Sstar.idx,])
## get (inner) bootstrap values to estimate the SD
SD_aSstar <- sd(sapply(1:D, FUN = function(d) {
Sstarstar.idx <- sample(Sstar.idx, sampleSize, replace = TRUE)
## return the attribute value
a(S[Sstarstar.idx,])
}))
z <- (aSstar - aS)/SD_aSstar
## Return the two values
c(aSstar = aSstar, z = z)
})
SDhat <- sd(bVals["aSstar", ])
zVals <- bVals["z", ]
## Now use these zVals to get the lower and upper c values.
cValues <- quantile(zVals, probs = c((1 - confidence)/2, (confidence +
1)/2), na.rm = TRUE)
cLower <- min(cValues)
cUpper <- max(cValues)
interval <- c(lower = aS - cUpper * SDhat, middle = aS, upper = aS -
cLower * SDhat)
return(interval)
}
calculatePVmulti <- function(pop, discrepancies, M_outer = 1000, M_inner) {
# pop is a list whose two members are two sub-populations
if (missing(M_inner))
M_inner <- M_outer
## Local function to calculate the significance levels over the
## discrepancies and return their minimum
getSLmin <- function(basePop, discrepancies, M) {
observedVals <- sapply(discrepancies, FUN = function(discrepancy) {
discrepancy(basePop)
})
K <- length(discrepancies)
total <- Reduce(function(counts, i) {
# mixRandomly mixes the two populations randomly, so the new
# sub-populations are indistinguishable
NewPop <- mixRandomly(basePop)
## calculate the discrepancy and counts
Map(function(k) {
Dk <- discrepancies[[k]](NewPop)
if (Dk >= observedVals[k])
counts[k] <<- counts[k] + 1
}, 1:K)
counts
}, 1:M, init = numeric(length = K))
SLs <- total/M
min(SLs)
}
SLmin <- getSLmin(pop, discrepancies, M_inner)
total <- Reduce(function(count, m) {
basePop <- mixRandomly(pop)
if (getSLmin(basePop, discrepancies, M_inner) <= SLmin)
count + 1 else count
}, 1:M_outer, init = 0)
SLstar <- total/M_outer
SLstar
}
knitr::opts_chunk$set(echo = TRUE, warning = FALSE)
library(knitr)
#opts_chunk$set(tidy.opts=list(width.cutoff=70),tidy=TRUE)
knitr::opts_chunk$set(cache=TRUE)
data <- read.csv("asos.csv")
summary(data[data$Version == "T",])
summary(data[data$Version == "C",])
p = seq(from = 0, to = 1, by = 0.01)
x = quantile(data[data$Version == "C",]$Order.Value, p)
y = quantile(data[data$Version == "T",]$Order.Value, p)
plot(x,y, xlab = 'Control Quantiles', ylab = 'Order Value', main='Control vs Order Quantiles')
abline(a = 0, b = 1)
mean_T <- mean(data[data$Version == "T",]$Order.Value)
mean_C <- mean(data[data$Version == "C",]$Order.Value)
D <- abs(mean_T - mean_C)
mixRandomly <- function(p) {
p1 <- p$pop1
n1 <- nrow(p1)
p2 <- p$pop2
n2 <- length(p2)
mix <- rbind(p1, p2)
select <- sample(1:(n1 + n2), n1, replace = FALSE)
newp1 <- mix[select,]
newp2 <- mix[-select,]
list(pop1 = newp1, pop2 = newp2)
}
x <- c()
for (i in 1:1000) {
p <- mixRandomly(list(pop1 = data[data$Version == "T",],
pop2=data[data$Version == "C",]))
dis <- abs(mean(p$pop1$Order.Value) - mean(p$pop2$Order.Value))
x <- c(x, dis)
}
hist(x, xlab="Idx", ylab="Discrepancy Measures", main="Histogram of Discrepancy Measures",prob=TRUE)
abline(v=D,col="RED",lwd=2)
p <- mean(x >= D)
p
getQCompFn <- function(p){
function(pop) {
as.numeric(abs(quantile(pop[[2]]$Order.Value, p) - quantile(pop[[1]]$Order.Value, p)))
}
}
data$Version <- ifelse(data$Version == "T", 1, 0)
fit <- lm(data$Order.Value ~ data$Version)
coeffs <- coefficients(fit)
alpha <- coeffs[[1]]; beta <- coeffs[[2]]
ATE <- beta
lift <- beta/alpha
print(paste0("ATE: ", ATE))
print(paste0("lift: ", lift))
B <- 1000
samples <- list()
for (b in 1:B) {
samples[[b]] <- data[sample(nrow(data), nrow(data), replace=TRUE), c("Order.Value", "Version")]
}
ATE_stars <- c()
lift_stars <- c()
for (b in 1:B) {
sample <- samples[[b]]
fit <- lm(sample$Order.Value ~ sample$Version)
coeffs <- coefficients(fit)
alpha <- coeffs[[1]]; beta <- coeffs[[2]]
ATE_stars <- c(ATE_stars, beta)
lift_stars <- c(lift_stars, beta/alpha)
}
hist(ATE_stars,prob=TRUE)
abline(v=ATE,col="RED",lwd=2)
hist(lift_stars,prob=TRUE)
abline(v=lift,col="RED",lwd=2)
cat("ATE CI: (", ATE + 1.96 * c(-1,1) * sd(ATE_stars),")\n")
cat("lift CI: (", lift + 1.96 * c(-1,1) * sd(lift_stars), ")")
print(paste0("ATE CI: (:", quantile(ATE_stars, probs=0.025), ", ", quantile(ATE_stars, prob=0.9725), ")"))
print(paste0("lift CI: (", quantile(lift_stars, probs=0.025), ", ", quantile(lift_stars, prob=0.9725), ")"))
bootstrap_t_interval_new <- function(S, a, confidence, B, D, ...) {
## S = an n row data frame containing the variate values in the sample
## a = a scalar-valued function that calculates the attribute a() of interest
## confidence = a value in (0,1) indicating the confidence level
## B = a numeric value representing the outer bootstrap count of
## replicates (used to calculate the lower and upper limits)
## D = a numeric value representing the inner bootstrap count of replicates
## (used to estimate the standard deviation of the sample attribute for
## each (outer) bootstrap sample)
aS <- a(S)
sampleSize <- nrow(S)
## get (outer) bootstrap values
bVals <- sapply(1:B, FUN = function(b) {
Sstar.idx <- sample(1:sampleSize, sampleSize, replace = TRUE)
aSstar <- a(S[Sstar.idx,])
## get (inner) bootstrap values to estimate the SD
SD_aSstar <- sd(sapply(1:D, FUN = function(d) {
Sstarstar.idx <- sample(Sstar.idx, sampleSize, replace = TRUE)
## return the attribute value
a(S[Sstarstar.idx,])
}))
z <- (aSstar - aS)/SD_aSstar
## Return the two values
c(aSstar = aSstar, z = z)
})
SDhat <- sd(bVals["aSstar", ])
zVals <- bVals["z", ]
## Now use these zVals to get the lower and upper c values.
cValues <- quantile(zVals, probs = c((1 - confidence)/2, (confidence +
1)/2), na.rm = TRUE)
cLower <- min(cValues)
cUpper <- max(cValues)
interval <- c(lower = aS - cUpper * SDhat, middle = aS, upper = aS -
cLower * SDhat)
return(interval)
}
findATE <- function(S) {
fit <- lm(S$Order.Value ~ S$Version)
coeffs <- coefficients(fit)
beta <- coeffs[[2]]
beta
}
findlift <- function(S) {
fit <- lm(S$Order.Value ~ S$Version)
coeffs <- coefficients(fit)
alpha <- coeffs[[1]]; beta <- coeffs[[2]]
beta/alpha
}
bootstrap_t_interval_new(data[, !(names(data) == "Day")], findATE, 0.95, 100, 100)
bootstrap_t_interval_new(data[, !(names(data) == "Day")], findlift, 0.95, 100, 100)
calculatePVmulti <- function(pop, discrepancies, M_outer = 1000, M_inner) {
# pop is a list whose two members are two sub-populations
if (missing(M_inner))
M_inner <- M_outer
## Local function to calculate the significance levels over the
## discrepancies and return their minimum
getSLmin <- function(basePop, discrepancies, M) {
observedVals <- sapply(discrepancies, FUN = function(discrepancy) {
discrepancy(basePop)
})
K <- length(discrepancies)
total <- Reduce(function(counts, i) {
# mixRandomly mixes the two populations randomly, so the new
# sub-populations are indistinguishable
NewPop <- mixRandomly(basePop)
## calculate the discrepancy and counts
Map(function(k) {
Dk <- discrepancies[[k]](NewPop)
if (Dk >= observedVals[k])
counts[k] <<- counts[k] + 1
}, 1:K)
counts
}, 1:M, init = numeric(length = K))
SLs <- total/M
min(SLs)
}
SLmin <- getSLmin(pop, discrepancies, M_inner)
total <- Reduce(function(count, m) {
basePop <- mixRandomly(pop)
if (getSLmin(basePop, discrepancies, M_inner) <= SLmin)
count + 1 else count
}, 1:M_outer, init = 0)
SLstar <- total/M_outer
SLstar
}
data <- read.csv("asos.csv")
summary(data[data$Version == "T",])
summary(data[data$Version == "C",])
p = seq(from = 0, to = 1, by = 0.01)
x = quantile(data[data$Version == "C",]$Order.Value, p)
y = quantile(data[data$Version == "T",]$Order.Value, p)
plot(x,y, xlab = 'Control Quantiles', ylab = 'Order Value', main='Control vs Order Quantiles')
abline(a = 0, b = 1)
mean_T <- mean(data[data$Version == "T",]$Order.Value)
mean_C <- mean(data[data$Version == "C",]$Order.Value)
D <- abs(mean_T - mean_C)
mean_T <- mean(data[data$Version == "T",]$Order.Value)
mean_C <- mean(data[data$Version == "C",]$Order.Value)
D <- abs(mean_T - mean_C)
D
mixRandomly <- function(p) {
p1 <- p$pop1
n1 <- nrow(p1)
p2 <- p$pop2
n2 <- length(p2)
mix <- rbind(p1, p2)
select <- sample(1:(n1 + n2), n1, replace = FALSE)
newp1 <- mix[select,]
newp2 <- mix[-select,]
list(pop1 = newp1, pop2 = newp2)
}
x <- c()
for (i in 1:1000) {
p <- mixRandomly(list(pop1 = data[data$Version == "T",],
pop2=data[data$Version == "C",]))
dis <- abs(mean(p$pop1$Order.Value) - mean(p$pop2$Order.Value))
x <- c(x, dis)
}
hist(x, xlab="Idx", ylab="Discrepancy Measures", main="Histogram of Discrepancy Measures",prob=TRUE)
abline(v=D,col="RED",lwd=2)
data$Version <- ifelse(data$Version == "T", 1, 0)
fit <- lm(data$Order.Value ~ data$Version)
coeffs <- coefficients(fit)
alpha <- coeffs[[1]]; beta <- coeffs[[2]]
ATE <- beta
lift <- beta/alpha
print(paste0("ATE: ", ATE))
print(paste0("lift: ", lift))
bootstrap_t_interval_new <- function(S, a, confidence, B, D, ...) {
## S = an n row data frame containing the variate values in the sample
## a = a scalar-valued function that calculates the attribute a() of interest
## confidence = a value in (0,1) indicating the confidence level
## B = a numeric value representing the outer bootstrap count of
## replicates (used to calculate the lower and upper limits)
## D = a numeric value representing the inner bootstrap count of replicates
## (used to estimate the standard deviation of the sample attribute for
## each (outer) bootstrap sample)
aS <- a(S)
sampleSize <- nrow(S)
## get (outer) bootstrap values
bVals <- sapply(1:B, FUN = function(b) {
Sstar.idx <- sample(1:sampleSize, sampleSize, replace = TRUE)
aSstar <- a(S[Sstar.idx,])
## get (inner) bootstrap values to estimate the SD
SD_aSstar <- sd(sapply(1:D, FUN = function(d) {
Sstarstar.idx <- sample(Sstar.idx, sampleSize, replace = TRUE)
## return the attribute value
a(S[Sstarstar.idx,])
}))
z <- (aSstar - aS)/SD_aSstar
## Return the two values
c(aSstar = aSstar, z = z)
})
SDhat <- sd(bVals["aSstar", ])
zVals <- bVals["z", ]
## Now use these zVals to get the lower and upper c values.
cValues <- quantile(zVals, probs = c((1 - confidence)/2, (confidence +
1)/2), na.rm = TRUE)
cLower <- min(cValues)
cUpper <- max(cValues)
interval <- c(lower = aS - cUpper * SDhat, middle = aS, upper = aS -
cLower * SDhat)
return(interval)
}
findATE <- function(S) {
fit <- lm(S$Order.Value ~ S$Version)
coeffs <- coefficients(fit)
beta <- coeffs[[2]]
beta
}
findlift <- function(S) {
fit <- lm(S$Order.Value ~ S$Version)
coeffs <- coefficients(fit)
alpha <- coeffs[[1]]; beta <- coeffs[[2]]
beta/alpha
}
bootstrap_t_interval_new(data[, !(names(data) == "Day")], findATE, 0.95, 100, 100)
bootstrap_t_interval_new(data[, !(names(data) == "Day")], findlift, 0.95, 100, 100)
print(paste0("ATE CI: (:", quantile(ATE_stars, probs=0.025), ", ", quantile(ATE_stars, prob=0.9725), ")"))
print(paste0("lift CI: (", quantile(lift_stars, probs=0.025), ", ", quantile(lift_stars, prob=0.9725), ")"))
cat("ATE CI: (", ATE + 1.96 * c(-1,1) * sd(ATE_stars),")\n")
cat("lift CI: (", lift + 1.96 * c(-1,1) * sd(lift_stars), ")")
library(knitr)
#opts_chunk$set(tidy.opts=list(width.cutoff=70),tidy=TRUE)
knitr::opts_chunk$set(cache=TRUE)
data <- read.csv("asos.csv")
summary(data[data$Version == "T",])
summary(data[data$Version == "C",])
p = seq(from = 0, to = 1, by = 0.01)
x = quantile(data[data$Version == "C",]$Order.Value, p)
y = quantile(data[data$Version == "T",]$Order.Value, p)
plot(x,y, xlab = 'Control Quantiles', ylab = 'Order Value', main='Control vs Order Quantiles')
abline(a = 0, b = 1)
knitr::opts_chunk$set(echo = TRUE, warning = FALSE)
library(knitr)
#opts_chunk$set(tidy.opts=list(width.cutoff=70),tidy=TRUE)
knitr::opts_chunk$set(cache=TRUE)
data <- read.csv("asos.csv")
summary(data[data$Version == "T",])
summary(data[data$Version == "C",])
p = seq(from = 0, to = 1, by = 0.01)
x = quantile(data[data$Version == "C",]$Order.Value, p)
y = quantile(data[data$Version == "T",]$Order.Value, p)
plot(x,y, xlab = 'Control Quantiles', ylab = 'Order Value', main='Control vs Order Quantiles')
abline(a = 0, b = 1)
mean_T <- mean(data[data$Version == "T",]$Order.Value)
mean_C <- mean(data[data$Version == "C",]$Order.Value)
D <- abs(mean_T - mean_C)
D
mixRandomly <- function(p) {
p1 <- p$pop1
n1 <- nrow(p1)
p2 <- p$pop2
n2 <- length(p2)
mix <- rbind(p1, p2)
select <- sample(1:(n1 + n2), n1, replace = FALSE)
newp1 <- mix[select,]
newp2 <- mix[-select,]
list(pop1 = newp1, pop2 = newp2)
}
x <- c()
for (i in 1:1000) {
p <- mixRandomly(list(pop1 = data[data$Version == "T",],
pop2=data[data$Version == "C",]))
dis <- abs(mean(p$pop1$Order.Value) - mean(p$pop2$Order.Value))
x <- c(x, dis)
}
hist(x, xlab="Idx", ylab="Discrepancy Measures", main="Histogram of Discrepancy Measures",prob=TRUE)
abline(v=D,col="RED",lwd=2)
p <- mean(x >= D)
p
getQCompFn <- function(p){
function(pop) {
as.numeric(abs(quantile(pop[[2]]$Order.Value, p) - quantile(pop[[1]]$Order.Value, p)))
}
}
data$Version <- ifelse(data$Version == "T", 1, 0)
fit <- lm(data$Order.Value ~ data$Version)
coeffs <- coefficients(fit)
alpha <- coeffs[[1]]; beta <- coeffs[[2]]
ATE <- beta
lift <- beta/alpha
print(paste0("ATE: ", ATE))
print(paste0("lift: ", lift))
B <- 1000
samples <- list()
for (b in 1:B) {
samples[[b]] <- data[sample(nrow(data), nrow(data), replace=TRUE), c("Order.Value", "Version")]
}
ATE_stars <- c()
lift_stars <- c()
for (b in 1:B) {
sample <- samples[[b]]
fit <- lm(sample$Order.Value ~ sample$Version)
coeffs <- coefficients(fit)
alpha <- coeffs[[1]]; beta <- coeffs[[2]]
ATE_stars <- c(ATE_stars, beta)
lift_stars <- c(lift_stars, beta/alpha)
}
hist(ATE_stars,prob=TRUE)
abline(v=ATE,col="RED",lwd=2)
hist(lift_stars,prob=TRUE)
abline(v=lift,col="RED",lwd=2)
cat("ATE CI: (", ATE + 1.96 * c(-1,1) * sd(ATE_stars),")\n")
cat("lift CI: (", lift + 1.96 * c(-1,1) * sd(lift_stars), ")")
print(paste0("ATE CI: (:", quantile(ATE_stars, probs=0.025), ", ", quantile(ATE_stars, prob=0.9725), ")"))
print(paste0("lift CI: (", quantile(lift_stars, probs=0.025), ", ", quantile(lift_stars, prob=0.9725), ")"))
bootstrap_t_interval_new <- function(S, a, confidence, B, D, ...) {
## S = an n row data frame containing the variate values in the sample
## a = a scalar-valued function that calculates the attribute a() of interest
## confidence = a value in (0,1) indicating the confidence level
## B = a numeric value representing the outer bootstrap count of
## replicates (used to calculate the lower and upper limits)
## D = a numeric value representing the inner bootstrap count of replicates
## (used to estimate the standard deviation of the sample attribute for
## each (outer) bootstrap sample)
aS <- a(S)
sampleSize <- nrow(S)
## get (outer) bootstrap values
bVals <- sapply(1:B, FUN = function(b) {
Sstar.idx <- sample(1:sampleSize, sampleSize, replace = TRUE)
aSstar <- a(S[Sstar.idx,])
## get (inner) bootstrap values to estimate the SD
SD_aSstar <- sd(sapply(1:D, FUN = function(d) {
Sstarstar.idx <- sample(Sstar.idx, sampleSize, replace = TRUE)
## return the attribute value
a(S[Sstarstar.idx,])
}))
z <- (aSstar - aS)/SD_aSstar
## Return the two values
c(aSstar = aSstar, z = z)
})
SDhat <- sd(bVals["aSstar", ])
zVals <- bVals["z", ]
## Now use these zVals to get the lower and upper c values.
cValues <- quantile(zVals, probs = c((1 - confidence)/2, (confidence +
1)/2), na.rm = TRUE)
cLower <- min(cValues)
cUpper <- max(cValues)
interval <- c(lower = aS - cUpper * SDhat, middle = aS, upper = aS -
cLower * SDhat)
return(interval)
}
findATE <- function(S) {
fit <- lm(S$Order.Value ~ S$Version)
coeffs <- coefficients(fit)
beta <- coeffs[[2]]
beta
}
findlift <- function(S) {
fit <- lm(S$Order.Value ~ S$Version)
coeffs <- coefficients(fit)
alpha <- coeffs[[1]]; beta <- coeffs[[2]]
beta/alpha
}
bootstrap_t_interval_new(data[, !(names(data) == "Day")], findATE, 0.95, 100, 100)
bootstrap_t_interval_new(data[, !(names(data) == "Day")], findlift, 0.95, 100, 100)
calculatePVmulti <- function(pop, discrepancies, M_outer = 1000, M_inner) {
# pop is a list whose two members are two sub-populations
if (missing(M_inner))
M_inner <- M_outer
## Local function to calculate the significance levels over the
## discrepancies and return their minimum
getSLmin <- function(basePop, discrepancies, M) {
observedVals <- sapply(discrepancies, FUN = function(discrepancy) {
discrepancy(basePop)
})
K <- length(discrepancies)
total <- Reduce(function(counts, i) {
# mixRandomly mixes the two populations randomly, so the new
# sub-populations are indistinguishable
NewPop <- mixRandomly(basePop)
## calculate the discrepancy and counts
Map(function(k) {
Dk <- discrepancies[[k]](NewPop)
if (Dk >= observedVals[k])
counts[k] <<- counts[k] + 1
}, 1:K)
counts
}, 1:M, init = numeric(length = K))
SLs <- total/M
min(SLs)
}
SLmin <- getSLmin(pop, discrepancies, M_inner)
total <- Reduce(function(count, m) {
basePop <- mixRandomly(pop)
if (getSLmin(basePop, discrepancies, M_inner) <= SLmin)
count + 1 else count
}, 1:M_outer, init = 0)
SLstar <- total/M_outer
SLstar
}
