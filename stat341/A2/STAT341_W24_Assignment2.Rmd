---
title: "STAT 341: Assignment 2"
subtitle: "DUE: Friday, February 16, 2024 by 5:00pm EST"
output:
  pdf_document: default
urlcolor: blue
header-include:
  - \usepackage{amsmath}
---
```{r setup, include = FALSE}
knitr::opts_chunk$set(echo = TRUE, warning = FALSE)
```

```{r, include = FALSE}
library(knitr)
opts_chunk$set(tidy.opts=list(width.cutoff=70),tidy=TRUE)
```

$\;$
$\;$
$\;$
$\;$

## NOTES

Your assignment must be submitted by the due date listed at the top of this document, and it must be submitted electronically in .pdf format via Crowdmark. This means that your responses for different questions should begin on separate pages of your .pdf file. Note that your .pdf solution file must have been generated by R Markdown. Additionally:

* For mathematical questions: your solutions must be produced by LaTeX (from within R Markdown). Neither screenshots nor scanned/photographed handwritten solutions will be accepted -- these will receive zero points.

* For computational questions: R code should always be included in your solution (via code chunks in R Markdown). If code is required and you provide none, you will receive zero points.
   + **Exception** any functions defined in the lecture notes can be loaded using `echo=FALSE` but any other code chunks should have `echo=TRUE`. e.g., the code chunk loading `gradientDescent` can use `echo=FALSE` but chunks that call `gradientDescent` should have `echo=TRUE`.
   
* For interpretation questions: plain text (within R Markdown) is required. Text responses embedded as comments within code chunks will not be accepted.

Organization and comprehensibility is part of a full solution. Consequently, points will be deducted for solutions that are not organized and incomprehensible. Furthermore, if you submit your assignment to Crowdmark, but you do so incorrectly in any way (e.g., you upload your Question 2 solution in the Question 1 box), you will receive a 5% deduction (i.e., 5% of the assignmentâ€™s point total will be deducted from your point total).

\newpage

## QUESTION 1: Implicit Attributes with Optimal Kernel Bandwidth [20 points]

As mentioned on Assignment 1, the *Billboard* Hot 100 chart is used by the American music industry to assess the popularity of songs. Each week, the chart ranks the 100 most popular songs based on data from the corresponding tracking period. The chart rankings are based on radio play, online streaming, and physical and digital sales in the United States.

In this question, you will consider streaming data used to inform the rankings from the most recent *Billboard* Hot 100 chart, dated January 20$^{\text{th}}$. The data available to you are stored in the `bh100_jan20.csv` file. Note that this data set was curated from Twitter posts about chart predictions from [this account](https://twitter.com/talkofthecharts) since *Billboard* Magazine does not publicly post all data used to create their charts. For each of the $N=100$ songs on this week's chart, we have observations for each of the following variates:

Variate        | Description
-------------- |-----------------------------------------------------
`Position`         | A numeric variate recording a chart position between 1 and 100, inclusive.
`Title`        | A character string corresponding to the title of the song in that position.
`Artist`       | A character string signifying the artist(s) credited on that song.
`Streams`      | The number of streams that song received during the tracking week, recorded in millions.

(a) [2 points] Create a plot of the kernel density estimate for the `Streams` variate using the Gaussian kernel and bandwidth selected by Silverman's rule of thumb. You *can* use the `density()` function on this assignment. Remember to include informative titles for the plot and both axes.

```{r}
set.seed(123)
data <- read.csv("bh100_jan20.csv")

h <- 0.9*100^(-1/5)*min(sd(data$Streams),IQR(data$Streams)/1.34)
estimate <- density(data$Streams, bw=h)
plot(estimate, col='red',lwd=2)
```

(b) [5 points] In this question, you will investigate the impact of the bandwidth $h$ on the kernel density estimate. Instead of using Silverman's rule of thumb, the kernel bandwidth could be chosen to minimize a cost function $c(h; \mathcal{P})$. Given a cost function, the optimal bandwidth $\hat{h} =  \text{argmin}_{h > 0}~c(h; {\mathcal{P}})$ can be viewed as an implicit population attribute. We will use the following cost function for the Gaussian kernel taken from [this paper](https://link.springer.com/article/10.1007/s10827-009-0180-4):
$$c(h; y_1, \dots, y_N) = \dfrac{1}{2\sqrt{\pi}N^2}\left[\dfrac{N}{h} + \dfrac{2}{h}\sum_{i = 1}^N\sum_{j = i + 1}^N\left( \text{exp}\left(\dfrac{-(y_i - y_j)^2}{4h^2}\right) - 2\sqrt{2}\text{exp}\left(\dfrac{-(y_i - y_j)^2}{2h^2}\right)\right)\right].$$
Create a function `cost()` that returns the value of the above function for a given bandwidth `h` and the variates $\{y_1, \dots, y_N\}$. Obtain the optimal bandwidth with respect to this cost function for the streams variate; you can use an optimizer like `optim()` to do so. Output the optimal bandwidth $h$ you found. 

    *Optional* suggestion: you need not use a `for` loop or `apply` function to compute cost if you reformat the streaming data so that there is one row for each combination of chart positions $(i, j)$ such that $i < j$.

```{r}
cost <- function(h, y) {
  N <- length(y)
  val <- 0
  for(i in 1:(N-1)) {
    for (j in (i+1):N) {
      val <- val+exp(-(y[i]-y[j])^2/(4*h^2))-2*sqrt(2)*exp(-(y[i]-y[j])^2/(2*h^2))
    }
  }
  return (1/(2*sqrt(pi)*N^2)*(N/h+(2/h*val)))
}

h_hat <- optim(par=1,fn=cost,y=data$Streams)$par
print(h_hat)
```

(c) [4 points] Replicate your plot from part (a) and add a second density curve in a different colour for the streaming data created using a Gaussian kernel with the optimal bandwidth found in part (b). A legend in the top right corner of your plot should denote the *numeric* value of $h$ that each colour corresponds to. Ensure that both density curves fit completely on your plot by adjusting the limits of plot axes if necessary. How does changing the bandwidth $h$ change the shape of the density estimate?

```{r}
estimate2 <- density(data$Streams, h_hat)
plot(estimate2, col="blue", lwd=2, main="Density of Streams",xlab="Streams")
lines(estimate, lwd=1, col="red")
legend("topright", legend=c(h, h_hat), col=c("red","blue"), lty=1, lwd=2)
```

Changing the bandwidth makes the shape of the density estimate to be less smooth.

(d) [5 points] Implement the bump rule for histograms to make the streaming variate more symmetric. Starting with $\alpha = 1$, increase or decrease $\alpha$ in increments of 0.5. Make a composite plot of histograms similar to that on page 40 of Section 2.2 in the course notes. You *can* use `powerfun()` and other code from the course notes. This composite plot should satisfy the following requirements:

    * Let your preferred value of $\alpha$ be $\alpha^*$. If $\alpha^* < 1$, include the plots for $\alpha = \{1, 0.5, ..., \alpha^*, \alpha^*-0.5\}$. If $\alpha^* > 1$, include the plots for $\alpha = \{1, 1.5, ..., \alpha^*, \alpha^* + 0.5\}$.
    * This composite plot should have two columns and as many rows as you require. The plot and axis titles for each individual plot should be formatted as on page 40 of Section 2.2.
    
     What value of $\alpha$ is your preferred transformation? Briefly describe your process to obtain this $\alpha$ value.
     
```{r}
powerfun <- function(y, alpha) {
  if(sum(y <= 0) > 0) stop("y must be positive")
  if (alpha == 0)
    log(y)
  else if (alpha > 0) {
    y^alpha
  } else -y^alpha
}

a <- seq(-0.5,1, length.out=7)

par(mfrow=c(2,2))
for (i in 1:7) {
  hist(powerfun(data$Streams + 1, a[i]), col=adjustcolor("grey",alpha=0.5),main=bquote(alpha== .(a[i])),xlab="",breaks=50)
}
```

When $\alpha = 0$ the histogram is most symmetric. I acquired this $\alpha^*$ value by sequencing from -2.5 to 1, and choosing the alpha value that was most symmetric.

(e) [4 points] Use `cost()` from part (b) to find the optimal bandwidth $\hat{h}$ for the streaming data transformed using your preferred value of $\alpha$ from part (d). For the transformed data, create a plot that compares two Gaussian kernel density estimates: one with $h$ selected via Silverman's rule of thumb and the other with $\hat{h}$ found in this question. This plot should be formatted as in part (c). What do you notice about the two density estimates for the transformed data?
```{r}
trans_y <- powerfun(data$Streams + 1, 0)

h_hat2 <- optim(par=1,fn=cost,y=trans_y)$par
h2 <- 0.9*100^(-1/5)*min(sd(trans_y),IQR(trans_y)/1.34)
estimate3 <- density(trans_y,bw=h_hat2)
estimate4 <- density(trans_y,bw=h2)

plot(estimate3, col="red",main="Density of Streams with alpha=0",xlab="Transformed Streams with alpha=0")
lines(estimate4, col="blue")
legend(x="topright", legend=c(h_hat2, h2), col=c("red","blue"), lty=1, lwd=2)

```

I notice that two bandwidth values are very similar, meaning that the kernel density estimate is almost identical.

\newpage

## QUESTION 2: Price's Function [20 points]

Price's Function is one of many non-convex [test functions](https://en.wikipedia.org/wiki/Test_functions_for_optimization) used for evaluating the performance of optimization methods. For $\boldsymbol\theta = (\theta_1,\theta_2) \in \mathbb{R}^2$ the function is defined as follows $$\rho(\boldsymbol{\theta}) = 100(\theta_2-\theta_1^2)^2 + (6.4(\theta_2 - 0.5)^2 - \theta_1 - 0.6)^2$$ The figures below depict the function (as a 3-dimensional surface and with 2-dimensional contours) for $\theta_1 \in [-1.5,1.5]$ and $\theta_2 \in [-1,2]$.

\begin{center}
\end{center}

As can be seen in these plots, the function has three local minima labeled (in the contour plot) $A=(-0.664, 0.441)$, $B=(0.342, 0.116)$, $C=(1,1)$. The three minima are also labeled on the three-dimensional surface by the white points. However, only $B=(0.342, 0.116)$ and $C=(1,1)$ are global minima, at which points Price's Function attains a value of 0. 

(a) [4 points] Write `rho` and `gradient` functions for Price's Function which take a single vector-valued input `theta`. Note that you may use without proof or derivation the fact that $$\frac{\partial\rho}{\partial\theta_1} = -400\theta_1(\theta_2 - \theta_1^2) - 2(6.4(\theta_2 - 0.5)^2 - \theta_1 - 0.6)$$ and $$\frac{\partial\rho}{\partial\theta_2} = 200(\theta_2 - \theta_1^2) + 25.6(\theta_2 - 0.5)(6.4(\theta_2 - 0.5)^2 - \theta_1 - 0.6)$$

```{r}
rho <- function(theta) {
  return ( 100*(theta[2]-theta[1]^2)^2 + (6.4*(theta[2] - 0.5)^2-theta[1]-0.6)^2 )
}
gradient <- function(theta) {
  p1 <- -400*theta[1]*(theta[2]-theta[1]^2)-2*(6.4*(theta[2]-0.5)^2-theta[1]-0.6)
  p2 <- 200 *(theta[2]-theta[1]^2)+25.6*(theta[2]-0.5)*(6.4*(theta[2]-0.5)^2-theta[1]-0.6)
  result <- c(p1,p2)
  return (result)
}
```

(b) [5 points] In this question, you will explore the surface of Price's Function using gradient descent. In particular, you will consider 5 different starting values and explore the impact of changing one's starting location. Using the `gradientDescent()` function (from class) together with the `gridLineSearch()` and `testConvergence()` functions (from class) as well as the `rho` and `gradient` functions from part (a), find the solution to $$\operatorname*{arg min}_{\boldsymbol\theta \in \mathbb{R}^2} \rho(\boldsymbol\theta)$$ for each of the following five starting values. In each case, state which minima you've converged to (A, B, or C) and be sure to include the output from the `gradientDescent()` function.

  i. $\widehat{\boldsymbol{\theta}}_0=(1.1,1.75)$

  ii. $\widehat{\boldsymbol{\theta}}_0=(-0.5,0.6)$

  iii. $\widehat{\boldsymbol{\theta}}_0=(-0.2,-0.8)$

  iv. $\widehat{\boldsymbol{\theta}}_0=(0.1,1.1)$

  v. $\widehat{\boldsymbol{\theta}}_0=(0.75,0.6)$

```{r}
gradientDescent <- function(theta = 0, rhoFn, gradientFn, lineSearchFn, testConvergenceFn, maxIterations = 100, tolerance = 1e-06, relative = FALSE, lambdaStepsize = 0.01, lambdaMax = 0.5) {
  converged <- FALSE
  i <- 0
  while (!converged & i <= maxIterations) {
    g <- gradientFn(theta) ## gradient
    glength <- sqrt(sum(g^2)) ## gradient direction
    if (glength > 0)
      d <- g/glength
    lambda <- lineSearchFn(theta, rhoFn, d, lambdaStepsize = lambdaStepsize, lambdaMax = lambdaMax)
    thetaNew <- theta - lambda * d
    converged <- testConvergenceFn(thetaNew, theta, tolerance = tolerance, relative = relative)
    theta <- thetaNew
    i <- i + 1
  }
  ## Return last value and whether converged or not
  list(theta = theta, converged = converged, iteration = i, fnValue = rhoFn(theta))
}

gridLineSearch <- function(theta, rhoFn, d, lambdaStepsize = 0.01, lambdaMax = 1) {
  ## grid of lambda values to search
  lambdas <- seq(from = 0, by = lambdaStepsize, to = lambdaMax)
  ## line search
  rhoVals <- sapply(lambdas, function(lambda) {
    rhoFn(theta - lambda * d)
  })
  ## Return the lambda that gave the minimum
  lambdas[which.min(rhoVals)]
}

testConvergence <- function(thetaNew, thetaOld, tolerance = 1e-10, relative = FALSE) {
  sum(abs(thetaNew - thetaOld)) < if (relative)
  tolerance * sum(abs(thetaOld)) else tolerance
}

i <- c(1.1,1.75)
ii <- c(-0.5,0.6)
iii <- c(-0.2,-0.8)
iv <- c(0.1,1.1)
v <- c(0.75,0.6)

gradientDescent(theta = i, rhoFn = rho, gradientFn = gradient, lineSearchFn = gridLineSearch, testConvergenceFn = testConvergence)$theta
gradientDescent(theta = ii, rhoFn = rho, gradientFn = gradient, lineSearchFn = gridLineSearch, testConvergenceFn = testConvergence)$theta
gradientDescent(theta = iii, rhoFn = rho, gradientFn = gradient, lineSearchFn = gridLineSearch, testConvergenceFn = testConvergence)$theta
gradientDescent(theta = iv, rhoFn = rho, gradientFn = gradient, lineSearchFn = gridLineSearch, testConvergenceFn = testConvergence)$theta
gradientDescent(theta = v, rhoFn = rho, gradientFn = gradient, lineSearchFn = gridLineSearch, testConvergenceFn = testConvergence)$theta
```
i) is converging to point C,
ii) is converging to A
iii) is converging to B
iv) is converging to B
v) is converging to C

(c) [5 points] Recreate the contour plot shown above. You may find the functions `outer()`, `image()`, and `contour()` useful for this task. Include on this plot `green` squares at each of the starting points specified in (b) as well as `green` line `segments()` connecting these starting points with their respective points of convergence.

```{r}
y <- seq(-1, 2, by=0.5)
x <- seq(-1.5, 1.5, by=0.5)

z <- outer(X=x, Y=y, FUN=Vectorize(function(x,y) { rho(c(x,y))}))
contour(x,y,z, levels=c(1,10,50,150,350,700),xlab=expression(theta[1]),ylab=expression(theta[2]))

rect(xleft=1.075,xright=1.1+0.025,ytop=1.72,ybottom=1.78,col="green")
rect(xleft=(-0.5-0.025),xright=-0.5+0.025,ytop=0.6-0.06,ybottom=0.6+0.06,col="green")
rect(xleft=(-0.2-0.025),xright=-0.2+0.025,ytop=-0.8-0.06,ybottom=-0.8+0.06,col="green")
rect(xleft=(0.1-0.025),xright=0.1+0.025,ytop=1.1-0.06,ybottom=1.1+0.06,col="green")
rect(xleft=0.75-0.025,xright=0.75+0.025,ytop=0.6-0.06,ybottom=0.6+0.06,col="green")
segments(x0=1.1,y0=1.75,x1=0.9999632,y1=1.0079989,col="green",lwd=1)
segments(x0=-0.5,y0=0.6,x1=-0.6628252,y1=0.4375563,col="green",lwd=1)
segments(x0=-0.2,y0=-0.8,x1=0.3455740,y1=0.1131815,col="green",lwd=1)
segments(x0=0.1,y0=1.1,x1=0.3469271,y1=0.1139806,col="green",lwd=1)
segments(x0=0.75,y0=0.6,x1=0.9975885,y1=0.9996567,col="green",lwd=1)
```

(d) [2 points] Based on what you found in part (b), and visualized in part (c), explain the importance of the starting value when performing non-convex optimization (when locating a global optimum is desired).

Since the gradient descent algorithm converges towards its local minima, the starting value when performing non-convex optimization may not head towards the global minima if a local minima is closer.

(e) [4 points]  Repeat the process from part (b) for an additional starting value: $\widehat{\boldsymbol{\theta}}_0=(-1,-0.5)$. Describe what occurs when using the `gradientDescent()` function with this starting value, where the inputs `maxIterations`, `tolerance`, `relative`, `lambdaStepsize`, and `lambdaMax` are set to their default values from the course notes. Try to obtain better performance by adjusting *one* of the inputs in the previous sentence from its default value. Which input can be tweaked to yield better performance, and why is this input important for gradient descent methods?

```{r}
gradientDescent(theta = c(-1, -0.5), rhoFn = rho, gradientFn = gradient, lineSearchFn = gridLineSearch, testConvergenceFn = testConvergence)$theta
gradientDescent(theta = c(-1, -0.5), rhoFn = rho, gradientFn = gradient, lineSearchFn = gridLineSearch, testConvergenceFn = testConvergence,lambdaStepsize=0.001)$theta
```

I modified the lambdaStepsize to be 0.001 instead of 0.01 and the convergence yielded better performance by providing a value much closer to the global minima B. This input is important for gradient descent methods, because it adjusts how many lambas we test for in each line search.
\newpage

## QUESTION 3: Robust Regression with Mind Monitor Data [23 points]

For this question, you have the data from a collection of brainwave reading sessions that were collected from an informed, consenting participant. These were collected by using a Muse 2 headband, and the Mind Monitor phone app. Both the headband and app are pictured below.

\begin{center}
\end{center}

\begin{center}
\end{center}

Several times per second, this headband collects electrical signal information and translates it into strength of different brainwaves at four locations (TP9, TP10 by the ears, and AF7, AF8 on the forehead). It also records head movements through an accelerometer and a gyroscope. These headbands are usually used for guided meditation by playing one sound when the user's brainwaves are in a state of deep relaxation, and another sound when they are far from that state. In this case, the study participant wore the headband while working at a computer (active), napping (resting), or doing other activities like watching TV.

The Mind Monitor app creates a detailed data set `eeg_all.csv`, with measurements processed several times per second. The code below is provided to illustrate how the variables in the detailed data set (not provided) are transformed into a summary data set, which is provided in the `eeg_summary.csv` file.

```{r, eval=FALSE, tidy=FALSE}
library(plyr)

eeg_all = read.csv("eeg_all.csv")

eeg_summary = 
ddply(eeg_all, "sessionnum", summarise,
      activity = activity[1],
      session_time = session_time[1],
      mean_alpha = mean(total_alpha, na.rm=TRUE),
      mean_beta = mean(total_beta, na.rm=TRUE),
      mean_gamma = mean(total_gamma, na.rm=TRUE),
      mean_delta = mean(total_delta, na.rm=TRUE),
      mean_theta = mean(total_theta, na.rm=TRUE),
      
      var_alpha = var(total_alpha, na.rm=TRUE),
      var_beta =  var(total_beta, na.rm=TRUE),
      var_gamma = var(total_gamma, na.rm=TRUE),
      var_delta = var(total_delta, na.rm=TRUE),
      var_theta = var(total_theta, na.rm=TRUE),
      
      blinks_minute = Nblinks[1]/session_time[1]*60,
      jaws_minute = Njaw[1]/session_time[1]*60,
      mean_pos_xy = mean(sqrt(Accelerometer_X^2 + Accelerometer_Y^2), na.rm=TRUE),
      mad_accel = mean(abs(Accelerometer_X^2 + Accelerometer_Y^2 + Accelerometer_Z^2 - 1), 
                       na.rm=TRUE),
      rmse_gyro = mean(sqrt(Gyro_X^2 + Gyro_Y^2 + Gyro_Z^2), na.rm=TRUE)
)
```

The summary data set condenses each 20-80 minute session into a single row of summary statistics. The summary data set has the following data dictionary. You will be analyzing the summary data set for this question.

| Variable      | Description                                                                     |
|---------------|---------------------------------------------------------------------------------|
| `sessionnum`    | unique ID for that session                                                      |
| `activity`      | main activity taken during session 'active' means various work or social things |
| `session_time`  | length of session in seconds                                                    |
| `mean_alpha`    | alpha wave activity, totaled across all sensors, average over time             |
| `var_alpha`     | alpha wave activity, totaled across all sensors, variance over time            |
| `blinks_minute` | recorded blink events per minute                                                |
| `jaws_minute`   | recorded jaw clench events per minute                                           |
| `mean_pos_xy`   | horitzonalness of head position, averaged over time. 0 = upright, 1 = laying    |
| `mad_accel`     | mean absolute deviation from zero acceleration, in g. Higher = more movement    |
| `rmse_gyro`     | variance in gyroscope movements, in degrees/sec. Higher = more rotation         |

In the questions that follow, you will consider modeling the relationship between $x=$ `blinks_minute` and $y=$ `jaws_minute` with the following simple linear regression: $$y_u = \alpha +  \beta x_u + r_u, ~~~ u \in \mathcal{P}$$

(a) [3 points] Construct a scatter plot of `blinks_minute` versus `jaws_minute`, and add the least squares regression line to this plot. Note that you may use the `lm` function to determine the equation of this line. Be sure to add a title and informative axis labels.

```{r}
eeg_summary <- read.csv("eeg_summary.csv")

x <- eeg_summary$blinks_minute
y <- eeg_summary$jaws_minute

plot(x=x, y=y, main="Blinks/min vs Jaws/min", xlab="Blinks/min",ylab='Jaw Clenches/min')
df <- data.frame(jaws_minute=y, blinks_minute=x)
model <- lm(y ~ x, data=df)
abline(model)
```

(b) [4 points] For each unit in the population, calculate the influence that it has on the fitted regression line from part (a), using the following definition of influence: $$\Delta(\boldsymbol{\theta},u) = ||\widehat{\boldsymbol{\theta}} - \widehat{\boldsymbol{\theta}}_{[-u]}||_2$$ where $\widehat{\boldsymbol{\theta}}=\left(\widehat{\alpha},\widehat{\beta}\right)^T$ are the regression coefficients estimated from all of the data, $\widehat{\boldsymbol{\theta}}_{[-u]}=\left(\widehat{\alpha}_{[-u]},\widehat{\beta}_{[-u]}\right)^T$ are the regression coefficients estimated from all of the data excluding unit $u$, and $||\cdot||_2$ is the Euclidean norm. Construct a scatter plot of all influence values and determine on which `session_num` the two most influential recordings occurred.  

```{r}
theta_hat <- c(coef(model)[1], coef(model)[2])
influences <- c()

for (x in 1:nrow(df)) {
  nou <- df[-x,]
  u <- lm(jaws_minute ~ blinks_minute, data=nou)
  theta_hat_u <- c(coef(u)[1], coef(u)[2])
  influence <- norm((theta_hat - theta_hat_u), type='2')
  influences <- append(influences, influence)
}

x <- 1:nrow(df)
plot(x=x, y=influences, main="Influence vs Session Number", xlab="Session Number",ylab='Influence')
```
The most influential session numbers are 10 and 14.

(c) [4 points] Re-construct the scatter plot from part (a). Remove the two most influential observations from the population and calculate the least squares regression line using the observations that remain (you may use `lm` again). Add this line to the plot in a colour different from what you used on the first line. How would your conclusions about the relationship between `blinks_minute` and `jaws_minute` differ when using the line created in part (a) vs. using the line created in part (c)?

```{r}
new_df <- df[-c(10,14),]

x <- eeg_summary$blinks_minute
y <- eeg_summary$jaws_minute

plot(x=x, y=y, main="Blinks/min vs Jaws/min", xlab="Blinks/min",ylab='Jaw Clenches/min')
abline(model)
x <- eeg_summary$blinks_minute[-c(10,14)]
y <- eeg_summary$jaws_minute[-c(10,14)]
new_model <- lm(y ~ x, df=new_df)
abline(new_model, col="red")
```

Using the line from part a, we can conclude that there is a weak negative correlation between jaws_minute and blinks_minute, but using the data from part c, we conclude that there is no correlation between them instead because the line is flat.

(d) [12 points] Rather than removing highly influential observations prior to analysis, we could instead mitigate their influence by performing a robust linear regression. In class we saw that *robust regression* is an outlier-resistant means to estimate $\boldsymbol{\theta}=(\alpha,\beta)^T$ in the context of the simple linear regression model defined above. We did so using the **Huber objective function** which behaved like the least squares objective function for small (in magnitude) values of $r_u$ but which was less sensitive than the least squares objective function for large (in magnitude) values of $r_u$. Another objective function that similarly facilitates robust regression is the **Hampel objective function**: $$\rho(\boldsymbol{\theta};\mathcal{P}) = \sum_{u \in \mathcal{P}}\rho_{a,b,c}(r_u)$$ where $\boldsymbol{\theta}=(\alpha,\beta)^T$, $r_u = y_u - \alpha - \beta x_u$ and $$\rho_{a,b,c}(r) = \left\{ \begin{array}{ll} \frac{a(b-a+c)}{2}  & \mbox{for } r < -c \\ \frac{-a(cr+r^2/2)}{c-b} - \frac{ab^2}{2(c-b)} - \frac{a^2}{2}  & \mbox{for } -c \le r < -b \\  -ar-\frac{a^2}{2} & \mbox{for } -b \le r < -a \\\frac{r^2}{2} & \mbox{for } -a \le r \le a \\ ar-\frac{a^2}{2} & \mbox{for } a < r \le b \\ \frac{a(cr-r^2/2)}{c-b} - \frac{ab^2}{2(c-b)} - \frac{a^2}{2}  & \mbox{for } b < r \le c \\ \frac{a(b-a+c)}{2}  & \mbox{for } r > c \end{array} \right.$$ This function, for $a=2,b=4,c=8$, is shown below. In the questions that follow you will fit a robust linear regression using Hampel's objective function.

```{r, echo=FALSE, fig.align='center', fig.width=4.5, fig.height=4.5}
hampel.fn <- function(r, a,b,c){
  val <- rep(a*(b-a+c)/2, length(r))
  val[r > -c & r<= -b] <- -a * (c*r[r > -c & r<= -b] + (0.5*r^2)[r > -c & r<= -b]) / (c-b) - 0.5*(a*b^2)/(c-b) - 0.5*a^2
  val[r > -b & r<= -a] <- -a * r[r > -b & r<= -a] - 0.5*a^2
  val[r > -a & r<= a] <- (0.5*r^2)[r > -a & r<= a]
  val[r > a & r<= b] <- a * r[r > a & r<= b] - 0.5*a^2
  val[r > b & r<= c] <- a * (c*r[r > b & r<= c] - (0.5*r^2)[r > b & r<= c]) / (c-b) - 0.5*(a*b^2)/(c-b) - 0.5*a^2
  return(val)
}

plot(x = seq(-10,10,0.1), 
     y = hampel.fn(r = seq(-10,10,0.1), a = 2, b = 4, c = 8), type = "l", 
     xlab = "r", ylab = bquote(rho["a,b,c"]*"(r)"), 
     main = bquote("Hampel's" ~ rho))
```

i. [2 points] By taking appropriate derivatives, determine the $2 \times 1$ gradient vector $\boldsymbol{g} =  \nabla\rho(\boldsymbol{\theta};\mathcal{P})$. Show your work. 
$$
\frac{d\rho}{d\alpha} \frac{-a(cr+r^2/2)}{c-b}
$$
$$
= \frac{d\rho}{d\alpha}\frac{-acr + \frac{ar^2}{2}}{c-b}
$$
$$
= \frac{d\rho}{d\alpha} \frac{1}{c-b}(\frac{ar^2}{2}-acr)
$$
$$
= \frac{d\rho}{d\alpha} \frac{1}{c-b}(\frac{a(y - \alpha - \beta x)^2}{2} - ac(y - \alpha - \beta x))
$$
$$
= \frac{1}{c-b}\frac{d\rho}{d\alpha}\frac{a(y - \alpha - \beta x)^2}{2} - \frac{d\rho}{d\alpha}ac(y - \alpha - \beta x)
$$
$$
= \frac{a(\alpha - y + \beta x)+ac}{c-b}, \; \text{for } -c \leq r < -b
$$

$$
\frac{1}{c-b}\frac{d\rho}{d\beta}\frac{a(y - \alpha - \beta x)^2}{2} - \frac{d\rho}{d\beta}ac(y - \alpha - \beta x)
$$
$$
= \frac{acx - ax(y-\alpha-\beta x)}{c-b}, \; \text{for } -c \leq r < -b
$$
$$
\frac{d\rho}{d\alpha}-a(y - \alpha - \beta x) = a,\;\text{for }-b\leq r< -a
$$
$$
\frac{d\rho}{d\beta}-a(y - \alpha - \beta x) = ax, \text{for } =b \leq r < -a
$$

$$
\frac{d\rho}{d\alpha}\frac{(y-\alpha-\beta x)^2}{2} = \alpha-y+\beta x, \text{for }-a\leq r \leq a
$$

$$
\frac{d\rho}{d\beta}\frac{(y-\alpha-\beta x)^2}{2} = \alpha x - yx + \beta x^2,\text{for } -a\leq r \leq a
$$

$$
\frac{d\rho}{d\alpha} a(y-\alpha-\beta x) = -a,\text{for }a<r\leq b
$$

$$
\frac{d\rho}{d\beta} a(y - \alpha - \beta x) = -ax, \text{for } a < r \leq b
$$

$$
\frac{d\rho}{d\alpha}\frac{a(cr - r^2/2)}{c-b} = \frac{d\alpha}{d\beta}\frac{acr-\frac{ar^2}{2}}{c-b}
$$
$$
= \frac{1}{c-b}\frac{d\rho}{d\alpha}ac(y-\alpha-\beta x)-\frac{a}{2}\frac{d\rho}{d\alpha}(y-\alpha-\beta x)^2
$$
$$
= \frac{a(y-\alpha-\beta x)-ac}{c-b},\text{for } b<r\leq c
$$

$$
\frac{1}{c-b}\frac{d\rho}{d\beta}ac(y-\alpha-\beta x)-\frac{a}{2}\frac{d\rho}{d\beta}(y-\alpha-\beta x)^2
$$
$$
= \frac{ax(y-\alpha-\beta x) -acx}{c-b},\text{for } b<r\leq c
$$
$$
f_1=\begin{cases}
0 &\ \text{for }r<-c \\
\frac{ac - ar}{c-b} &\ \text{for }-c\leq r<-b \\
a &\ \text{for } -b\leq r<-a \\
-r &\ \text{for } -a\leq r\leq a \\
-a &\ \text{for } a<r\leq b \\
\frac{ar-ac}{c-b} &\ \text{for }b<r\leq c \\
0 &\ \text{for } r>c
\end{cases}
$$
$$
f_2 =\begin{cases}
0 &\ \text{for }r<-c \\
\frac{acx - axr}{c-b} &\ \text{for }-c\leq r<-b \\
ax &\ \text{for } -b\leq r<-a \\
-arx &\ \text{for } -a\leq r\leq a \\
-ax &\ \text{for } a<r\leq b \\
\frac{axr-ac}{c-b} &\ \text{for }b<r\leq c \\
0 &\ \text{for } r>c
\end{cases}
$$
$$
g = \nabla\rho(\theta;P) = \begin{bmatrix}
f1 \\
f2
\end{bmatrix}
$$
ii. [4 points] Write *factory functions* `createRobustHampelRho(x, y, aval, bval, cval)` and `createRobustHampelGradient(x, y, aval, bval, cval)` which take in as inputs the data and values for the constants $a,b,c$, and which respectively return as output the Hampel objective function and the corresponding gradient function. **Hint:** Use the `createRobustHuberRho` and `createRobustHuberGradient` functions from the lecture notes as a guide. You may also use the following two functions as necessary.

```{r}
hampel.fn <- function(r, a, b, c){
  val <- rep(a*(b-a+c)/2, length(r))
  val[r > -c & r<= -b] <- -a * (c*r[r > -c & r<= -b] + (0.5*r^2)[r > -c & r<= -b]) / (c-b) - 0.5*(a*b^2)/(c-b) - 0.5*a^2
  val[r > -b & r<= -a] <- -a * r[r > -b & r<= -a] - 0.5*a^2
  val[r > -a & r<= a] <- (0.5*r^2)[r > -a & r<= a]
  val[r > a & r<= b] <- a * r[r > a & r<= b] - 0.5*a^2
  val[r > b & r<= c] <- a * (c*r[r > b & r<= c] - (0.5*r^2)[r > b & r<= c]) / (c-b) - 0.5*(a*b^2)/(c-b) - 0.5*a^2
  return(val)
}
hampel.fn.prime <- function(r, a, b, c){
  val <- rep(0, length(r))
  val[r > -c & r<= -b] <- -a * (c+r[r > -c & r<= -b]) / (c-b)
  val[r > -b & r<= -a] <- -a
  val[r > -a & r<= a] <- r[r > -a & r<= a]
  val[r > a & r<= b] <- a
  val[r > b & r<= c] <- a * (c-r[r > b & r<= c]) / (c-b)
  return(val)
}

createRobustHampelRho <- function(x, y, a, b, c) {
  ## local variable
  xbar <- mean(x)
  ## Return this function
  function(theta) {
    alpha <- theta[1]
    beta <- theta[2]
    sum(hampel.fn(y - alpha - beta * (x - xbar), a=a,b=b,c=c))
  }
}

createRobustHampelGradient <- function(x, y, a, b, c) {
  ## local variables
  xbar <- mean(x)
  ybar <- mean(y)
  function(theta) {
    alpha <- theta[1]
    beta <- theta[2]
    ru = y - alpha - beta * (x - xbar)
    rhok = hampel.fn.prime(ru, a=a,b=b,c=c)
    -1 * c(sum(rhok * 1), sum(rhok * (x - xbar)))
  }
}
```

iii. [2 point] Using the `nlminb` function with the `rho` and `gradient` functions created by your factory functions from part ii., find $\widehat{\boldsymbol{\theta}} = \left(\widehat\alpha, \widehat\beta \right)^T$, the solution to $$\operatorname*{arg min}_{\boldsymbol\theta \in \mathbb{R}^2}\rho(\boldsymbol{\theta};\mathcal{P}).$$ Start the optimization at the least squares estimates of $\alpha$ and $\beta$ determined in part (a) and use $a=2,b=4,c=8$. For full points be sure to include the output from the `nlminb` function. **Hint:** Your robust regression estimate for $\beta$ should take a value between the two least squares estimates for $\beta$ found in parts (a) and (c).
```{r}
nlminb(model,objective=createRobustHampelRho,gradient=createRobustHampelGradient,a=2,b=4,c=8)
```

iv. [4 points] Re-construct the scatter plot from part (c). In a third colour, add the Hampel regression line that you calculated in part iii. Be sure to include a legend that distinguishes among the three lines. Does the robust regression line convey similar findings about the relationship between `blinks_minute` and `jaws_minute` as the other two? With that in mind, what does this suggest about how influential units impact the reliability of our conclusions?
