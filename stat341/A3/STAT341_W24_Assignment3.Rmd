---
title: "STAT 341: Assignment 3"
subtitle: 'DUE: Friday, March 22, 2024 by 5:00pm EDT'
output:
  pdf_document: default
  word_document: default
  html_document: default
urlcolor: blue
---
```{r setup, include = FALSE}
knitr::opts_chunk$set(echo = TRUE, warning = FALSE)
```

```{r, include = FALSE}
library(knitr)
#opts_chunk$set(tidy.opts=list(width.cutoff=70),tidy=TRUE)
```

$\;$
$\;$
$\;$
$\;$

## NOTES

Your assignment must be submitted by the due date listed at the top of this document, and it must be submitted electronically in .pdf format via Crowdmark. This means that your responses for different question parts should begin on separate pages of your .pdf file. Note that your .pdf solution file must have been generated by R Markdown. Additionally:

* For mathematical questions: your solutions must be produced by LaTeX (from within R Markdown). Neither screenshots nor scanned/photographed handwritten solutions will be accepted -- these will receive zero points.

* For computational questions: R code should always be included in your solution (via code chunks in R Markdown). If code is required and you provide none, you will receive zero points.
   + **Exception** any functions used in the notes or function glossary can loaded using `echo=FALSE` but any other code chunks should have `echo=TRUE`. e.g. the code chuck loading `NewtonRaphson` can use `echo=FALSE` but chunks that call `NewtonRaphson` should have `echo=TRUE`.

* For interpretation questions: plain text (within R Markdown) is required. Text responses embedded as comments within code chunks will not be accepted.

Organization and comprehensibility is part of a full solution. Consequently, points will be deducted for solutions that are not organized and incomprehensible. Furthermore, if you submit your assignment to Crowdmark, but you do so incorrectly in any way (e.g., you upload your Question 2 solution in the Question 1 box), you will receive a 5% deduction (i.e., 5% of the assignmentâ€™s point total will be deducted from your point total).


\newpage


## QUESTION 1: Parameter Estimation for the Log-Normal Distribution [33 points]

The log-normal distribution is a two-parameter continuous distribution useful for right-skewed data. The distribution $\text{LNORM}(\mu,\tau)$ is governed by a *log-mean* parameter $\mu$ and a *log-variance* parameter $\tau$, with probability density function given by $$f(y;\mu,\tau) =\frac{1}{y\sqrt{2\pi\tau}}\text{exp}\left(\dfrac{-(\text{log}~y - \mu)^2}{2\tau}\right)$$ for $y>0$, $\mu \in \mathbb{R}$, and $\tau>0$, and where $\text{log}(\cdot)$ is the natural logarithm. This density function, for several different values of $\mu$ and $\tau$, is visualized below.

```{r, fig.align="center", fig.height=5, fig.width=5, echo=FALSE}
plot(x = seq(0,8,0.005), y = dlnorm(x = seq(0,8,0.005), meanlog = 0, sdlog = 1), type = "l", col = "red", xlab = "y", ylab = "", ylim = c(0, 1.5), main = expression(paste("LNORM(\u03BC, ",tau,")")), cex.main = 0.75, cex.lab = 0.8)
text(x = 6.875, y = 1.4, bquote("\u03BC = 0,"~tau~"= 1"), col = "red", pos = 3, cex = 0.75)
lines(x = seq(0,8,0.005), y = dlnorm(x = seq(0,8,0.005), meanlog = 0, sdlog = sqrt(0.1)), col = "blue")
text(x = 6.875, y = 1.3, bquote("\u03BC = 0,"~tau~"= 0.1"), col = "blue", pos = 3, cex = 0.75)
lines(x = seq(0,8,0.005), y = dlnorm(x = seq(0,8,0.005), meanlog = -0.5, sdlog = 1), col = "darkgreen")
text(x = 6.875, y = 1.2, bquote("\u03BC = -0.5,"~tau~"= 1"), col = "darkgreen", pos = 3, cex = 0.75)
lines(x = seq(0,8,0.005), y = dlnorm(x = seq(0,8,0.005), meanlog = 0.5, sdlog = sqrt(1.5)), col = "darkorchid")
text(x = 6.875, y = 1.1, bquote("\u03BC = 0.5,"~tau~"= 1.5"), col = "darkorchid", pos = 3, cex = 0.75)
lines(x = seq(0,8,0.005), y = dlnorm(x = seq(0,8,0.005), meanlog = 1, sdlog = sqrt(0.05)), col = "orange")
text(x = 6.875, y = 1, bquote("\u03BC = 1,"~tau~"= 0.05"), col = "orange", pos = 3, cex = 0.75)
```

In this question, you will fit a log-normal distribution to data from a [Netflix engagement report](https://about.netflix.com/en/news/what-we-watched-a-netflix-engagement-report) by estimating $\mu$ and $\tau$ under a variety of conditions. This Netflix engagement report is summarized for the 500 most-watched movies and series between January and June 2023 in the `netflix.csv` file; its variates are described in the table below.

| Variable               | Description                                                 |
|------------------------|-------------------------------------------------------------|
| `title`              | The title of the movie or series. |
| `available_globally`      | A binary variate that denotes if the movie or series is available globally.         |
| `release_date`       | The release date of the movie or series.                |
| `hours_viewed`  | The number of hours that the movie or series was viewed.                |
| `movie`      | A binary variate that denotes if the content is a movie (Yes) or series (No).                        |

(a) [4 points] The population $\mathcal{P}=\{y_1, y_2, \ldots, y_N\}$ for this question is the subpopulation of the 500 titles *that are movies*. The variate of interest $y$ is the watch time for each movie, expressed in *millions of hours*. Construct a *density* histogram of $y$, being sure to include an informative title and axis labels. Comment on the suitability of the log-normal distribution as a potential model for these data.

```{r}
netflix <- read.csv("netflix.csv")

movies <- subset(netflix, movie == "Yes")
hours <- sapply(movies$hours_viewed, FUN = function(x) {
  x / 1000000
})
hist(hours, main="Watch time for Movies", xlab="Movie", ylab="Density",prob=TRUE)
```
A log-normal distribution is suitable for this data because it is heavily
right-skewed

(b) [2 points] For a probability distribution involving two parameters, the Method of Moments (MM) estimation procedure involves solving (for the unknown parameters) the system of equations that arises by equating the distribution's first and second moments with their corresponding sample moments: $$\text{E}[Y] = \frac{1}{N}\sum_{i=1}^Ny_i ~~~ \text{and}~~~\text{E}[Y^2] = \frac{1}{N}\sum_{i=1}^Ny^2_i.$$ 
The method of moments estimates for the log-normal distribution are
$$\hat\mu_{MM} = 2 \log M_1 - \dfrac{1}{2}\log M_2 ~~~~~ \text{and} ~~~~~ \hat\tau_{MM} = \log M_2 - 2 \log M_1,$$
where $M_1 = \sum_{i = 1}^Ny_i/N$ and $M_2 = \sum_{i = 1}^Ny_i^2/N$. Use R and these expressions to calculate the MM estimates of $\mu$ and $\tau$ for the watch time data $y$ from part (a).

```{r}
M1 <- sum(hours)/length(hours)
M2 <- sum(hours^2)/length(hours)

mu <- 2*log(M1) - 1/2*log(M2)
tau <- log(M2) - 2 * log(M1)

print(paste0("mu: ", mu))
print(paste0("tau: ", tau))
```

(c) [2 points] Maximum Likelihood (ML) estimation is another approach for using data to estimate a distribution's unknown parameters. With this method, a likelihood function is maximized in order to determine which parameter values are most consistent with the observed data, assuming the underlying model is correct. The likelihood function for the $\text{LNORM}(\mu, \tau)$ distribution is given by $$L(\mu,\tau;\mathcal{P}) = \prod_{i=1}^Nf(y_i;\mu,\tau) = \left[\dfrac{1}{\prod_{i=1}^Ny_i}\right]\times\left[\dfrac{1}{\sqrt{2\pi\tau}}\right]^N\times\exp\left(\dfrac{-\sum_{i = 1}^N(\log y_i - \mu)^2}{2\tau}\right).$$ Derive the log-likelihood function $l(\mu,\tau;\mathcal{P})$.

$$l(\mu,\tau,\mathcal{P}) = \ln\left(L(\mu,\tau,\mathcal{P})\right)$$
$$l(\mu,\tau,\mathcal{P}) = \ln \left( \left[\dfrac{1}{\prod_{i=1}^Ny_i}\right]\times\left[\dfrac{1}{\sqrt{2\pi\tau}}\right]^N\times\exp\left(\dfrac{-\sum_{i = 1}^N(\log y_i - \mu)^2}{2\tau}\right)\right)$$
$$l(\mu,\tau,\mathcal{P}) = \ln \left[\dfrac{1}{\prod_{i=1}^Ny_i}\right] + \ln\left[\dfrac{1}{\sqrt{2\pi\tau}}\right]^N + \ln\left(\exp\left(\dfrac{-\sum_{i = 1}^N(\log y_i - \mu)^2}{2\tau}\right)\right)$$
$$l(\mu,\tau,\mathcal{P}) = \ln \left[\dfrac{1}{\prod_{i=1}^Ny_i}\right] + \ln\left[\left(\dfrac{1}{2\pi\tau}\right)^{1/2}\right]^N + \ln\left(\exp\left(\dfrac{-\sum_{i = 1}^N(\log y_i - \mu)^2}{2\tau}\right)\right)$$
$$l(\mu,\tau,\mathcal{P}) = \ln \left[\dfrac{1}{\prod_{i=1}^Ny_i}\right] + \frac{N}{2}\ln\left(\dfrac{1}{2\pi\tau}\right) - \dfrac{\sum_{i = 1}^N(\log y_i - \mu)^2}{2\tau}$$
$$
\ln(\mu,\tau,\mathcal{P})=\sum_{i=1}^N\ln{\frac{1}{y_i}} + \frac{N}{2}\ln\Big(\frac{1}{2\pi\tau}\Big)-\frac{\sum_{i=1}^N(\log y_i-\mu)^2}{2\tau}
$$

(d) [4 points] In order to determine the ML estimates of $\mu$ and $\tau$, we must simultaneously solve $$\frac{\partial l(\mu,\tau;\mathcal{P})}{\partial\mu}=0 ~~~~~ \text{and} ~~~~~ \frac{\partial l(\mu,\tau;\mathcal{P})}{\partial\tau}=0.$$ Determine the partial derivatives $\frac{\partial l(\mu,\tau;\mathcal{P})}{\partial\mu}$ and $\frac{\partial l(\mu,\tau;\mathcal{P})}{\partial\tau}$.

$$\frac{\partial l(\mu,\tau;\mathcal{P})}{\partial\mu} = -\frac{\partial}{\partial\mu} \dfrac{\sum_{i = 1}^N(\log y_i - \mu)^2}{2\tau} = \dfrac{\sum_{i = 1}^N(\log y_i - \mu)}{\tau}$$
$$\frac{\partial l(\mu,\tau;\mathcal{P})}{\partial\tau} = \frac{\partial}{\partial\tau} \left(\frac{N}{2}\ln\left(\dfrac{1}{2\pi\tau}\right) - \dfrac{\sum_{i = 1}^N(\log y_i - \mu)^2}{2\tau}\right) = \frac{N}{2}(2\pi\tau)\left(\frac{-1}{2\pi\tau^2}\right) + \dfrac{\sum_{i = 1}^N(\log y_i - \mu)^2}{2\tau^2} = \frac{-N}{2\tau} + \dfrac{\sum_{i = 1}^N(\log y_i - \mu)^2}{2\tau^2} $$

(e) [5 points] Determine the second partial derivatives $\frac{\partial^2 l(\mu,\tau;\mathcal{P})}{\partial\mu^2}$, $\frac{\partial^2 l(\mu,\tau;\mathcal{P})}{\partial\mu\partial\tau}$, $\frac{\partial^2 l(\mu,\tau;\mathcal{P})}{\partial\tau\partial\mu}$, and $\frac{\partial^2 l(\mu,\tau;\mathcal{P})}{\partial\tau^2}$. 

$$\frac{\partial^2 l(\mu,\tau;\mathcal{P})}{\partial\mu^2} = \frac{\partial}{\partial\mu} \dfrac{\sum_{i = 1}^N(\log y_i - \mu)}{\tau} = \dfrac{\sum_{i = 1}^N-1}{\tau} = \frac{-N}{\tau}$$
$$\frac{\partial^2 l(\mu,\tau;\mathcal{P})}{\partial\mu\partial\tau} = \frac{\partial}{\partial\tau} \dfrac{\sum_{i = 1}^N(\log y_i - \mu)}{\tau} = -\dfrac{\sum_{i = 1}^N(\log y_i - \mu)}{\tau^2}$$
$$\frac{\partial^2 l(\mu,\tau;\mathcal{P})}{\partial\tau\partial\mu} = \frac{\partial}{\partial\mu} \left(\frac{-N}{2\tau} + \dfrac{\sum_{i = 1}^N(\log y_i - \mu)^2}{2\tau^2}\right) = -2 \dfrac{\sum_{i = 1}^N(\log y_i - \mu)}{2\tau^2} = -\dfrac{\sum_{i = 1}^N(\log y_i-\mu)}{\tau^2}$$
$$\frac{\partial^2 l(\mu,\tau;\mathcal{P})}{\partial\tau^2} = \frac{\partial}{\partial\tau} \left(\frac{-N}{2\tau} + \dfrac{\sum_{i = 1}^N(\log y_i - \mu)^2}{2\tau^2}\right) = \frac{N}{2\tau^2} - \dfrac{\sum_{i = 1}^N(\log y_i - \mu)^2}{\tau^3}$$

(f) [2 points] Using the derivatives found in parts (d) and (e), define the vector $\boldsymbol\psi(\mu,\tau;\mathcal{P})$ and the matrix $\boldsymbol\psi'(\mu,\tau;\mathcal{P})$ required for the Newton-Raphson algorithm. Note this is a LaTeX (not an R) question.

$$\boldsymbol\psi(\mu,\tau;\mathcal{P}) = \begin{bmatrix} \psi_1 \\ \psi_2 \end{bmatrix} = \begin{bmatrix}  \dfrac{\sum_{i = 1}^N(\log y_i - \mu)}{\tau} \\ \frac{-N}{2\tau} + \dfrac{\sum_{i = 1}^N(\log y_i - \mu)^2}{2\tau^2} \end{bmatrix}$$
$$\boldsymbol\psi'(\mu,\tau;\mathcal{P}) = \begin{bmatrix}  \frac{-N}{\tau} & -\dfrac{\sum_{i = 1}^N(\log y_i - \mu)}{\tau^2} \\ -\dfrac{\sum_{i = 1}^N(\log y_i - \mu)}{\tau^2} & \frac{N}{2\tau^2} - \dfrac{\sum_{i = 1}^N(\log y_i - \mu)^2}{\tau^3} \end{bmatrix}$$

(g) [4 points] Write a single *factory function* `createLogNormPsiFns(y)` which takes in as input the data `y`, and which returns as output *both* functions corresponding to $\boldsymbol\psi(\mu,\tau;\mathcal{P})$ and $\boldsymbol\psi'(\mu,\tau;\mathcal{P})$ defined in part (f). 

```{r}
createLogNormPsiFns <- function(y) {
  N <- length(y)
  psi <- function(theta) {
    mu <- theta[1]
    tau <- theta[2]
    c(sum(log(y) - mu)/tau, -N/(2*tau) + sum((log(y) - mu)^2)/(2*tau^2))
  }
  psiPrime <- function(theta) {
    mu <- theta[1]
    tau <- theta[2]
    val = matrix(0, nrow=2, ncol=2)
    val[1,1] = -N/tau
    val[1,2] = -sum(log(y)-mu)/(tau^2)
    val[2,1] = -sum(log(y)-mu)/(tau^2)
    val[2,2] = N/(2*tau^2) - sum((log(y) - mu)^2)/(tau^3)
    return(val)
  }
  list(psi=psi, psiPrime=psiPrime)
}
```

(h) [4 points] The `NewtonRaphsonPrint` function below modifies the `NewtonRaphson` function from class to print the current estimate for $\boldsymbol{\theta}$ at each iteration of the algorithm for this example. Using `NewtonRaphsonPrint` with the `psiFn` and `psiPrimeFn` functions created by your factory function from part (g), attempt to calculate the ML estimates of $\mu$ and $\tau$ associated with the movie watch time data. Start the algorithm at $(\mu_0,\tau_0)$ where $\mu_0$ and $\tau_0$ are the MM estimates of $\mu$ and $\tau$ you calculated in part (b). Do not change `maxIterations` from its default value of 10. Interpret the output from `NewtonRaphsonPrint` and describe whether the printed estimates for $\mu$ and $\tau$ are sensible.

```{r}
NewtonRaphsonPrint <- function(theta, psiFn, psiPrimeFn, dim, testConvergenceFn = testConvergence, 
                          maxIterations = 10, tolerance = 1e-06, relative = FALSE) {
  if (missing(theta)) {
    ## need to figure out the dimensionality
    if (missing(dim)) {
      dim <- length(psiFn())
    }
    theta <- rep(0, dim)
  }
  converged <- FALSE
  i <- 0
  while (!converged & i <= maxIterations) {
    ## added line to output estimates at each iteration
    print(paste0("Iteration ", i, ": Estimate for mu is ", round(theta[1],4), 
                 ", and estimate for tau is ", round(theta[2], 4), "."))
    thetaNew <- theta - solve(psiPrimeFn(theta), psiFn(theta))
    converged <- testConvergenceFn(thetaNew, theta, tolerance = tolerance, 
                                   relative = relative)
    
    theta <- thetaNew
    i <- i + 1
  }
  ## Return last value and whether converged or not
  list(theta = theta, converged = converged, iteration = i, fnValue = psiFn(theta))
}
testConvergence <- function(thetaNew, thetaOld, tolerance = 1e-10, relative = FALSE) {
  sum(abs(thetaNew - thetaOld)) < if (relative)
  tolerance * sum(abs(thetaOld)) else tolerance
}
psiFns <- createLogNormPsiFns(y=hours)
psi <- psiFns$psi
psiPrime <- psiFns$psiPrime
theta <- c(mu, tau)
NewtonRaphsonPrint(theta, psi, psiPrime, dim=2)
```
Our newton-raphson function does not converge after 10 iterations. The printed
estimate for $\mu$ is sensible, and does not vary far from the original estimate.
However, our $\tau$ value does not make sense because log-variance cannot be
negative.

(i) [3 points] Using `NewtonRaphsonPrint` with the `psiFn` and `psiPrimeFn` functions created by your factory function from part (g), calculate $\widehat\mu$ and $\widehat\tau$, the ML estimates of $\mu$ and $\tau$ associated with the movie watch time data. This time, start the algorithm at $(\mu_0,0.3)$, where $\mu_0$ is the MM estimate of $\mu$ you calculated in part (b). For full points, include the output from the `NewtonRaphsonPrint` function. Does this output make more sense than the output from part (h)? In one sentence, justify your answer. 

```{r}
theta <- c(mu, 0.3)
NewtonRaphsonPrint(theta,psi,psiPrime,dim=2)
```
This output makes much more sense than from part (h) because our log-variance,
$\tau$ has a sensible value (it is positive).

(j) [3 points] Reconstruct the density histogram of watch times from part (a) and overlay the ML-estimated log-normal probability density curve for the data. Be sure to ensure the horizontal axis spans the interval from $0$ to $y_{(N)}$ and include an informative title and axis labels.

```{r}
hist(hours, main="Watch time for Movies", xlab="Movie", ylab="Density",prob=TRUE,
     ylim=c(0,0.025))
curve(dlnorm(x, meanlog=mu, sdlog=tau),from=0,to=250,add=TRUE)
```

\newpage

## QUESTION 2: Exploration of Sampling Distributions with IRLS [17 points]

In the course notes, we considered the following simple linear regression equation in various contexts:
$$y_u = \alpha + \beta \left( x_u - c \right) + r_u \qquad \forall \; u \in \mathcal{P}.$$
All $x$ variates in the course notes were continuous, but linear regression is also useful when $x_u \in \{0,1\}$. If $x_u$ is binary and $c = 0$, $\alpha$ provides a measure of location for $y_u$ when $x_u = 0$ and $\alpha + \beta$ provides a measure of location for $y_u$ when $x_u = 1$. For least squares (LS) regression, the argmin $\widehat{\boldsymbol{\theta}}_{LS}=\big(\widehat{\alpha}_{LS},\widehat{\beta}_{LS}\big)^T$ of $\rho(\boldsymbol{\theta};\mathcal{P}) = \sum_{u \in \mathcal{P}}\rho(r_u) = \sum_{u \in \mathcal{P}} r_u^2$ is such that $$\widehat{\alpha}_{LS}= \dfrac{\sum_{u \in \mathcal{P}} y_u \mathbb{I}(x_u = 0)}{\sum_{u \in \mathcal{P}} \mathbb{I}(x_u = 0)} ~~~ \text{and}~~~  \widehat{\beta}_{LS}= \dfrac{\sum_{u \in \mathcal{P}} y_u \mathbb{I}(x_u = 1)}{\sum_{u \in \mathcal{P}} \mathbb{I}(x_u = 1)}- \widehat{\alpha}_{LS}.$$ 
That is, $\widehat{\alpha}_{LS}$ is the mean of the $y$ variates for the subpopulation where $x_u = 0$, and $\widehat{\alpha}_{LS} + \widehat{\beta}_{LS}$ is the mean of the $y$ variates for the subpopulation where $x_u = 1$. For robust regression methods, $\widehat{\alpha}$ and $\widehat{\alpha} + \widehat{\beta}$ provide measures of location for the corresponding subpopulations that are more resistant to outliers.

You will now reuse the Netflix engagement data. In this question, $y_u$ is the watch time for title $u$ expressed in millions of hours, and $x_u$ is the `movie` variate (i.e., $x_u = 1$ if the title $u$ is a movie and $x_u = 0$ if the title $u$ is a series). You will use these variates to obtain robust measures of location via regression with the Huber loss function and iteratively reweighted least squares (IRLS). You will also consider taking many samples and evaluate the sample error across all of them. 

(a) [2 points] To begin, update the functions `getTheta` and `getResids` used in Section 2.3.4 of the course notes. These functions were originally written so that $c = \bar{x}$ in the linear regression equation above. These functions should be updated so that $c$ is hard coded as 0. 

```{r}
getTheta <- function(y, x, wt) {
  theta <- numeric(length = 2)
  ybarw <- sum(wt * y)/sum(wt)
  xbarw <- sum(wt * x)/sum(wt)
  theta[1] <- ybarw - (sum(wt * (x - xbarw) * y)/sum(wt * (x - xbarw)^2)) *
  (xbarw)
  theta[2] <- sum(wt * (x - xbarw) * y)/sum(wt * (x - xbarw)^2)
  ## return theta
  theta
}
getResids <- function(y, x, wt, theta) {
  xbar <- mean(x)
  alpha <- theta[1]
  beta <- theta[2]
  ## resids are
  y - alpha - beta * x
}
```

You will also use this modified version of `getWeights` in this question.

```{r}
getWeights <- function(resids, rhoPrimeFn, delta = 1e-12) {
  ## for calculating weights, minimum |residual| will be delta
  smallResids <- abs(resids) <= delta
  ## take care to preserve sign (in case rhoPrime not symmetric)
  resids[smallResids] <- delta * ifelse(resids[smallResids] >= 0, 1, -1)
  ## calculate and return weights
  rhoPrimeFn(resids)/resids
}
```

(b) [3 points] Using the `irls` and `testConvergence` functions from class along with the modified `getTheta`, `getResids`, and `getWeights` functions from part (a), determine the Huber robust regression estimates of $\alpha$ and $\beta$: $\widehat{\alpha}_{Huber}$ and $\widehat{\beta}_{Huber}$. Start the algorithm at $(\alpha_0, \beta_0) = \big(\widehat{\alpha}_{LS},\widehat{\beta}_{LS}\big)$ and use $k = 25$ in the Huber loss function
$$\rho_{k}(r) = \left\{ \begin{array}{ll}   \frac{1}{2}r^2  & \mbox{for } \lvert r \rvert \le k \\ k ~\lvert r \rvert - \frac{1}{2}k^2  & \mbox{for } \lvert r \rvert > k. \end{array} \right.$$
For full points, include the output from the `irls` function.

```{r}
huber.fn <- function(r, k=25) {
  val = r^2/2
  subr = abs(r) > k
  val[subr] = k * (abs(r[subr]) - k/2)
  return(val)
}
huber.fn.prime <- function(resid, k = 25) {
  val = resid
  subr = abs(resid) > k
  val[subr] = k * sign(resid[subr])
  return(val)
}

irls <- function(y, x, theta, rhoPrimeFn,
  dim = 2, delta = 1E-10,
  testConvergenceFn = testConvergence,
  maxIterations = 100, # maximum number of iterations
  tolerance = 1E-6, # parameters for the test
  relative = FALSE # for convergence function
) {
  if (missing(theta)) {
    theta <- rep(0, dim)
  }
  ## Initialize
  converged <- FALSE
  i <- 0
  N <- length(y)
  wt <- rep(1,N)
  ## LOOP
  while (!converged & i <= maxIterations) {
    ## get residuals
    resids <- getResids(y, x, wt, theta)
    ## update weights (should check for zero resids)
    wt <- getWeights(resids, rhoPrimeFn, delta)
    ## solve the least squares problem
    thetaNew <- getTheta(y, x, wt)
    ##
    ## Check convergence
    converged <- testConvergenceFn(thetaNew, theta,
    tolerance = tolerance,
    relative = relative)
    ## Update iteration
    theta <- thetaNew
    i <- i + 1
  }
  ## Return last value and whether converged or not
  list(theta = theta, converged = converged, iteration = i)
}

movies <- subset(netflix, movie == "Yes")
series <- subset(netflix, movie == "No")

alpha_ls <- (sum(series$hours_viewed) / 1000000)/length(series)
beta_ls <- sum((movies$hours_viewed / 1000000))/length(movies) - alpha_ls

result <- irls(netflix$hours_viewed / 1000000, c(0,1), theta=c(alpha_ls, beta_ls),
               rhoPrimeFn=huber.fn.prime, maxIterations=100)

print(result)
```

(c) [9 points] You will now repeat part (b) 1000 times for *samples* from this population, storing the $\alpha$ and $\beta$ estimates from each sample. That is:

    * Take 1000 samples of size $n=100$ from the Netflix engagement data. You will sample from each subpopulation separately without replacement so that $n_0 = 80$ units in the sample are series ($x_u = 0$) and the remaining $n_1 = 20$ units are movies ($x_u = 1$). These proportions for the sample reflect roughly 80\% and 20\% of the units in the population being series and movies, respectively.
    * On each sample, use IRLS to calculate the Huber robust regression estimates of $\alpha$ and $\beta$. For each sample, start the algorithm at the LS estimates derived from *that sample*.

    Next, you will construct two reflected density plots (oriented side-by-side) that visualize the sample error associated with this estimation. Reflected density plots allow us to compare two density functions. The example reflected density plots given below were created using simulated normal data to provide guidance for the formatting of these plots.  

    * On the left, visualize the estimated sampling distribution of the Huber-based location measures. Plot the density estimate of $\widehat{\alpha}_{Huber}$ above the horizontal axis in the shade of `adjustcolor("firebrick", 0.25)`. Plot the density estimate for $\widehat{\alpha}_{Huber} + \widehat{\beta}_{Huber}$ below the horizontal axis in the shade of `adjustcolor("steelblue", 0.25)`. Both density curves should fit entirely on the $x$-axis and $y$-axis of this plot. Lastly, include dashed vertical lines denoting the Huber-based location measures for the movie and series subpopulations found in part (b).
    * On the right, construct a similar plot for the sample error (found by subtracting the dashed vertical lines from the sample estimates for the Huber-based location measure in each group). The colour scheme should be the same as the left plot, but no dashed vertical lines are required. 

```{r}
alphas <- c()
betas <- c()
for (i in 1:1000) {
  while(TRUE) {
    sam <- netflix[sample(nrow(netflix), 100),]
    cnt <- sum(sam$movie == "Yes")
    if (cnt == 20) {
      break
    }
  }
  movies <- subset(sam, movie == "Yes")
  series <- subset(sam, movie == "No")

  alpha_ls <- (sum(series$hours_viewed) / 1000000)/length(series)
  beta_ls <- sum((movies$hours_viewed / 1000000))/length(movies) - alpha_ls
  
  res <- irls(sam$hours_viewed / 1000000, c(0,1), theta=c(alpha_ls, beta_ls),
                 rhoPrimeFn=huber.fn.prime, maxIterations=100)
  alphas <- c(alphas, result$theta[1])
  betas <- c(betas, result$theta[2])
}

plot(density(alphas),ylim=c(-0.05,0.05),main="Sampling Distribution"
     ,xlab="Huber-based location measures")
combined <- density(alphas + betas)
combined$y <- -combined$y
lines(combined)
polygon(c(min(density(alphas)$x), density(alphas)$x, max(density(alphas)$x)),
        c(0,density(alphas)$y, 0), col=adjustcolor("firebrick",0.25))
polygon(c(min(combined$x), combined$x, max(combined$x)),
         c(0,combined$y, 0), col=adjustcolor("steelblue",0.25))

segments(result$theta[1], 0, result$theta[1], 1, lty=2)
segments(result$theta[1] + result$theta[2], 0,
          result$theta[1] + result$theta[2], -1, lty=2)

a <- density(alphas)
a$x <- a$x - result$theta[1]
plot(a,ylim=c(-0.05,0.05), main="Sampling Distribution", 
     xlab="Sample Error")
combined$x <- combined$x - result$theta[1] + result$theta[2]
lines(combined)
polygon(c(min(a$x), a$x, max(a$x)),
        c(0,a$y, 0), col=adjustcolor("firebrick",0.25))
polygon(c(min(combined$x), combined$x, max(combined$x)),
         c(0,combined$y, 0), col=adjustcolor("steelblue",0.25))
```

(d) [3 points] Does the subpopulation for Netflix series *or* movies have greater dispersion in its sample error sampling distribution? Why do you think this subpopulation has greater dispersion in its sample error? How might you modify the sampling mechanism used in part (c) so that the two subpopulations have more comparable levels of dispersion in their sample error sampling distributions?

The sample errors between them are very similar, but is slighty more dispersed
for movies. This may be because we are sampling fewer movies, so more influential
points may have a stronger impact on our sample density. We can modify our
sampling mechanism to not have a hard limit of 80/20, since with 1000 samples,
the sample average will approach 80/20 regardless due to the central limit
theorem.

\newpage

## QUESTION 3: Horvitz-Thompson Estimation (SRSWOR) [20 points]

In this question, you will explore data from the National Basketball Association (NBA). In NBA games, the team that plays at their home arena is called the home team. You will estimate the average number of points that the home team wins basketball games by in the NBA; this metric is often called the home team advantage. The `nba.csv` file and its key variates are described in the table below. This file contains the team data for the 2023-24 NBA season up to 2024-02-15, inclusive.

*For interest*: This data set of team performances comes from the `load_nba_team_box()` function in the `hoopR` package. See https://hoopr.sportsdataverse.org/reference/index.html for more detail.

| Variable               | Description                                                 |
|------------------------|-------------------------------------------------------------|
| `team_home_away`              | A variate that takes a value of "home" if the team in question was playing at their home arena, and "away" otherwise. |
| `team_name`      | The name of the team in question (e.g. "Raptors", "Pistons").         |
| `team_score`       | The number of points scored by the team in question.             |
| `opponent_team_score`  | The number of points scored by the opposing team for that game.                |
| 53 other variates      | Not used in this question (for your interest).   |


a) [2 points] Subset the data to include only home games. If done correctly, the resulting data frame should contain $N=821$ games. Consider this the population $\mathcal{P}$. The attribute of interest in this population is the home team advantage (in points). Calculate this attribute as the difference between `team_score` and `opponent_team_score`.

```{r}
nba <- read.csv("nba.csv")

home <- subset(nba, team_home_away == "home")
advantage <- home$team_score - home$opponent_team_score
```

b) [5 points] Use the following code to take a sample of home games (assuming that `nba_home` is your data set of home games). This is not for marks.


```{r}
srsSampIndex <- read.table("srsSampIndex.txt")$V1
nba_sample <- home[srsSampIndex,]
```


i. [2 points] Calculate the Horvitz-Thompson estimate of the population home team advantage based on this sample.
```{r}
inc_prob <- nrow(nba_sample) / nrow(home)

ht_e <- sum(advantage / inc_prob)
ht_e
```

ii. [2 points] Calculate the standard error for this estimate. You may find the following function useful:
    
```{r}
estVarHT <- function(y_u, pi_u, pi_uv){
  ## y_u = an n element array containing the variate values for the sample
  ## pi_u = an n element array containing the (marginal) inclusion probabilities for the sample
  ## pi_uv = an nxn matrix containing the joint inclusion probabilities for the sample
  delta <- pi_uv - outer(pi_u, pi_u)
  estimateVar <-  sum( (delta/pi_uv) * outer(y_u/pi_u,y_u/pi_u) ) 
  return(abs(estimateVar))
}
y <- nba_sample$team_score - nba_sample$opponent_team_score
pi_uv <- (nrow(nba_sample) * (nrow(nba_sample) - 1)) / (nrow(home) * (nrow(home) - 1))
se <- sqrt(estVarHT(y, replicate(nrow(nba_sample), inc_prob), pi_uv) /
       nrow(nba_sample))
se
```

iii. [1 point] Calculate an approximate 95% confidence interval for the average home team advantage.

```{r}
print(paste0("[",-1.96 * se + ht_e, "," ,1.96 * se + ht_e, "]"))
```

c) [13 marks] We now consider an atypical sampling design, where the population is divided into two subpopulations. The first subpopulation $\mathcal{P}_1$ consists of the $N_1 = 27$ Toronto Raptors home games, whereas $\mathcal{P}_2$ consists of the $N_2 = 794$ non-Raptors home games. We will select samples of size $n = 107$ that contain *all* 27 home games for the Toronto Raptors. The samples will also contain $n_2 = 80$ of the remaining $N_2 = 794$ games selected by SRSWOR. 

    This question compares two estimators for the population home team advantage under this sampling design. The first estimator is the naive average of the home team advantages (in points) from our sample. The second is the standard Horvitz-Thompson estimator that weights the home team advantages in our sample according to the reciprocal of their inclusion probabilities.

i. [2 marks] To compute the Horvitz-Thompson estimate, we will need to calculate the inclusion probabilities $\pi_u$. Compute these as a vector of 107 elements, putting the 27 Raptors games first. Show your code.

```{r}
pi_u <- c(rep(1,27),rep(80/794,80))
```

ii. [4 marks] To compute the standard error for Horvitz-Thompson, we will need to calculate the joint inclusion probabilities $\pi_{uv}$. Compute these as a 107x107 matrix `pi_uv`, putting the 27 Raptors games as the first 27 rows and columns. Show your code and print `pi_uv[c(1, 2, 106, 107), c(1, 2, 106, 107)]` for full marks.

```{r}
pi_uv = matrix(0, nrow=107, ncol=107)
for (i in 1:107) {
  for (j in 1:107) {
    if (i <= 27 && j <= 27) {
      pi_uv[i,j] <- 27*26/27*26
    } else {
      pi_uv[i,j] <- (80*79/794*793)
    }
  }
}
print(pi_uv[c(1,2,106,107), c(1,2,106,107)])
```

iii.  [5 marks] You will now implement the atypical sampling process 5000 times.  

      * From each sample, you will compute two estimates: (i) the naive sample average of the home team advantages and (ii) the Horvitz-Thompson estimate of the home team advantage. 
      * For each estimate, you will compute an approximate 95\% confidence interval. The confidence interval based on Horvitz-Thompson can be computed using a process similar to that in part (b). The following expression can be used (without proof) to estimate the variance of the naive average under this sampling design without bias:
$$\widehat{\text{Var}}\left(\dfrac{1}{n}\sum_{u \in \mathcal{S}}y_u \right) = \dfrac{n_2(N_2 - n_2)}{(N_1 + n_2)^2(N_2 - 1)}\hat{\sigma}^2_2,$$
where $\hat{\sigma}^2_2$ is the sample variance computed using *only* the observed variate values from $\mathcal{P}_2$. 
      * Use your sample estimates to estimate the mean squared error (MSE) for both estimators.
      * Estimate coverage of the population home team advantage (in points) for both types of confidence intervals you constructed. Coverage is the proportion of these intervals that contain $a(\mathcal{P})$.

```{r}
raps <- subset(home, team_name=="Raptors")
nonraps <- subset(home, team_name!="Raptors")
n1 = 27
N1 = 27
n2 = 80
N2 = 794
n = 107
for (i in 1:5000) {
  sam <- c(raps, nonraps[sample(nrow(nonraps), 80),])
  adv <- sam$team_score - sam$opponent_team_score
  naive_avg <- adv / 107
  ht_e <- sum(adv / pi_u)
  
  sam_var <- sum((adv - naive_avg)^2) / (n-1)
  var_n_avg <- (n2 * N2 - n2) / ((N1 + n2)^2 * (N2 - 1))
  se_n_avg <- sqrt(var_n_avg / n)
  
  se_hte <- sqrt(estVarHT(adv, pi_u, pi_uv) / n)
  
  n_avg_ci <- c(-1.96 * se_n_avg + naive_avg, 1.96 * se_n_avg + naive_avg)
  hte_ci <- c(-1.96 * se_hte + ht_e, 1.96 * se_hte + ht_e)
}

```

iv. [2 marks] Based on MSE, is the naive average or Horvitz-Thompson estimator to be preferred? Does your answer change if you compare the estimators based on their corresponding 95\% confidence interval coverage.
