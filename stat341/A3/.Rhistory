movies <- subset(sam, movie == "Yes")
series <- subset(sam, movie == "No")
alpha_ls <- (sum(series$hours_viewed) / 1000000)/length(series)
beta_ls <- sum((movies$hours_viewed / 1000000))/length(movies) - alpha_ls
res <- irls(sam$hours_viewed / 1000000, c(0,1), theta=c(alpha_ls, beta_ls),
rhoPrimeFn=huber.fn.prime, maxIterations=100)
alphas <- c(alphas, result$theta[1])
betas <- c(betas, result$theta[2])
}
plot(density(alphas),ylim=c(-0.05,0.05),main="Sampling Distribution"
,xlab="Huber-based location measures")
combined <- density(alphas + betas)
combined$y <- -combined$y
lines(combined)
polygon(c(min(density(alphas)$x), density(alphas)$x, max(density(alphas)$x)),
c(0,density(alphas)$y, 0), col=adjustcolor("firebrick",0.25))
polygon(c(min(combined$x), combined$x, max(combined$x)),
c(0,combined$y, 0), col=adjustcolor("steelblue",0.25))
segments(result$theta[1], 0, result$theta[1], 1, lty=2)
segments(result$theta[1] + result$theta[2], 0,
result$theta[1] + result$theta[2], -1, lty=2)
a <- density(alphas)
a$x <- a$x - result$theta[1]
plot(a,ylim=c(-0.05,0.05), main="Sampling Distribution",
xlab="Sample Error")
combined$x <- combined$x - result$theta[1] + result$theta[2]
lines(combined)
polygon(c(min(a$x), a$x, max(a$x)),
c(0,a$y, 0), col=adjustcolor("firebrick",0.25))
polygon(c(min(combined$x), combined$x, max(combined$x)),
c(0,combined$y, 0), col=adjustcolor("steelblue",0.25))
alphas <- c()
betas <- c()
for (i in 1:1000) {
while(TRUE) {
sam <- netflix[sample(nrow(netflix), 100),]
cnt <- sum(sam$movie == "Yes")
if (cnt == 20) {
break
}
}
movies <- subset(sam, movie == "Yes")
series <- subset(sam, movie == "No")
alpha_ls <- (sum(series$hours_viewed) / 1000000)/length(series)
beta_ls <- sum((movies$hours_viewed / 1000000))/length(movies) - alpha_ls
res <- irls(sam$hours_viewed / 1000000, c(0,1), theta=c(alpha_ls, beta_ls),
rhoPrimeFn=huber.fn.prime, maxIterations=100)
alphas <- c(alphas, result$theta[1])
betas <- c(betas, result$theta[2])
}
plot(density(alphas),ylim=c(-0.05,0.05),main="Sampling Distribution"
,xlab="Huber-based location measures")
combined <- density(alphas + betas)
combined$y <- -combined$y
lines(combined)
polygon(c(min(density(alphas)$x), density(alphas)$x, max(density(alphas)$x)),
c(0,density(alphas)$y, 0), col=adjustcolor("firebrick",0.25))
polygon(c(min(combined$x), combined$x, max(combined$x)),
c(0,combined$y, 0), col=adjustcolor("steelblue",0.25))
segments(result$theta[1], 0, result$theta[1], 1, lty=2)
segments(result$theta[1] + result$theta[2], 0,
result$theta[1] + result$theta[2], -1, lty=2)
a <- density(alphas)
a$x <- a$x - result$theta[1]
plot(a,ylim=c(-0.05,0.05), main="Sampling Distribution",
xlab="Sample Error")
combined$x <- combined$x - result$theta[1] + result$theta[2]
lines(combined)
polygon(c(min(a$x), a$x, max(a$x)),
c(0,a$y, 0), col=adjustcolor("firebrick",0.25))
polygon(c(min(combined$x), combined$x, max(combined$x)),
c(0,combined$y, 0), col=adjustcolor("steelblue",0.25))
library(knitr)
#opts_chunk$set(tidy.opts=list(width.cutoff=70),tidy=TRUE)
knitr::opts_chunk$set(echo = TRUE, warning = FALSE)
netflix <- read.csv("netflix.csv")
movies <- subset(netflix, movie == "Yes")
hours <- sapply(movies$hours_viewed, FUN = function(x) {
x / 1000000
})
hist(hours, main="Watch time for Movies", xlab="Movie", ylab="Density",prob=TRUE)
getTheta <- function(y, x, wt) {
theta <- numeric(length = 2)
ybarw <- sum(wt * y)/sum(wt)
xbarw <- sum(wt * x)/sum(wt)
theta[1] <- ybarw - (sum(wt * (x - xbarw) * y)/sum(wt * (x - xbarw)^2)) *
(xbarw)
theta[2] <- sum(wt * (x - xbarw) * y)/sum(wt * (x - xbarw)^2)
## return theta
theta
}
getResids <- function(y, x, wt, theta) {
xbar <- mean(x)
alpha <- theta[1]
beta <- theta[2]
## resids are
y - alpha - beta * x
}
getWeights <- function(resids, rhoPrimeFn, delta = 1e-12) {
## for calculating weights, minimum |residual| will be delta
smallResids <- abs(resids) <= delta
## take care to preserve sign (in case rhoPrime not symmetric)
resids[smallResids] <- delta * ifelse(resids[smallResids] >= 0, 1, -1)
## calculate and return weights
rhoPrimeFn(resids)/resids
}
huber.fn <- function(r, k=25) {
val = r^2/2
subr = abs(r) > k
val[subr] = k * (abs(r[subr]) - k/2)
return(val)
}
huber.fn.prime <- function(resid, k = 25) {
val = resid
subr = abs(resid) > k
val[subr] = k * sign(resid[subr])
return(val)
}
irls <- function(y, x, theta, rhoPrimeFn,
dim = 2, delta = 1E-10,
testConvergenceFn = testConvergence,
maxIterations = 100, # maximum number of iterations
tolerance = 1E-6, # parameters for the test
relative = FALSE # for convergence function
) {
if (missing(theta)) {
theta <- rep(0, dim)
}
## Initialize
converged <- FALSE
i <- 0
N <- length(y)
wt <- rep(1,N)
## LOOP
while (!converged & i <= maxIterations) {
## get residuals
resids <- getResids(y, x, wt, theta)
## update weights (should check for zero resids)
wt <- getWeights(resids, rhoPrimeFn, delta)
## solve the least squares problem
thetaNew <- getTheta(y, x, wt)
##
## Check convergence
converged <- testConvergenceFn(thetaNew, theta,
tolerance = tolerance,
relative = relative)
## Update iteration
theta <- thetaNew
i <- i + 1
}
## Return last value and whether converged or not
list(theta = theta, converged = converged, iteration = i)
}
movies <- subset(netflix, movie == "Yes")
series <- subset(netflix, movie == "No")
alpha_ls <- (sum(series$hours_viewed) / 1000000)/length(series)
beta_ls <- sum((movies$hours_viewed / 1000000))/length(movies) - alpha_ls
result <- irls(netflix$hours_viewed / 1000000, c(0,1), theta=c(alpha_ls, beta_ls),
rhoPrimeFn=huber.fn.prime, maxIterations=100)
testConvergence <- function(thetaNew, thetaOld, tolerance = 1e-10, relative = FALSE) {
sum(abs(thetaNew - thetaOld)) < if (relative)
tolerance * sum(abs(thetaOld)) else tolerance
}
huber.fn <- function(r, k=25) {
val = r^2/2
subr = abs(r) > k
val[subr] = k * (abs(r[subr]) - k/2)
return(val)
}
huber.fn.prime <- function(resid, k = 25) {
val = resid
subr = abs(resid) > k
val[subr] = k * sign(resid[subr])
return(val)
}
irls <- function(y, x, theta, rhoPrimeFn,
dim = 2, delta = 1E-10,
testConvergenceFn = testConvergence,
maxIterations = 100, # maximum number of iterations
tolerance = 1E-6, # parameters for the test
relative = FALSE # for convergence function
) {
if (missing(theta)) {
theta <- rep(0, dim)
}
## Initialize
converged <- FALSE
i <- 0
N <- length(y)
wt <- rep(1,N)
## LOOP
while (!converged & i <= maxIterations) {
## get residuals
resids <- getResids(y, x, wt, theta)
## update weights (should check for zero resids)
wt <- getWeights(resids, rhoPrimeFn, delta)
## solve the least squares problem
thetaNew <- getTheta(y, x, wt)
##
## Check convergence
converged <- testConvergenceFn(thetaNew, theta,
tolerance = tolerance,
relative = relative)
## Update iteration
theta <- thetaNew
i <- i + 1
}
## Return last value and whether converged or not
list(theta = theta, converged = converged, iteration = i)
}
movies <- subset(netflix, movie == "Yes")
series <- subset(netflix, movie == "No")
alpha_ls <- (sum(series$hours_viewed) / 1000000)/length(series)
beta_ls <- sum((movies$hours_viewed / 1000000))/length(movies) - alpha_ls
result <- irls(netflix$hours_viewed / 1000000, c(0,1), theta=c(alpha_ls, beta_ls),
rhoPrimeFn=huber.fn.prime, maxIterations=100)
print(result)
alphas <- c()
betas <- c()
for (i in 1:1000) {
while(TRUE) {
sam <- netflix[sample(nrow(netflix), 100),]
cnt <- sum(sam$movie == "Yes")
if (cnt == 20) {
break
}
}
movies <- subset(sam, movie == "Yes")
series <- subset(sam, movie == "No")
alpha_ls <- (sum(series$hours_viewed) / 1000000)/length(series)
beta_ls <- sum((movies$hours_viewed / 1000000))/length(movies) - alpha_ls
res <- irls(sam$hours_viewed / 1000000, c(0,1), theta=c(alpha_ls, beta_ls),
rhoPrimeFn=huber.fn.prime, maxIterations=100)
alphas <- c(alphas, result$theta[1])
betas <- c(betas, result$theta[2])
}
plot(density(alphas),ylim=c(-0.05,0.05),main="Sampling Distribution"
,xlab="Huber-based location measures")
combined <- density(alphas + betas)
combined$y <- -combined$y
lines(combined)
polygon(c(min(density(alphas)$x), density(alphas)$x, max(density(alphas)$x)),
c(0,density(alphas)$y, 0), col=adjustcolor("firebrick",0.25))
polygon(c(min(combined$x), combined$x, max(combined$x)),
c(0,combined$y, 0), col=adjustcolor("steelblue",0.25))
segments(result$theta[1], 0, result$theta[1], 1, lty=2)
segments(result$theta[1] + result$theta[2], 0,
result$theta[1] + result$theta[2], -1, lty=2)
a <- density(alphas)
a$x <- a$x - result$theta[1]
plot(a,ylim=c(-0.05,0.05), main="Sampling Distribution",
xlab="Sample Error")
combined$x <- combined$x - result$theta[1] + result$theta[2]
lines(combined)
polygon(c(min(a$x), a$x, max(a$x)),
c(0,a$y, 0), col=adjustcolor("firebrick",0.25))
polygon(c(min(combined$x), combined$x, max(combined$x)),
c(0,combined$y, 0), col=adjustcolor("steelblue",0.25))
alphas <- c()
betas <- c()
for (i in 1:1000) {
while(TRUE) {
sam <- netflix[sample(nrow(netflix), 100),]
cnt <- sum(sam$movie == "Yes")
if (cnt == 20) {
break
}
}
movies <- subset(sam, movie == "Yes")
series <- subset(sam, movie == "No")
print(movies$title)
alpha_ls <- (sum(series$hours_viewed) / 1000000)/length(series)
beta_ls <- sum((movies$hours_viewed / 1000000))/length(movies) - alpha_ls
res <- irls(sam$hours_viewed / 1000000, c(0,1), theta=c(alpha_ls, beta_ls),
rhoPrimeFn=huber.fn.prime, maxIterations=100)
alphas <- c(alphas, result$theta[1])
betas <- c(betas, result$theta[2])
}
plot(density(alphas),ylim=c(-0.05,0.05),main="Sampling Distribution"
,xlab="Huber-based location measures")
combined <- density(alphas + betas)
combined$y <- -combined$y
lines(combined)
polygon(c(min(density(alphas)$x), density(alphas)$x, max(density(alphas)$x)),
c(0,density(alphas)$y, 0), col=adjustcolor("firebrick",0.25))
polygon(c(min(combined$x), combined$x, max(combined$x)),
c(0,combined$y, 0), col=adjustcolor("steelblue",0.25))
segments(result$theta[1], 0, result$theta[1], 1, lty=2)
segments(result$theta[1] + result$theta[2], 0,
result$theta[1] + result$theta[2], -1, lty=2)
a <- density(alphas)
a$x <- a$x - result$theta[1]
plot(a,ylim=c(-0.05,0.05), main="Sampling Distribution",
xlab="Sample Error")
combined$x <- combined$x - result$theta[1] + result$theta[2]
lines(combined)
polygon(c(min(a$x), a$x, max(a$x)),
c(0,a$y, 0), col=adjustcolor("firebrick",0.25))
polygon(c(min(combined$x), combined$x, max(combined$x)),
c(0,combined$y, 0), col=adjustcolor("steelblue",0.25))
nba <- read.csv("nba.csv")
View(nba)
nba <- read.csv("nba.csv")
home <- subset(nba, team_home_away == "home")
home_advantage <- sapply(home, FUN=function(x) {
x$team_score - x$opponent_team-score
})
nba <- read.csv("nba.csv")
home <- subset(nba, team_home_away == "home")
home_advantage <- sapply(home, FUN=function(x) {
print(x)
x$team_score - x$opponent_team-score
})
View(home)
nba <- read.csv("nba.csv")
home <- subset(nba, team_home_away == "home")
advantage <- home$team_score - home$opponent_team_score
srsSampIndex <- read.table("srsSampIndex.txt")$V1
srsSampIndex <- read.table("srsSampIndex.txt")$V1
srsSampIndex <- read.table("srsSampIndex.txt")$V1
nba_sample <- home[srsSampIndex,]
inc_prob <- length(nba_sample) / length(home)
ht_e <- sum(advantage / inc_prob)
srsSampIndex <- read.table("srsSampIndex.txt")$V1
nba_sample <- home[srsSampIndex,]
inc_prob <- length(nba_sample) / length(home)
ht_e <- sum(advantage / inc_prob)
srsSampIndex <- read.table("srsSampIndex.txt")$V1
nba_sample <- home[srsSampIndex,]
inc_prob <- nrow(nba_sample) / nrow(home)
ht_e <- sum(advantage / inc_prob)
inc_prob <- nrow(nba_sample) / nrow(home)
ht_e <- sum(advantage / inc_prob)
ht_e
estVarHT <- function(y_u, pi_u, pi_uv){
## y_u = an n element array containing the variate values for the sample
## pi_u = an n element array containing the (marginal) inclusion probabilities for the sample
## pi_uv = an nxn matrix containing the joint inclusion probabilities for the sample
delta <- pi_uv - outer(pi_u, pi_u)
estimateVar <-  sum( (delta/pi_uv) * outer(y_u/pi_u,y_u/pi_u) )
return(abs(estimateVar))
}
estVarHT <- function(y_u, pi_u, pi_uv){
## y_u = an n element array containing the variate values for the sample
## pi_u = an n element array containing the (marginal) inclusion probabilities for the sample
## pi_uv = an nxn matrix containing the joint inclusion probabilities for the sample
delta <- pi_uv - outer(pi_u, pi_u)
estimateVar <-  sum( (delta/pi_uv) * outer(y_u/pi_u,y_u/pi_u) )
return(abs(estimateVar))
}
sqrt(estVarHT / nrow(nba_sample))
estVarHT <- function(y_u, pi_u, pi_uv){
## y_u = an n element array containing the variate values for the sample
## pi_u = an n element array containing the (marginal) inclusion probabilities for the sample
## pi_uv = an nxn matrix containing the joint inclusion probabilities for the sample
delta <- pi_uv - outer(pi_u, pi_u)
estimateVar <-  sum( (delta/pi_uv) * outer(y_u/pi_u,y_u/pi_u) )
return(abs(estimateVar))
}
y <- nba_sample$team_score - nba_sample$opponent_team_score
pi_uv <- (nrow(nba_sample) * (nrow(nba_sample) - 1)) / (nrow(home) * (nrow(home) - 1))
sqrt(estVarHT(y, replicate(nrow(nba_sample), inc_prob), pi_uv) /
nrow(nba_sample))
-1.96 * se + ht_e
estVarHT <- function(y_u, pi_u, pi_uv){
## y_u = an n element array containing the variate values for the sample
## pi_u = an n element array containing the (marginal) inclusion probabilities for the sample
## pi_uv = an nxn matrix containing the joint inclusion probabilities for the sample
delta <- pi_uv - outer(pi_u, pi_u)
estimateVar <-  sum( (delta/pi_uv) * outer(y_u/pi_u,y_u/pi_u) )
return(abs(estimateVar))
}
y <- nba_sample$team_score - nba_sample$opponent_team_score
pi_uv <- (nrow(nba_sample) * (nrow(nba_sample) - 1)) / (nrow(home) * (nrow(home) - 1))
se <- sqrt(estVarHT(y, replicate(nrow(nba_sample), inc_prob), pi_uv) /
nrow(nba_sample))
se
-1.96 * se + ht_e
print(paste0(-1.96 * se + ht_e, 1.96 * se + ht_e))
print(paste0("[",-1.96 * se + ht_e, "," ,1.96 * se + ht_e, "]"))
pi_u <- c(rep(1,27),rep(80/794,80))
pi_uv = matrix(0, nrow=107, ncol=107)
for (i in 1:107) {
for (j : 1:107) {
pi_uv = matrix(0, nrow=107, ncol=107)
for (i in 1:107) {
for (j : 1:107) {
pi_uv = matrix(0, nrow=107, ncol=107)
for (i in 1:107) {
for (j in 1:107) {
if (i <= 27 && j <= 27) {
pi_uv[i,j] <- 27*26/27*26
} else {
pi_uv[i,j] <- (80*79/794*793)
}
}
}
pi_uv = matrix(0, nrow=107, ncol=107)
for (i in 1:107) {
for (j in 1:107) {
if (i <= 27 && j <= 27) {
pi_uv[i,j] <- 27*26/27*26
} else {
pi_uv[i,j] <- (80*79/794*793)
}
}
}
print(pi_uv[c(1,2,106,107), c(1,2,106,107)])
srsSampIndex <- read.table("srsSampIndex.txt")$V1
nba_sample <- home[srsSampIndex,]
raps <- subset(home, team_name=="Raptors")
nonraps <- subset(home, team_name!="Raptors")
raps <- subset(home, team_name=="Raptors")
nonraps <- subset(home, team_name!="Raptors")
for (i in 1:5000) {
sam <- c(raps, nonraps[sample(nrows(nonraps), 80)],)
}
raps <- subset(home, team_name=="Raptors")
nonraps <- subset(home, team_name!="Raptors")
for (i in 1:5000) {
sam <- c(raps, nonraps[sample(nrows(nonraps), 80)],)
}
raps <- subset(home, team_name=="Raptors")
nonraps <- subset(home, team_name!="Raptors")
for (i in 1:5000) {
sam <- c(raps, nonraps[sample(nrows(nonraps), 80),])
}
raps <- subset(home, team_name=="Raptors")
nonraps <- subset(home, team_name!="Raptors")
for (i in 1:5000) {
sam.nonraps <- nonraps[sample(nrows(nonraps), 80),]
sam <- c(raps, sam.nonraps)
}
raps <- subset(home, team_name=="Raptors")
nonraps <- subset(home, team_name!="Raptors")
for (i in 1:5000) {
sam.nonraps <- nonraps[sample(nrows(nonraps), 80),]
sam <- c(raps, sam.nonraps)
}
raps <- subset(home, team_name=="Raptors")
nonraps <- subset(home, team_name!="Raptors")
for (i in 1:5000) {
sam <- c(raps, nonraps[sample(nrow(nonraps), 80),])
}
raps <- subset(home, team_name=="Raptors")
nonraps <- subset(home, team_name!="Raptors")
for (i in 1:1) {
sam <- c(raps, nonraps[sample(nrow(nonraps), 80),])
print(sam)
}
raps <- subset(home, team_name=="Raptors")
nonraps <- subset(home, team_name!="Raptors")
for (i in 1:5000) {
sam <- c(raps, nonraps[sample(nrow(nonraps), 80),])
adv <- sam$team_score - sam$opponent_team_score
naive_avg <- adv / 107
ht_e <- sum(sam / pi_u)
}
raps <- subset(home, team_name=="Raptors")
nonraps <- subset(home, team_name!="Raptors")
for (i in 1:5000) {
sam <- c(raps, nonraps[sample(nrow(nonraps), 80),])
adv <- sam$team_score - sam$opponent_team_score
naive_avg <- adv / 107
ht_e <- sum(adv / pi_u)
}
raps <- subset(home, team_name=="Raptors")
nonraps <- subset(home, team_name!="Raptors")
n1 = 27
N1 = 27
n1 = 80
N2 = 794
n = 107
for (i in 1:5000) {
sam <- c(raps, nonraps[sample(nrow(nonraps), 80),])
adv <- sam$team_score - sam$opponent_team_score
naive_avg <- adv / 107
ht_e <- sum(adv / pi_u)
sam_var <- sum((adv - naive_avg)^2) / (n-1)
var_n_avg <- (n2 * N2 - n2) / ((N1 + n2)^2 * (N2 - 1))
}
raps <- subset(home, team_name=="Raptors")
nonraps <- subset(home, team_name!="Raptors")
n1 = 27
N1 = 27
n2 = 80
N2 = 794
n = 107
for (i in 1:5000) {
sam <- c(raps, nonraps[sample(nrow(nonraps), 80),])
adv <- sam$team_score - sam$opponent_team_score
naive_avg <- adv / 107
ht_e <- sum(adv / pi_u)
sam_var <- sum((adv - naive_avg)^2) / (n-1)
var_n_avg <- (n2 * N2 - n2) / ((N1 + n2)^2 * (N2 - 1))
}
raps <- subset(home, team_name=="Raptors")
nonraps <- subset(home, team_name!="Raptors")
n1 = 27
N1 = 27
n2 = 80
N2 = 794
n = 107
for (i in 1:5000) {
sam <- c(raps, nonraps[sample(nrow(nonraps), 80),])
adv <- sam$team_score - sam$opponent_team_score
naive_avg <- adv / 107
ht_e <- sum(adv / pi_u)
sam_var <- sum((adv - naive_avg)^2) / (n-1)
var_n_avg <- (n2 * N2 - n2) / ((N1 + n2)^2 * (N2 - 1))
se_hte <- sqrt(estVarHT(adv, pi_u, pi_uv) / n)
}
