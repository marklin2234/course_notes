data_y0 <- data.frame(x1 = x1_y0, x2 = x2_y0, noise_vars[1:n_obs_y0, ])
set.seed(123)
generate_mixture_data <- function(n, y) {
means <- runif(10, 0, 1)
data <- data.frame(x1 = numeric(n), x2 = numeric(n), y = y)
for (i in 1:n) {
component <- sample(1:10, 1)
data$x1[i] <- rnorm(1, means[component], 0.01)
component <- sample(1:10, 1)
data$x2[i] <- rnorm(1, means[component], 0.01)
}
return(data)
}
y0 <- generate_mixture_data(500, 0)
y1 <- generate_mixture_data(500, 1)
data <- rbind(y0, y1)
generate_noise_data <- function() {
noise <- runif(100, 0, 1)
data <- c()
for (i in 1:1000) {
s <- sample(1:100, 1)
data[i] <- noise[s]
}
return(data)
}
data$noise1 <- generate_noise_data()
data$noise2 <- generate_noise_data()
train_indices <- sample(1:nrow(data), nrow(data)/2)
train_data <- data[train_indices, ]
test_data <- data[-train_indices, ]
library(ggplot2)
library(dplyr)
ggplot(data, aes(x = x1, y = x1, color = factor(y))) +
geom_point(alpha = 0.6) +
labs(title = "Scatter Plot of x1 vs x1", x = "x1", y = "x1") +
scale_color_manual(values = c("blue", "red"), labels = c("y = 0", "y = 1")) +
theme_minimal()
set.seed(123)
generate_mixture_data <- function(n, y) {
means <- runif(10, 0, 1)
data <- data.frame(x1 = numeric(n), x2 = numeric(n), y = y)
for (i in 1:n) {
component <- sample(1:10, 1)
data$x1[i] <- rnorm(1, means[component], 0.01)
component <- sample(1:10, 1)
data$x2[i] <- rnorm(1, means[component], 0.01)
}
return(data)
}
y0 <- generate_mixture_data(500, 0)
y1 <- generate_mixture_data(500, 1)
data <- rbind(y0, y1)
generate_noise_data <- function() {
noise <- runif(100, 0, 1)
data <- c()
for (i in 1:1000) {
s <- sample(1:100, 1)
data[i] <- noise[s]
}
return(data)
}
data$noise1 <- generate_noise_data()
data$noise2 <- generate_noise_data()
train_indices <- sample(1:nrow(data), nrow(data)/2)
train_data <- data[train_indices, ]
test_data <- data[-train_indices, ]
library(ggplot2)
library(dplyr)
ggplot(data, aes(x = data$x1, y = data$x1, color = factor(y))) +
geom_point(alpha = 0.6) +
labs(title = "Scatter Plot of x1 vs x1", x = "x1", y = "x1") +
scale_color_manual(values = c("blue", "red"), labels = c("y = 0", "y = 1")) +
theme_minimal()
set.seed(123)
generate_mixture_data <- function(n, y) {
means <- runif(10, 0, 1)
data <- data.frame(x1 = numeric(n), x2 = numeric(n), y = y)
for (i in 1:n) {
component <- sample(1:10, 1)
data$x1[i] <- rnorm(1, means[component], 0.01)
component <- sample(1:10, 1)
data$x2[i] <- rnorm(1, means[component], 0.01)
}
return(data)
}
y0 <- generate_mixture_data(500, 0)
y1 <- generate_mixture_data(500, 1)
data <- rbind(y0, y1)
generate_noise_data <- function() {
noise <- runif(100, 0, 1)
data <- c()
for (i in 1:1000) {
s <- sample(1:100, 1)
data[i] <- noise[s]
}
return(data)
}
data$noise <- generate_noise_data()
train_indices <- sample(1:nrow(data), nrow(data)/2)
train_data <- data[train_indices, ]
test_data <- data[-train_indices, ]
library(ggplot2)
library(dplyr)
ggplot(data, aes(x = x1, y = x1, color = factor(y))) +
geom_point(alpha = 0.6) +
labs(title = "Scatter Plot of x1 vs x1", x = "x1", y = "x1") +
scale_color_manual(values = c("blue", "red"), labels = c("y = 0", "y = 1")) +
theme_minimal()
ggplot(data, aes(x=noise, y=noise, color=factor(y))) +
geom_point(alpha=0.6) +
labs(title="Scatter Plot of noise data", x="x1", y="x1") +
scale_color_manual(values=c("blue", "red"), labels=c("y=0","y-1")) +
theme_minimal()
View(data)
set.seed(123)
generate_mixture_data <- function(n, y) {
means <- runif(10, 0, 1)
data <- data.frame(x1 = numeric(n), x2 = numeric(n), y = y)
for (i in 1:n) {
component <- sample(1:10, 1)
data$x1[i] <- rnorm(1, means[component], 0.01)
component <- sample(1:10, 1)
data$x2[i] <- rnorm(1, means[component], 0.01)
}
return(data)
}
y0 <- generate_mixture_data(500, 0)
y1 <- generate_mixture_data(500, 1)
data <- rbind(y0, y1)
generate_noise_data_with_means <- function() {
noise_means <- numeric(p)
noise_data <- matrix(0, nrow = 1000, ncol = 100)
for (j in 1:100) {
noise_column <- runif(1000, 0, 1)
noise_data[, j] <- noise_column
noise_means[j] <- mean(noise_column)
}
list(noise_data = noise_data, noise_means = noise_means)
}
noise <- generate_noise_data()
data$noise <- noise$noise_data
noise
set.seed(123)
generate_mixture_data <- function(n, y) {
means <- runif(10, 0, 1)
data <- data.frame(x1 = numeric(n), x2 = numeric(n), y = y)
for (i in 1:n) {
component <- sample(1:10, 1)
data$x1[i] <- rnorm(1, means[component], 0.01)
component <- sample(1:10, 1)
data$x2[i] <- rnorm(1, means[component], 0.01)
}
return(data)
}
y0 <- generate_mixture_data(500, 0)
y1 <- generate_mixture_data(500, 1)
data <- rbind(y0, y1)
generate_noise_data_with_means <- function() {
noise_means <- numeric(p)
noise_data <- matrix(0, nrow = 1000, ncol = 100)
for (j in 1:100) {
noise_column <- runif(1000, 0, 1)
noise_data[, j] <- noise_column
noise_means[j] <- mean(noise_column)
}
list(noise_data = noise_data, noise_means = noise_means)
}
noise <- generate_noise_data_with_means()
set.seed(123)
generate_mixture_data <- function(n, y) {
means <- runif(10, 0, 1)
data <- data.frame(x1 = numeric(n), x2 = numeric(n), y = y)
for (i in 1:n) {
component <- sample(1:10, 1)
data$x1[i] <- rnorm(1, means[component], 0.01)
component <- sample(1:10, 1)
data$x2[i] <- rnorm(1, means[component], 0.01)
}
return(data)
}
y0 <- generate_mixture_data(500, 0)
y1 <- generate_mixture_data(500, 1)
data <- rbind(y0, y1)
generate_noise_data_with_means <- function() {
noise_means <- numeric(100)
noise_data <- matrix(0, nrow = 1000, ncol = 100)
for (j in 1:100) {
noise_column <- runif(1000, 0, 1)
noise_data[, j] <- noise_column
noise_means[j] <- mean(noise_column)
}
list(noise_data = noise_data, noise_means = noise_means)
}
noise <- generate_noise_data_with_means()
data$noise <- noise$noise_data
noise_means <- noise$noise_means
train_indices <- sample(1:nrow(data), nrow(data)/2)
train_data <- data[train_indices, ]
test_data <- data[-train_indices, ]
library(ggplot2)
library(dplyr)
ggplot(data, aes(x = x1, y = x1, color = factor(y))) +
geom_point(alpha = 0.6) +
labs(title = "Scatter Plot of x1 vs x1", x = "x1", y = "x1") +
scale_color_manual(values = c("blue", "red"), labels = c("y = 0", "y = 1")) +
theme_minimal()
ggplot(data, aes(x=noise, y=noise, color=factor(y))) +
geom_point(alpha=0.6) +
labs(title="Scatter Plot of noise data", x="x1", y="x1") +
scale_color_manual(values=c("blue", "red"), labels=c("y=0","y=1")) +
theme_minimal()
View(data)
View(data)
set.seed(123)
generate_mixture_data <- function(n, y) {
means <- runif(10, 0, 1)
data <- data.frame(x1 = numeric(n), x2 = numeric(n), y = y)
for (i in 1:n) {
component <- sample(1:10, 1)
data$x1[i] <- rnorm(1, means[component], 0.01)
component <- sample(1:10, 1)
data$x2[i] <- rnorm(1, means[component], 0.01)
}
return(data)
}
y0 <- generate_mixture_data(500, 0)
y1 <- generate_mixture_data(500, 1)
data <- rbind(y0, y1)
generate_noise_data <- function() {
noise <- runif(100, 0, 1)
data <- c()
for (i in 1:1000) {
s <- sample(1:100, 1)
data[i] <- noise[s]
}
return(data)
}
data$noise <- generate_noise_data()
train_indices <- sample(1:nrow(data), nrow(data)/2)
train_data <- data[train_indices, ]
test_data <- data[-train_indices, ]
library(ggplot2)
library(dplyr)
ggplot(data, aes(x = x1, y = x1, color = factor(y))) +
geom_point(alpha = 0.6) +
labs(title = "Scatter Plot of x1 vs x1", x = "x1", y = "x1") +
scale_color_manual(values = c("blue", "red"), labels = c("y = 0", "y = 1")) +
theme_minimal()
ggplot(data, aes(x=noise, y=noise, color=factor(y))) +
geom_point(alpha=0.6) +
labs(title="Scatter Plot of noise data", x="x1", y="x1") +
scale_color_manual(values=c("blue", "red"), labels=c("y=0","y=1")) +
theme_minimal()
noise_means <- mean(data$noise)
ggplot(data.frame(noise_means), aes(x = noise_means)) +
geom_histogram(binwidth = 0.05, fill = "skyblue", color = "black") +
labs(title = "Histogram of the Means of the Noise Variables", x = "Mean", y = "Count") +
theme_minimal()
noise_means
x <- c(0, 1, 2, 3)
y <- c(1, 2, 2, 1)
fit <- lm(y ~ x)
summary(fit)
View(fit)
setwd("/Users/marklin/Documents/waterloo/stat441/kaggle")
# SETUP
education_train = read.csv('module_Education_train_set.csv')
household_train = read.csv('module_HouseholdInfo_train_set.csv')
poverty_train = read.csv('module_SubjectivePoverty_train_set.csv')
education_test = read.csv('module_Education_test_set.csv')
household_test = read.csv('module_HouseholdInfo_test_set.csv')
library(tidyr)
library(dplyr)
poverty_train = poverty_train %>%
separate(psu_hh_idcode, into=c("psu","hh","idcode"),sep="_",convert=TRUE) %>%
rowwise() %>%
mutate(
poverty = which(c_across(starts_with("subjective_poverty_")) == 1)
) %>%
ungroup() %>%
select(psu,hh,idcode,poverty)
colnames(education_train) <- gsub("^q", "Q", colnames(education_train))
### What strategies should we use?
### 1. Lasso on each training set individually -> combine variates -> regress
### 2. Lasso on combined training set -> regress
### 3. How do we regress on variates? Elastic net, ridge regression, logit,
### least absolute error, robust ridge, select best model using APSE, or APAE
### Can try using a random forest
# Setup
## We want to remove some unecessary data.
## Household data Q4 and Q5 is DOB and age, q12 is just idcode, q17 is just
## idcode
## In education data, if q3 is NO (2) then q4 should be 0, not the average
train_data <- merge(household_train, education_train, by=c("psu","hh","idcode"),
all=FALSE)
train_data <- merge(train_data, poverty_train, by=c("psu","hh","idcode"),
all=FALSE)
test_data <- merge(household_test, education_test, by=c("psu","hh","idcode"))
train <- train_data %>% dplyr::sample_frac(0.8)
val <- dplyr::anti_join(train_data, train, by=c("psu","hh","idcode"))
x_train <- train %>% select(-poverty,-psu,-hh,-idcode,-hhid)
y_train <- train$poverty
x_val <- val %>% select(-poverty,-psu,-hh,-idcode)
y_val <- val$poverty
x_train[, sapply(x_train, is.numeric)] <-
lapply(x_train[, sapply(x_train, is.numeric)],
function(x) ifelse(is.na(x), mean(x, na.rm = TRUE), x))
x_train <- x_train[, !apply(is.na(x_train), 2, all)]
# Variable Selection
library(glmnet)
poly_x <- as.data.frame(lapply(x_train, function(col) {
n_unique <- length(unique(col))
if(n_unique > 3) {
return(poly(col, degree = 3))
} else {
return(col) # Return original column if not enough unique values
}
}))
Log.Lambda.Seq = c(c(-15, -10, -5, -2, -1, -0.5), seq(0, 10,
by = 0.1))
Lambda.Seq = exp(Log.Lambda.Seq)
CV = cv.glmnet(as.matrix(poly_x),y_train,alpha=1,standardize=TRUE,
intercept=TRUE,lambda=Lambda.Seq)
lasso_fit <- glmnet(as.matrix(poly_x), y_train,alpha=1,lambda=CV$min)
selected_vars <- which(coef(CV,s="lambda.min")[-1] != 0)
X_selected <- poly_x[,selected_vars]
rf <- randomForest(X_selected, factor(y_train), ntree=500, importance=TRUE)
library(randomForest)
plot(lasso_fit)
plot(lasso_fit,xvar=lambda)
plot(lasso_fit,xvar="lambda")
CV
plot(CV)
plot(lasso_fit,xvar="lambda")
poly_x <- as.data.frame(lapply(x_train, function(col) {
n_unique <- length(unique(col))
if(n_unique > 2) {
return(poly(col, degree = 2))
} else {
return(col) # Return original column if not enough unique values
}
}))
Log.Lambda.Seq = c(c(-15, -10, -5, -2, -1, -0.5), seq(0, 10,
by = 0.1))
Lambda.Seq = exp(Log.Lambda.Seq)
CV = cv.glmnet(as.matrix(poly_x),y_train,alpha=1,standardize=TRUE,
intercept=TRUE,lambda=Lambda.Seq)
lasso_fit <- glmnet(as.matrix(poly_x), y_train,alpha=1,lambda=CV$min)
selected_vars <- which(coef(CV,s="lambda.min")[-1] != 0)
X_selected <- poly_x[,selected_vars]
plot(lasso_fit, xvar="lambda")
plot(CV)
CV
logit <- glm(poly_x, y_train)
logit <- glm(y_train ~ ., data=data.frame(y_train, X_selected),family="binomial")
logit <- glm(y_train ~ ., data=data.frame(y_train, X_selected),family="multinomial")
logit <- glm(y_train ~ ., data=data.frame(y_train, X_selected))
logit
rf <- randomForest(X_selected, factor(y_train), ntree=500, importance=TRUE)
View(household_train)
View(education_train)
X_selected
View(X_selected)
CV = cv.glmnet(as.matrix(poly_x),y_train,alpha=1,standardize=TRUE,
intercept=TRUE)
lasso_fit <- glmnet(as.matrix(poly_x), y_train,alpha=1,lambda=CV$min)
selected_vars <- which(coef(CV,s="lambda.min")[-1] != 0)
X_selected <- poly_x[,selected_vars]
View(X_selected)
rf <- randomForest(X_selected, factor(y_train), ntree=500, importance=TRUE)
View(X_selected)
View(train_data)
View(x_train)
# SETUP
education_train = read.csv('module_Education_train_set.csv')
household_train = read.csv('module_HouseholdInfo_train_set.csv')
poverty_train = read.csv('module_SubjectivePoverty_train_set.csv')
education_test = read.csv('module_Education_test_set.csv')
household_test = read.csv('module_HouseholdInfo_test_set.csv')
library(tidyr)
library(dplyr)
poverty_train = poverty_train %>%
separate(psu_hh_idcode, into=c("psu","hh","idcode"),sep="_",convert=TRUE) %>%
rowwise() %>%
mutate(
poverty = which(c_across(starts_with("subjective_poverty_")) == 1)
) %>%
ungroup() %>%
select(psu,hh,idcode,poverty)
colnames(education_train) <- gsub("^q", "Q", colnames(education_train))
### What strategies should we use?
### 1. Lasso on each training set individually -> combine variates -> regress
### 2. Lasso on combined training set -> regress
### 3. How do we regress on variates? Elastic net, ridge regression, logit,
### least absolute error, robust ridge, select best model using APSE, or APAE
### Can try using a random forest
# Setup
## We want to remove some unecessary data.
## Household data Q4 and Q5 is DOB and age, q12 is just idcode, q17 is just
## idcode
## In education data, if q3 is NO (2) then q4 should be 0, not the average
train_data <- merge(household_train, education_train, by=c("psu","hh","idcode"),
all=FALSE)
train_data <- merge(train_data, poverty_train, by=c("psu","hh","idcode"),
all=FALSE)
test_data <- merge(household_test, education_test, by=c("psu","hh","idcode"))
train_data <- sapply(train_data, function())
# SETUP
education_train = read.csv('module_Education_train_set.csv')
household_train = read.csv('module_HouseholdInfo_train_set.csv')
poverty_train = read.csv('module_SubjectivePoverty_train_set.csv')
education_test = read.csv('module_Education_test_set.csv')
household_test = read.csv('module_HouseholdInfo_test_set.csv')
library(tidyr)
library(dplyr)
poverty_train = poverty_train %>%
separate(psu_hh_idcode, into=c("psu","hh","idcode"),sep="_",convert=TRUE) %>%
rowwise() %>%
mutate(
poverty = which(c_across(starts_with("subjective_poverty_")) == 1)
) %>%
ungroup() %>%
select(psu,hh,idcode,poverty)
colnames(education_train) <- gsub("^q", "Q", colnames(education_train))
### What strategies should we use?
### 1. Lasso on each training set individually -> combine variates -> regress
### 2. Lasso on combined training set -> regress
### 3. How do we regress on variates? Elastic net, ridge regression, logit,
### least absolute error, robust ridge, select best model using APSE, or APAE
### Can try using a random forest
# Setup
## We want to remove some unecessary data.
## Household data Q4 and Q5 is DOB and age, q12 is just idcode, q17 is just
## idcode
## In education data, if q3 is NO (2) then q4 should be 0, not the average
train_data <- merge(household_train, education_train, by=c("psu","hh","idcode"),
all=FALSE)
train_data <- merge(train_data, poverty_train, by=c("psu","hh","idcode"),
all=FALSE)
test_data <- merge(household_test, education_test, by=c("psu","hh","idcode"))
cols_to_replace <- c("Q4", "Q5", "Q6", "Q7", "Q8")
train_data[cols_to_replace] <- lapply(train_data[cols_to_replace], function(x) {
x[is.na(x)] <- 0
return(x)
})
# SETUP
education_train = read.csv('module_Education_train_set.csv')
household_train = read.csv('module_HouseholdInfo_train_set.csv')
poverty_train = read.csv('module_SubjectivePoverty_train_set.csv')
education_test = read.csv('module_Education_test_set.csv')
household_test = read.csv('module_HouseholdInfo_test_set.csv')
library(tidyr)
library(dplyr)
poverty_train = poverty_train %>%
separate(psu_hh_idcode, into=c("psu","hh","idcode"),sep="_",convert=TRUE) %>%
rowwise() %>%
mutate(
poverty = which(c_across(starts_with("subjective_poverty_")) == 1)
) %>%
ungroup() %>%
select(psu,hh,idcode,poverty)
colnames(education_train) <- gsub("^q", "Q", colnames(education_train))
### What strategies should we use?
### 1. Lasso on each training set individually -> combine variates -> regress
### 2. Lasso on combined training set -> regress
### 3. How do we regress on variates? Elastic net, ridge regression, logit,
### least absolute error, robust ridge, select best model using APSE, or APAE
### Can try using a random forest
# Setup
## We want to remove some unecessary data.
## Household data Q4 and Q5 is DOB and age, q12 is just idcode, q17 is just
## idcode
## In education data, if q3 is NO (2) then q4 should be 0, not the average
train_data <- merge(household_train, education_train, by=c("psu","hh","idcode"),
all=FALSE)
train_data <- merge(train_data, poverty_train, by=c("psu","hh","idcode"),
all=FALSE)
test_data <- merge(household_test, education_test, by=c("psu","hh","idcode"))
cols_to_replace <- c("Q04", "Q05", "Q06", "Q07", "Q08")
train_data[cols_to_replace] <- lapply(train_data[cols_to_replace], function(x) {
x[is.na(x)] <- 0
return(x)
})
train <- train_data %>% dplyr::sample_frac(0.8)
val <- dplyr::anti_join(train_data, train, by=c("psu","hh","idcode"))
x_train <- train %>% select(-poverty,-psu,-hh,-idcode,-hhid)
y_train <- train$poverty
x_val <- val %>% select(-poverty,-psu,-hh,-idcode)
y_val <- val$poverty
x_train[, sapply(x_train, is.numeric)] <-
lapply(x_train[, sapply(x_train, is.numeric)],
function(x) ifelse(is.na(x), mean(x, na.rm = TRUE), x))
x_train <- x_train[, !apply(is.na(x_train), 2, all)]
# Variable Selection
library(glmnet)
library(randomForest)
poly_x <- as.data.frame(lapply(x_train, function(col) {
n_unique <- length(unique(col))
if(n_unique > 2) {
return(poly(col, degree = 2))
} else {
return(col) # Return original column if not enough unique values
}
}))
CV = cv.glmnet(as.matrix(poly_x),y_train,alpha=1,standardize=TRUE,
intercept=TRUE)
lasso_fit <- glmnet(as.matrix(poly_x), y_train,alpha=1,lambda=CV$min)
selected_vars <- which(coef(CV,s="lambda.min")[-1] != 0)
X_selected <- poly_x[,selected_vars]
# rf <- randomForest(X_selected, factor(y_train), ntree=500, importance=TRUE)
rf <- randomForest(X_selected, factor(y_train), ntree=500, importance=TRUE)
rf
