{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "jukit_cell_id": "GzvNSQBdCw"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/sklearn/impute/_base.py:597: UserWarning: Skipping features without any observed values: ['Q16' 'Q31' 'Q49' 'Q60']. At least one non-missing value is needed for imputation with strategy='median'.\n",
      "  warnings.warn(\n",
      "/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/sklearn/impute/_base.py:597: UserWarning: Skipping features without any observed values: ['Q16' 'Q31' 'Q49' 'Q60']. At least one non-missing value is needed for imputation with strategy='median'.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Load datasets\n",
    "education_train = pd.read_csv('module_Education_train_set.csv')\n",
    "household_train = pd.read_csv('module_HouseholdInfo_train_set.csv')\n",
    "poverty_train = pd.read_csv('module_SubjectivePoverty_train_set.csv')\n",
    "education_test = pd.read_csv('module_Education_test_set.csv')\n",
    "household_test = pd.read_csv('module_HouseholdInfo_test_set.csv')\n",
    "\n",
    "# Split `psu_hh_idcode` into `psu`, `hh`, and `idcode`\n",
    "poverty_train[['psu', 'hh', 'idcode']] = poverty_train['psu_hh_idcode'].str.split('_', expand=True).astype(int)\n",
    "\n",
    "# Merge the training datasets\n",
    "train_data = pd.merge(education_train, household_train, on=['psu', 'hh', 'idcode'], how='inner')\n",
    "train_data = pd.merge(train_data, poverty_train, on=['psu', 'hh', 'idcode'], how='inner')\n",
    "\n",
    "# Merge the test datasets\n",
    "test_data = pd.merge(education_test, household_test, on=['psu', 'hh', 'idcode'], how='inner')\n",
    "\n",
    "# Define features and target\n",
    "X = train_data.drop(columns=['psu_hh_idcode', 'subjective_poverty_1', 'subjective_poverty_2', 'subjective_poverty_3', 'subjective_poverty_4', 'subjective_poverty_5', 'subjective_poverty_6', 'subjective_poverty_7', 'subjective_poverty_8', 'subjective_poverty_9', 'subjective_poverty_10', 'psu', 'hh', 'idcode'])\n",
    "y = train_data[['subjective_poverty_1', 'subjective_poverty_2', 'subjective_poverty_3', 'subjective_poverty_4', 'subjective_poverty_5', 'subjective_poverty_6', 'subjective_poverty_7', 'subjective_poverty_8', 'subjective_poverty_9', 'subjective_poverty_10']].idxmax(axis=1).str.split('_').str[-1].astype(int) - 1\n",
    "\n",
    "# Align the test data columns with training data features\n",
    "X_test = test_data[X.columns]\n",
    "\n",
    "# Impute missing values\n",
    "imputer = SimpleImputer(strategy='median')\n",
    "X_imputed = imputer.fit_transform(X)\n",
    "X_test_imputed = imputer.transform(X_test)\n",
    "\n",
    "# Train-validation split\n",
    "X_train, X_val, y_train, y_val = train_test_split(X_imputed, y, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "jukit_cell_id": "E77t5N2jYO"
   },
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'xgboost'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[10], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mxgboost\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m XGBClassifier\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msklearn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmetrics\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m log_loss\n\u001b[1;32m      4\u001b[0m \u001b[38;5;66;03m# Initialize the XGBClassifier with GPU support\u001b[39;00m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'xgboost'"
     ]
    }
   ],
   "source": [
    "from xgboost import XGBClassifier\n",
    "from sklearn.metrics import log_loss\n",
    "\n",
    "# Initialize the XGBClassifier with GPU support\n",
    "model = XGBClassifier(\n",
    "    n_estimators=100,\n",
    "    max_depth=5,\n",
    "    learning_rate=0.1,\n",
    "    objective='multi:softprob',\n",
    "    num_class=10,\n",
    "    random_state=42,\n",
    "    tree_method='gpu_hist'  # Use GPU for training if available\n",
    ")\n",
    "\n",
    "# Fit the model on the training data\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# Predict probabilities on the validation set\n",
    "y_val_pred_proba = model.predict_proba(X_val)\n",
    "\n",
    "# Calculate Log Loss on validation set\n",
    "validation_log_loss = log_loss(y_val, y_val_pred_proba)\n",
    "print(\"Validation Log Loss on Original Data:\", validation_log_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jukit_cell_id": "TXcSKi6qVg"
   },
   "outputs": [],
   "source": [
    "from sklearn.calibration import CalibratedClassifierCV\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.metrics import log_loss\n",
    "\n",
    "# Initialize and train the XGBoost model with best parameters and GPU support\n",
    "model_optimized = XGBClassifier(\n",
    "    colsample_bytree=0.8,\n",
    "    learning_rate=0.1,\n",
    "    max_depth=4,\n",
    "    n_estimators=100,\n",
    "    subsample=0.8,\n",
    "    objective='multi:softprob',\n",
    "    num_class=10,\n",
    "    random_state=42,\n",
    "    tree_method='gpu_hist'  # Enable GPU support if available\n",
    ")\n",
    "model_optimized.fit(X_train, y_train)\n",
    "\n",
    "# Calibrate the model using isotonic regression\n",
    "calibrated_model = CalibratedClassifierCV(estimator=model_optimized, method='isotonic', cv=3)\n",
    "calibrated_model.fit(X_train, y_train)\n",
    "\n",
    "# Predict calibrated probabilities on the validation set\n",
    "y_val_pred_proba_calibrated = calibrated_model.predict_proba(X_val)\n",
    "\n",
    "# Calculate Log Loss with calibrated probabilities\n",
    "validation_log_loss_calibrated = log_loss(y_val, y_val_pred_proba_calibrated)\n",
    "print(\"Validation Log Loss after Calibration:\", validation_log_loss_calibrated)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jukit_cell_id": "kxuEvRlR40"
   },
   "outputs": [],
   "source": [
    "!pip install lightgbm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jukit_cell_id": "LwLc0TRCXv"
   },
   "outputs": [],
   "source": [
    "from xgboost import XGBClassifier\n",
    "from lightgbm import LGBMClassifier\n",
    "from sklearn.metrics import log_loss\n",
    "import numpy as np\n",
    "\n",
    "# Initialize models with GPU support\n",
    "model_xgb = XGBClassifier(\n",
    "    n_estimators=100,\n",
    "    max_depth=5,\n",
    "    learning_rate=0.1,\n",
    "    objective='multi:softprob',\n",
    "    num_class=10,\n",
    "    random_state=42,\n",
    "    tree_method='gpu_hist'  # Enable GPU for XGBoost\n",
    ")\n",
    "\n",
    "model_lgb = LGBMClassifier(\n",
    "    n_estimators=100,\n",
    "    max_depth=5,\n",
    "    learning_rate=0.1,\n",
    "    objective='multiclass',\n",
    "    num_class=10,\n",
    "    random_state=42,\n",
    "    device='gpu'  # Enable GPU for LightGBM\n",
    ")\n",
    "\n",
    "# Train both models on the training data\n",
    "model_xgb.fit(X_train, y_train)\n",
    "model_lgb.fit(X_train, y_train)\n",
    "\n",
    "# Predict probabilities on the validation set\n",
    "y_val_pred_proba_xgb = model_xgb.predict_proba(X_val)\n",
    "y_val_pred_proba_lgb = model_lgb.predict_proba(X_val)\n",
    "\n",
    "# Calculate log loss for each model\n",
    "log_loss_xgb = log_loss(y_val, y_val_pred_proba_xgb)\n",
    "log_loss_lgb = log_loss(y_val, y_val_pred_proba_lgb)\n",
    "\n",
    "print(\"Validation Log Loss for XGBoost Model:\", log_loss_xgb)\n",
    "print(\"Validation Log Loss for LightGBM Model:\", log_loss_lgb)\n",
    "\n",
    "# Ensemble the predictions by averaging and normalize the ensemble probabilities\n",
    "y_val_pred_proba_ensemble = (y_val_pred_proba_xgb + y_val_pred_proba_lgb) / 2\n",
    "y_val_pred_proba_ensemble = y_val_pred_proba_ensemble / y_val_pred_proba_ensemble.sum(axis=1, keepdims=True)\n",
    "\n",
    "# Calculate log loss for the ensemble model\n",
    "validation_log_loss_ensemble_normalized = log_loss(y_val, y_val_pred_proba_ensemble)\n",
    "print(\"Validation Log Loss with Normalized Ensemble:\", validation_log_loss_ensemble_normalized)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jukit_cell_id": "cGFczx06TZ"
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "from xgboost import XGBClassifier\n",
    "\n",
    "# Define the parameter grid\n",
    "param_grid = {\n",
    "    'n_estimators': [200, 300, 500],\n",
    "    'learning_rate': [0.01, 0.05, 0.1],\n",
    "    'max_depth': [4, 6, 8]\n",
    "}\n",
    "\n",
    "# Initialize GridSearchCV with XGBoost Classifier, setting `tree_method` to 'gpu_hist' for GPU acceleration\n",
    "grid_search = GridSearchCV(\n",
    "    XGBClassifier(random_state=42, tree_method='gpu_hist', use_label_encoder=False, eval_metric='mlogloss'),\n",
    "    param_grid,\n",
    "    cv=3,\n",
    "    scoring='neg_log_loss',\n",
    "    verbose=2,\n",
    "    n_jobs=-1  # Use all available cores for faster computation\n",
    ")\n",
    "\n",
    "# Run the grid search\n",
    "grid_search.fit(X_train, y_train)\n",
    "\n",
    "# Display best parameters and best score\n",
    "print(\"Best Parameters:\", grid_search.best_params_)\n",
    "print(\"Best Log Loss:\", -grid_search.best_score_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jukit_cell_id": "A8iQLmNvWy"
   },
   "outputs": [],
   "source": [
    "# Drop identifiers and target columns to get the list of feature names\n",
    "original_feature_names = train_data.drop(columns=['psu', 'hh', 'idcode',\n",
    "                                                  'subjective_poverty_1',\n",
    "                                                  'subjective_poverty_2',\n",
    "                                                  'subjective_poverty_3',\n",
    "                                                  'subjective_poverty_4',\n",
    "                                                  'subjective_poverty_5',\n",
    "                                                  'subjective_poverty_6',\n",
    "                                                  'subjective_poverty_7',\n",
    "                                                  'subjective_poverty_8',\n",
    "                                                  'subjective_poverty_9',\n",
    "                                                  'subjective_poverty_10']).columns.tolist()\n",
    "\n",
    "print(\"Feature names:\", original_feature_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jukit_cell_id": "5O0GdL76rq"
   },
   "outputs": [],
   "source": [
    "if 'psu_hh_idcode' in original_feature_names:\n",
    "    original_feature_names.remove('psu_hh_idcode')\n",
    "\n",
    "print(\"Updated feature names without 'psu_hh_idcode':\", original_feature_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jukit_cell_id": "bTBI4Ifc6U"
   },
   "outputs": [],
   "source": [
    "# Recreate X_train and X_test with consistent columns\n",
    "X_train = train_data[original_feature_names].copy()\n",
    "X_test = test_data[original_feature_names].copy()\n",
    "\n",
    "print(\"X_train and X_test recreated with consistent feature names.\")\n",
    "print(\"X_train shape:\", X_train.shape)\n",
    "print(\"X_test shape:\", X_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jukit_cell_id": "F49HuNLcvg"
   },
   "outputs": [],
   "source": [
    "from sklearn.impute import SimpleImputer\n",
    "\n",
    "# Impute missing values\n",
    "imputer = SimpleImputer(strategy=\"median\")\n",
    "X_train_imputed = imputer.fit_transform(X_train)\n",
    "X_test_imputed = imputer.transform(X_test)\n",
    "\n",
    "print(\"Missing values handled successfully.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jukit_cell_id": "4f2T8BcDh5"
   },
   "outputs": [],
   "source": [
    "# Drop columns that are entirely missing\n",
    "columns_to_drop = ['Q16', 'Q31', 'Q49', 'Q60']\n",
    "X_train = X_train.drop(columns=columns_to_drop)\n",
    "X_test = X_test.drop(columns=columns_to_drop)\n",
    "\n",
    "print(\"Columns with only missing values dropped.\")\n",
    "print(\"New shape of X_train:\", X_train.shape)\n",
    "print(\"New shape of X_test:\", X_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jukit_cell_id": "IL7CLdRa7J"
   },
   "outputs": [],
   "source": [
    "# Impute missing values again after dropping columns\n",
    "imputer = SimpleImputer(strategy=\"median\")\n",
    "X_train_imputed = imputer.fit_transform(X_train)\n",
    "X_test_imputed = imputer.transform(X_test)\n",
    "\n",
    "print(\"Missing values handled successfully after dropping columns with all missing values.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jukit_cell_id": "dBMKXmKtFE"
   },
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import PolynomialFeatures, StandardScaler\n",
    "\n",
    "# Generate polynomial features up to degree 2\n",
    "poly = PolynomialFeatures(degree=2, interaction_only=True, include_bias=False)\n",
    "X_train_poly = poly.fit_transform(X_train_imputed)\n",
    "\n",
    "# Scale the transformed features\n",
    "scaler = StandardScaler()\n",
    "X_train_poly_scaled = scaler.fit_transform(X_train_poly)\n",
    "\n",
    "# Apply the same transformations to the test set\n",
    "X_test_poly = poly.transform(X_test_imputed)\n",
    "X_test_poly_scaled = scaler.transform(X_test_poly)\n",
    "\n",
    "print(\"Polynomial features created and scaled for both training and test sets.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jukit_cell_id": "E1sdSASnyL"
   },
   "outputs": [],
   "source": [
    "pip install xgboost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jukit_cell_id": "iIN6zf9T20"
   },
   "outputs": [],
   "source": [
    "# Check the shapes of X_train_poly_scaled and y_train\n",
    "print(\"Shape of X_train_poly_scaled:\", X_train_poly_scaled.shape)\n",
    "print(\"Length of y_train:\", len(y_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jukit_cell_id": "bKSM1NTsIl"
   },
   "outputs": [],
   "source": [
    "# Assuming the target columns are `subjective_poverty_1` through `subjective_poverty_10`\n",
    "target_columns = [\n",
    "    \"subjective_poverty_1\", \"subjective_poverty_2\", \"subjective_poverty_3\",\n",
    "    \"subjective_poverty_4\", \"subjective_poverty_5\", \"subjective_poverty_6\",\n",
    "    \"subjective_poverty_7\", \"subjective_poverty_8\", \"subjective_poverty_9\",\n",
    "    \"subjective_poverty_10\"\n",
    "]\n",
    "\n",
    "# Extract the target class from train_data based on the maximum probability column\n",
    "y_train = train_data[target_columns].idxmax(axis=1).str.split(\"_\").str[-1].astype(int)\n",
    "\n",
    "# Ensure y_train has the same number of rows as X_train\n",
    "print(\"Recreated y_train with length:\", len(y_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jukit_cell_id": "ylRQ9sEV0b"
   },
   "outputs": [],
   "source": [
    "# Check that lengths now match\n",
    "print(\"Shape of X_train_poly_scaled:\", X_train_poly_scaled.shape)\n",
    "print(\"Length of y_train:\", len(y_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jukit_cell_id": "EJKw2my4hT"
   },
   "outputs": [],
   "source": [
    "!pip install lightgbm catboost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jukit_cell_id": "ycE2rBjXoG"
   },
   "outputs": [],
   "source": [
    "from sklearn.ensemble import StackingClassifier\n",
    "from xgboost import XGBClassifier\n",
    "from catboost import CatBoostClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "# Define base models with GPU support\n",
    "base_models = [\n",
    "    ('catboost', CatBoostClassifier(iterations=200, learning_rate=0.01, depth=4, task_type=\"GPU\", logging_level=\"Silent\")),\n",
    "    ('xgb', XGBClassifier(n_estimators=200, learning_rate=0.01, max_depth=4, use_label_encoder=False, eval_metric='mlogloss', tree_method='gpu_hist'))\n",
    "]\n",
    "\n",
    "# Define meta-model\n",
    "meta_model = LogisticRegression()\n",
    "\n",
    "# Initialize the stacking model\n",
    "stacked_model = StackingClassifier(\n",
    "    estimators=base_models,\n",
    "    final_estimator=meta_model,\n",
    "    cv=5\n",
    ")\n",
    "\n",
    "# Train the stacked model on the polynomial, scaled features\n",
    "stacked_model.fit(X_train_poly_scaled, y_train)\n",
    "\n",
    "print(\"Stacked model trained successfully with GPU support.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jukit_cell_id": "P0cIXCZzwL"
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.metrics import log_loss\n",
    "import numpy as np\n",
    "\n",
    "# Initialize Stratified K-Fold\n",
    "kfold = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
    "cv_log_loss = []\n",
    "\n",
    "# Loop through each fold\n",
    "for train_index, val_index in kfold.split(X_train_poly_scaled, y_train):\n",
    "    # Split the data into training and validation for the current fold\n",
    "    X_train_fold, X_val_fold = X_train_poly_scaled[train_index], X_train_poly_scaled[val_index]\n",
    "    y_train_fold, y_val_fold = y_train[train_index], y_train[val_index]\n",
    "\n",
    "    # Train the stacking model on the current fold\n",
    "    stacked_model.fit(X_train_fold, y_train_fold)\n",
    "\n",
    "    # Predict probabilities on the validation set\n",
    "    val_pred_probs = stacked_model.predict_proba(X_val_fold)\n",
    "\n",
    "    # Calculate log loss for the current fold and store it\n",
    "    fold_log_loss = log_loss(y_val_fold, val_pred_probs)\n",
    "    cv_log_loss.append(fold_log_loss)\n",
    "    print(f\"Log Loss for current fold: {fold_log_loss}\")\n",
    "\n",
    "# Calculate the average log loss across all folds\n",
    "average_cv_log_loss = np.mean(cv_log_loss)\n",
    "print(\"Average CV Log Loss:\", average_cv_log_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jukit_cell_id": "ZMuDP5SXcE"
   },
   "outputs": [],
   "source": [
    "!pip install optuna"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jukit_cell_id": "PtsdcFVrqp"
   },
   "outputs": [],
   "source": [
    "import optuna\n",
    "from optuna.samplers import TPESampler  # Bayesian optimization sampler\n",
    "from xgboost import XGBClassifier\n",
    "from catboost import CatBoostClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import StackingClassifier\n",
    "from sklearn.metrics import log_loss\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Split validation data for Optuna tuning\n",
    "X_train_main, X_val, y_train_main, y_val = train_test_split(X_train_poly_scaled, y_train, test_size=0.2, random_state=42)\n",
    "\n",
    "def objective(trial):\n",
    "    # Narrowed hyperparameter ranges around the previous best values\n",
    "    catboost_learning_rate = trial.suggest_loguniform(\"catboost_learning_rate\", 0.03, 0.05)\n",
    "    catboost_depth = trial.suggest_int(\"catboost_depth\", 5, 7)\n",
    "    catboost_iterations = trial.suggest_int(\"catboost_iterations\", 300, 500)\n",
    "    \n",
    "    xgb_learning_rate = trial.suggest_loguniform(\"xgb_learning_rate\", 0.03, 0.05)\n",
    "    xgb_max_depth = trial.suggest_int(\"xgb_max_depth\", 4, 6)\n",
    "    xgb_n_estimators = trial.suggest_int(\"xgb_n_estimators\", 400, 500)\n",
    "\n",
    "    # Define CatBoost model with GPU support\n",
    "    catboost = CatBoostClassifier(\n",
    "        learning_rate=catboost_learning_rate,\n",
    "        depth=catboost_depth,\n",
    "        iterations=catboost_iterations,\n",
    "        task_type=\"GPU\",\n",
    "        logging_level=\"Silent\"\n",
    "    )\n",
    "    \n",
    "    # Define XGBoost model with GPU support\n",
    "    xgb = XGBClassifier(\n",
    "        learning_rate=xgb_learning_rate,\n",
    "        max_depth=xgb_max_depth,\n",
    "        n_estimators=xgb_n_estimators,\n",
    "        eval_metric='mlogloss',\n",
    "        use_label_encoder=False,\n",
    "        tree_method='gpu_hist'\n",
    "    )\n",
    "\n",
    "    # Define the stacking model with a LogisticRegression meta-model\n",
    "    stacked_model = StackingClassifier(\n",
    "        estimators=[('catboost', catboost), ('xgb', xgb)],\n",
    "        final_estimator=LogisticRegression(),\n",
    "        cv=10  # Increasing to 10-fold CV for better generalization\n",
    "    )\n",
    "\n",
    "    # Fit model on the training fold and evaluate on validation set\n",
    "    stacked_model.fit(X_train_main, y_train_main)\n",
    "    val_pred_probs = stacked_model.predict_proba(X_val)\n",
    "    return log_loss(y_val, val_pred_probs)\n",
    "\n",
    "# Optimize with Optuna\n",
    "study = optuna.create_study(direction=\"minimize\", sampler=TPESampler())\n",
    "study.optimize(objective, n_trials=30)  # Increase trials for better tuning\n",
    "\n",
    "# Display the best parameters and log loss\n",
    "print(\"Best Parameters:\", study.best_params)\n",
    "print(\"Best Log Loss:\", study.best_value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jukit_cell_id": "KeG3VeON9V"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from catboost import CatBoostClassifier\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import StackingClassifier\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "import optuna\n",
    "from optuna.samplers import TPESampler\n",
    "from sklearn.metrics import log_loss\n",
    "\n",
    "# Load datasets\n",
    "education_train = pd.read_csv('module_Education_train_set.csv')\n",
    "household_train = pd.read_csv('module_HouseholdInfo_train_set.csv')\n",
    "poverty_train = pd.read_csv('module_SubjectivePoverty_train_set.csv')\n",
    "education_test = pd.read_csv('module_Education_test_set.csv')\n",
    "household_test = pd.read_csv('module_HouseholdInfo_test_set.csv')\n",
    "\n",
    "# Split psu_hh_idcode for merge and processing\n",
    "poverty_train[['psu', 'hh', 'idcode']] = poverty_train['psu_hh_idcode'].str.split('_', expand=True).astype(int)\n",
    "\n",
    "# Merge datasets\n",
    "train_data = pd.merge(education_train, household_train, on=['psu', 'hh', 'idcode'], how='inner')\n",
    "train_data = pd.merge(train_data, poverty_train, on=['psu', 'hh', 'idcode'], how='inner')\n",
    "test_data = pd.merge(education_test, household_test, on=['psu', 'hh', 'idcode'], how='inner')\n",
    "\n",
    "# Define features and target\n",
    "X = train_data.drop(columns=['psu_hh_idcode', 'subjective_poverty_1', 'subjective_poverty_2', 'subjective_poverty_3', 'subjective_poverty_4', 'subjective_poverty_5', 'subjective_poverty_6', 'subjective_poverty_7', 'subjective_poverty_8', 'subjective_poverty_9', 'subjective_poverty_10', 'psu', 'hh', 'idcode'])\n",
    "y = train_data[['subjective_poverty_1', 'subjective_poverty_2', 'subjective_poverty_3', 'subjective_poverty_4', 'subjective_poverty_5', 'subjective_poverty_6', 'subjective_poverty_7', 'subjective_poverty_8', 'subjective_poverty_9', 'subjective_poverty_10']].idxmax(axis=1).str.split('_').str[-1].astype(int) - 1\n",
    "\n",
    "X_test = test_data[X.columns]\n",
    "\n",
    "# Impute missing values\n",
    "imputer = SimpleImputer(strategy='median')\n",
    "X_imputed = imputer.fit_transform(X)\n",
    "X_test_imputed = imputer.transform(X_test)\n",
    "\n",
    "# Polynomial feature transformation\n",
    "poly = PolynomialFeatures(degree=2, interaction_only=True, include_bias=False)\n",
    "X_train_poly_scaled = poly.fit_transform(X_imputed)\n",
    "X_test_poly_scaled = poly.transform(X_test_imputed)\n",
    "\n",
    "# Train-validation split\n",
    "X_train_main, X_val, y_train_main, y_val = train_test_split(X_train_poly_scaled, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Optuna hyperparameter tuning function\n",
    "def objective(trial):\n",
    "    # Narrowed hyperparameter ranges around the previous best values\n",
    "    catboost_learning_rate = trial.suggest_loguniform(\"catboost_learning_rate\", 0.03, 0.05)\n",
    "    catboost_depth = trial.suggest_int(\"catboost_depth\", 5, 7)\n",
    "    catboost_iterations = trial.suggest_int(\"catboost_iterations\", 300, 500)\n",
    "\n",
    "    xgb_learning_rate = trial.suggest_loguniform(\"xgb_learning_rate\", 0.03, 0.05)\n",
    "    xgb_max_depth = trial.suggest_int(\"xgb_max_depth\", 4, 6)\n",
    "    xgb_n_estimators = trial.suggest_int(\"xgb_n_estimators\", 400, 500)\n",
    "\n",
    "    # Define CatBoost model with GPU support\n",
    "    catboost = CatBoostClassifier(\n",
    "        learning_rate=catboost_learning_rate,\n",
    "        depth=catboost_depth,\n",
    "        iterations=catboost_iterations,\n",
    "        task_type=\"GPU\",\n",
    "        logging_level=\"Silent\"\n",
    "    )\n",
    "\n",
    "    # Define XGBoost model with GPU support\n",
    "    xgb = XGBClassifier(\n",
    "        learning_rate=xgb_learning_rate,\n",
    "        max_depth=xgb_max_depth,\n",
    "        n_estimators=xgb_n_estimators,\n",
    "        eval_metric='mlogloss',\n",
    "        use_label_encoder=False,\n",
    "        tree_method='gpu_hist'\n",
    "    )\n",
    "\n",
    "    # Define the stacking model with a LogisticRegression meta-model\n",
    "    stacked_model = StackingClassifier(\n",
    "        estimators=[('catboost', catboost), ('xgb', xgb)],\n",
    "        final_estimator=LogisticRegression(),\n",
    "        cv=10  # 10-fold CV for better generalization\n",
    "    )\n",
    "\n",
    "    # Fit model on the training fold and evaluate on validation set\n",
    "    stacked_model.fit(X_train_main, y_train_main)\n",
    "    val_pred_probs = stacked_model.predict_proba(X_val)\n",
    "    return log_loss(y_val, val_pred_probs)\n",
    "\n",
    "# Optimize with Optuna\n",
    "study = optuna.create_study(direction=\"minimize\", sampler=TPESampler())\n",
    "study.optimize(objective, n_trials=30)  # Increase trials for better tuning\n",
    "\n",
    "# Display the best parameters and log loss\n",
    "print(\"Best Parameters:\", study.best_params)\n",
    "print(\"Best Log Loss:\", study.best_value)\n",
    "\n",
    "# Retrieve the best parameters from the Optuna study\n",
    "best_params = study.best_params\n",
    "\n",
    "# Initialize models with the optimized parameters\n",
    "catboost_best = CatBoostClassifier(\n",
    "    learning_rate=best_params[\"catboost_learning_rate\"],\n",
    "    depth=best_params[\"catboost_depth\"],\n",
    "    iterations=best_params[\"catboost_iterations\"],\n",
    "    task_type=\"GPU\",\n",
    "    logging_level=\"Silent\"\n",
    ")\n",
    "\n",
    "xgb_best = XGBClassifier(\n",
    "    learning_rate=best_params[\"xgb_learning_rate\"],\n",
    "    max_depth=best_params[\"xgb_max_depth\"],\n",
    "    n_estimators=best_params[\"xgb_n_estimators\"],\n",
    "    eval_metric='mlogloss',\n",
    "    use_label_encoder=False,\n",
    "    tree_method='gpu_hist'\n",
    ")\n",
    "\n",
    "# Define the final stacking model with the optimized base models\n",
    "stacked_model_best = StackingClassifier(\n",
    "    estimators=[('catboost', catboost_best), ('xgb', xgb_best)],\n",
    "    final_estimator=LogisticRegression(),\n",
    "    cv=10\n",
    ")\n",
    "\n",
    "# Train the final stacked model on the full training data\n",
    "stacked_model_best.fit(X_train_poly_scaled, y_train)\n",
    "print(\"Final stacked model trained successfully with optimized parameters.\")\n",
    "\n",
    "# Generate predictions for the test set\n",
    "test_pred_probs = stacked_model_best.predict_proba(X_test_poly_scaled)\n",
    "\n",
    "# Check if 'psu_hh_idcode' exists in test_data, create it if necessary\n",
    "if 'psu_hh_idcode' not in test_data.columns:\n",
    "    if {'psu', 'hh', 'idcode'}.issubset(test_data.columns):\n",
    "        test_data['psu_hh_idcode'] = (\n",
    "            test_data['psu'].astype(str) + '_' + \n",
    "            test_data['hh'].astype(str) + '_' + \n",
    "            test_data['idcode'].astype(str)\n",
    "        )\n",
    "    else:\n",
    "        raise ValueError(\"test_data is missing 'psu_hh_idcode' and 'psu', 'hh', 'idcode' columns.\")\n",
    "\n",
    "# Create a submission DataFrame in the required format\n",
    "target_columns = [\n",
    "    \"subjective_poverty_1\", \"subjective_poverty_2\", \"subjective_poverty_3\",\n",
    "    \"subjective_poverty_4\", \"subjective_poverty_5\", \"subjective_poverty_6\",\n",
    "    \"subjective_poverty_7\", \"subjective_poverty_8\", \"subjective_poverty_9\",\n",
    "    \"subjective_poverty_10\"\n",
    "]\n",
    "submission = pd.DataFrame(test_pred_probs, columns=target_columns)\n",
    "\n",
    "# Include the identifier column\n",
    "submission.insert(0, \"psu_hh_idcode\", test_data[\"psu_hh_idcode\"])\n",
    "# Save the submission file to the local directory\n",
    "submission.to_csv('submission.csv', index=False)\n",
    "print(\"Submission file 'submission.csv' created and saved to the local directory.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jukit_cell_id": "1gUlRT2w1W"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
