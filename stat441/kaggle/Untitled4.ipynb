{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "02c424d5-4206-4eed-8f66-9c1219b4759f",
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'module_Education_train_set.csv'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 25\u001b[0m\n\u001b[0;32m     22\u001b[0m warnings\u001b[38;5;241m.\u001b[39mfilterwarnings(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mignore\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     24\u001b[0m \u001b[38;5;66;03m# 1. Load datasets\u001b[39;00m\n\u001b[1;32m---> 25\u001b[0m education_train \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mread_csv(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmodule_Education_train_set.csv\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m     26\u001b[0m household_train \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mread_csv(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmodule_HouseholdInfo_train_set.csv\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m     27\u001b[0m poverty_train \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mread_csv(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmodule_SubjectivePoverty_train_set.csv\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python312\\site-packages\\pandas\\io\\parsers\\readers.py:1026\u001b[0m, in \u001b[0;36mread_csv\u001b[1;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, date_format, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options, dtype_backend)\u001b[0m\n\u001b[0;32m   1013\u001b[0m kwds_defaults \u001b[38;5;241m=\u001b[39m _refine_defaults_read(\n\u001b[0;32m   1014\u001b[0m     dialect,\n\u001b[0;32m   1015\u001b[0m     delimiter,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1022\u001b[0m     dtype_backend\u001b[38;5;241m=\u001b[39mdtype_backend,\n\u001b[0;32m   1023\u001b[0m )\n\u001b[0;32m   1024\u001b[0m kwds\u001b[38;5;241m.\u001b[39mupdate(kwds_defaults)\n\u001b[1;32m-> 1026\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m _read(filepath_or_buffer, kwds)\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python312\\site-packages\\pandas\\io\\parsers\\readers.py:620\u001b[0m, in \u001b[0;36m_read\u001b[1;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[0;32m    617\u001b[0m _validate_names(kwds\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnames\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m))\n\u001b[0;32m    619\u001b[0m \u001b[38;5;66;03m# Create the parser.\u001b[39;00m\n\u001b[1;32m--> 620\u001b[0m parser \u001b[38;5;241m=\u001b[39m TextFileReader(filepath_or_buffer, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwds)\n\u001b[0;32m    622\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m chunksize \u001b[38;5;129;01mor\u001b[39;00m iterator:\n\u001b[0;32m    623\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m parser\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python312\\site-packages\\pandas\\io\\parsers\\readers.py:1620\u001b[0m, in \u001b[0;36mTextFileReader.__init__\u001b[1;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[0;32m   1617\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptions[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhas_index_names\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m kwds[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhas_index_names\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[0;32m   1619\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles: IOHandles \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m-> 1620\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_engine \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_make_engine(f, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mengine)\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python312\\site-packages\\pandas\\io\\parsers\\readers.py:1880\u001b[0m, in \u001b[0;36mTextFileReader._make_engine\u001b[1;34m(self, f, engine)\u001b[0m\n\u001b[0;32m   1878\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m mode:\n\u001b[0;32m   1879\u001b[0m         mode \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m-> 1880\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles \u001b[38;5;241m=\u001b[39m get_handle(\n\u001b[0;32m   1881\u001b[0m     f,\n\u001b[0;32m   1882\u001b[0m     mode,\n\u001b[0;32m   1883\u001b[0m     encoding\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptions\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mencoding\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m),\n\u001b[0;32m   1884\u001b[0m     compression\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptions\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcompression\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m),\n\u001b[0;32m   1885\u001b[0m     memory_map\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptions\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmemory_map\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mFalse\u001b[39;00m),\n\u001b[0;32m   1886\u001b[0m     is_text\u001b[38;5;241m=\u001b[39mis_text,\n\u001b[0;32m   1887\u001b[0m     errors\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptions\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mencoding_errors\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstrict\u001b[39m\u001b[38;5;124m\"\u001b[39m),\n\u001b[0;32m   1888\u001b[0m     storage_options\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptions\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstorage_options\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m),\n\u001b[0;32m   1889\u001b[0m )\n\u001b[0;32m   1890\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1891\u001b[0m f \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles\u001b[38;5;241m.\u001b[39mhandle\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python312\\site-packages\\pandas\\io\\common.py:873\u001b[0m, in \u001b[0;36mget_handle\u001b[1;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[0;32m    868\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(handle, \u001b[38;5;28mstr\u001b[39m):\n\u001b[0;32m    869\u001b[0m     \u001b[38;5;66;03m# Check whether the filename is to be opened in binary mode.\u001b[39;00m\n\u001b[0;32m    870\u001b[0m     \u001b[38;5;66;03m# Binary mode does not support 'encoding' and 'newline'.\u001b[39;00m\n\u001b[0;32m    871\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m ioargs\u001b[38;5;241m.\u001b[39mencoding \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m ioargs\u001b[38;5;241m.\u001b[39mmode:\n\u001b[0;32m    872\u001b[0m         \u001b[38;5;66;03m# Encoding\u001b[39;00m\n\u001b[1;32m--> 873\u001b[0m         handle \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mopen\u001b[39m(\n\u001b[0;32m    874\u001b[0m             handle,\n\u001b[0;32m    875\u001b[0m             ioargs\u001b[38;5;241m.\u001b[39mmode,\n\u001b[0;32m    876\u001b[0m             encoding\u001b[38;5;241m=\u001b[39mioargs\u001b[38;5;241m.\u001b[39mencoding,\n\u001b[0;32m    877\u001b[0m             errors\u001b[38;5;241m=\u001b[39merrors,\n\u001b[0;32m    878\u001b[0m             newline\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m    879\u001b[0m         )\n\u001b[0;32m    880\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    881\u001b[0m         \u001b[38;5;66;03m# Binary mode\u001b[39;00m\n\u001b[0;32m    882\u001b[0m         handle \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mopen\u001b[39m(handle, ioargs\u001b[38;5;241m.\u001b[39mmode)\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'module_Education_train_set.csv'"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.model_selection import train_test_split, StratifiedKFold\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.metrics import log_loss\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.compose import ColumnTransformer\n",
    "import optuna\n",
    "from optuna.samplers import TPESampler\n",
    "from optuna.pruners import MedianPruner\n",
    "from xgboost import XGBClassifier\n",
    "from catboost import CatBoostClassifier\n",
    "from lightgbm import LGBMClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import StackingClassifier\n",
    "import warnings\n",
    "import joblib\n",
    "\n",
    "# Suppress warnings for cleaner output\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "# 1. Load datasets\n",
    "education_train = pd.read_csv('module_Education_train_set.csv')\n",
    "household_train = pd.read_csv('module_HouseholdInfo_train_set.csv')\n",
    "poverty_train = pd.read_csv('module_SubjectivePoverty_train_set.csv')\n",
    "education_test = pd.read_csv('module_Education_test_set.csv')\n",
    "household_test = pd.read_csv('module_HouseholdInfo_test_set.csv')\n",
    "sample_submission = pd.read_csv('sample_submission.csv')\n",
    "\n",
    "# 2. Split `psu_hh_idcode` into `psu`, `hh`, and `idcode` for training\n",
    "# Check if 'psu_hh_idcode' exists in poverty_train\n",
    "if 'psu_hh_idcode' in poverty_train.columns:\n",
    "    poverty_train[['psu', 'hh', 'idcode']] = poverty_train['psu_hh_idcode'].str.split('_', expand=True).astype(int)\n",
    "else:\n",
    "    raise KeyError(\"'psu_hh_idcode' column is missing from 'module_SubjectivePoverty_train_set.csv'\")\n",
    "\n",
    "# 3. Merge the training datasets\n",
    "train_data = pd.merge(education_train, household_train, on=['psu', 'hh', 'idcode'], how='inner')\n",
    "train_data = pd.merge(train_data, poverty_train, on=['psu', 'hh', 'idcode'], how='inner')\n",
    "\n",
    "# 4. Merge the test datasets\n",
    "test_data = pd.merge(education_test, household_test, on=['psu', 'hh', 'idcode'], how='inner')\n",
    "\n",
    "# 5. Define columns to drop for training and test datasets\n",
    "# For training data: drop identifiers and target columns\n",
    "drop_columns_train = [\n",
    "    'psu_hh_idcode',\n",
    "    'subjective_poverty_1', 'subjective_poverty_2', 'subjective_poverty_3', \n",
    "    'subjective_poverty_4', 'subjective_poverty_5', 'subjective_poverty_6', \n",
    "    'subjective_poverty_7', 'subjective_poverty_8', 'subjective_poverty_9', \n",
    "    'subjective_poverty_10', 'psu', 'hh', 'idcode'\n",
    "]\n",
    "\n",
    "# For test data: ensure 'psu_hh_idcode' exists, then drop only identifier columns\n",
    "if 'psu_hh_idcode' not in test_data.columns:\n",
    "    if {'psu', 'hh', 'idcode'}.issubset(test_data.columns):\n",
    "        test_data['psu_hh_idcode'] = (\n",
    "            test_data['psu'].astype(str) + '_' + \n",
    "            test_data['hh'].astype(str) + '_' + \n",
    "            test_data['idcode'].astype(str)\n",
    "        )\n",
    "        print(\"'psu_hh_idcode' column was missing in test_data and has been created.\")\n",
    "    else:\n",
    "        raise KeyError(\"test_data must contain 'psu', 'hh', and 'idcode' columns to create 'psu_hh_idcode'.\")\n",
    "\n",
    "drop_columns_test = [\n",
    "    'psu', 'hh', 'idcode'  # Do NOT drop 'psu_hh_idcode' from test_data\n",
    "]\n",
    "\n",
    "# 6. Define features and target for training data\n",
    "X = train_data.drop(columns=drop_columns_train)\n",
    "# Extract the target variable\n",
    "y = train_data[['subjective_poverty_1', 'subjective_poverty_2', 'subjective_poverty_3', \n",
    "               'subjective_poverty_4', 'subjective_poverty_5', 'subjective_poverty_6', \n",
    "               'subjective_poverty_7', 'subjective_poverty_8', 'subjective_poverty_9', \n",
    "               'subjective_poverty_10']].idxmax(axis=1).str.split('_').str[-1].astype(int) - 1\n",
    "\n",
    "# 7. Define features for test data\n",
    "X_test = test_data.drop(columns=drop_columns_test)\n",
    "\n",
    "# 8. Identify numerical and categorical columns\n",
    "numerical_cols = X.select_dtypes(include=['int64', 'float64']).columns.tolist()\n",
    "categorical_cols = X.select_dtypes(include=['object', 'category']).columns.tolist()\n",
    "\n",
    "# 9. Define preprocessing for numerical and categorical data\n",
    "# Numerical features: impute missing values with median\n",
    "numerical_transformer = Pipeline(steps=[\n",
    "    ('imputer', SimpleImputer(strategy='median'))\n",
    "])\n",
    "\n",
    "# Categorical features: impute missing values with most frequent and then One-Hot Encode\n",
    "categorical_transformer = Pipeline(steps=[\n",
    "    ('imputer', SimpleImputer(strategy='most_frequent')),\n",
    "    ('onehot', OneHotEncoder(handle_unknown='ignore'))\n",
    "])\n",
    "\n",
    "# Combine preprocessing steps\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('num', numerical_transformer, numerical_cols),\n",
    "        ('cat', categorical_transformer, categorical_cols)\n",
    "    ]\n",
    ")\n",
    "\n",
    "# 10. Apply preprocessing to the data\n",
    "X_preprocessed = preprocessor.fit_transform(X)\n",
    "X_test_preprocessed = preprocessor.transform(X_test)\n",
    "\n",
    "# 11. Encode target variable\n",
    "# Since the target is multiclass (0-9), ensure it's properly encoded\n",
    "# No need for LabelEncoder as y is already integer\n",
    "\n",
    "# 12. Split data into training and validation sets using Stratified Split\n",
    "X_train_main, X_val, y_train_main, y_val = train_test_split(\n",
    "    X_preprocessed, y, \n",
    "    test_size=0.2, \n",
    "    random_state=42, \n",
    "    stratify=y\n",
    ")\n",
    "\n",
    "print(\"Data preprocessing and splitting complete.\")\n",
    "\n",
    "# 13. Define the Optuna optimization function\n",
    "def objective(trial):\n",
    "    # Define hyperparameter ranges for each model\n",
    "    catboost_params = {\n",
    "        'learning_rate': trial.suggest_loguniform(\"catboost_learning_rate\", 0.01, 0.1),\n",
    "        'depth': trial.suggest_int(\"catboost_depth\", 4, 10),\n",
    "        'iterations': trial.suggest_int(\"catboost_iterations\", 100, 1000),\n",
    "        'l2_leaf_reg': trial.suggest_float(\"catboost_l2_leaf_reg\", 1e-3, 10.0, log=True),\n",
    "        'random_state': 42,\n",
    "        'verbose': False,\n",
    "        'task_type': 'GPU',        # Ensure GPU usage if available\n",
    "        'thread_count': 1          # Limit threads to prevent GPU conflicts\n",
    "    }\n",
    "    \n",
    "    xgb_params = {\n",
    "        'learning_rate': trial.suggest_loguniform(\"xgb_learning_rate\", 0.01, 0.1),\n",
    "        'max_depth': trial.suggest_int(\"xgb_max_depth\", 3, 12),\n",
    "        'n_estimators': trial.suggest_int(\"xgb_n_estimators\", 100, 1000),\n",
    "        'subsample': trial.suggest_float(\"xgb_subsample\", 0.5, 1.0),\n",
    "        'colsample_bytree': trial.suggest_float(\"xgb_colsample_bytree\", 0.5, 1.0),\n",
    "        'gamma': trial.suggest_float(\"xgb_gamma\", 0, 5.0),\n",
    "        'random_state': 42,\n",
    "        'use_label_encoder': False,\n",
    "        'eval_metric': 'mlogloss',\n",
    "        'tree_method': 'gpu_hist',  # Ensure GPU usage if available\n",
    "        'n_jobs': 1                 # Limit threads to prevent GPU conflicts\n",
    "    }\n",
    "    \n",
    "    lgb_params = {\n",
    "        'learning_rate': trial.suggest_loguniform(\"lgb_learning_rate\", 0.01, 0.1),\n",
    "        'max_depth': trial.suggest_int(\"lgb_max_depth\", 3, 12),\n",
    "        'num_leaves': trial.suggest_int(\"lgb_num_leaves\", 20, 150),\n",
    "        'bagging_fraction': trial.suggest_float(\"lgb_bagging_fraction\", 0.5, 1.0),\n",
    "        'feature_fraction': trial.suggest_float(\"lgb_feature_fraction\", 0.5, 1.0),\n",
    "        'n_estimators': trial.suggest_int(\"lgb_n_estimators\", 100, 1000),\n",
    "        'random_state': 42,\n",
    "        'n_jobs': 1  # Limit threads to prevent potential conflicts\n",
    "    }\n",
    "    \n",
    "    # Initialize base models with current hyperparameters\n",
    "    catboost = CatBoostClassifier(**catboost_params)\n",
    "    xgb = XGBClassifier(**xgb_params)\n",
    "    lgb = LGBMClassifier(**lgb_params)\n",
    "    \n",
    "    # Define the stacking model\n",
    "    stacking_model = StackingClassifier(\n",
    "        estimators=[\n",
    "            ('catboost', catboost),\n",
    "            ('xgb', xgb),\n",
    "            ('lgb', lgb)\n",
    "        ],\n",
    "        final_estimator=LogisticRegression(max_iter=1000, random_state=42),\n",
    "        passthrough=False,\n",
    "        cv=3,\n",
    "        n_jobs=1  # Limit to 1 to prevent GPU conflicts\n",
    "    )\n",
    "    \n",
    "    # Define cross-validation strategy\n",
    "    skf = StratifiedKFold(n_splits=3, shuffle=True, random_state=42)\n",
    "    \n",
    "    # Perform cross-validation and calculate average log loss\n",
    "    cv_log_loss = []\n",
    "    for train_idx, valid_idx in skf.split(X_train_main, y_train_main):\n",
    "        X_train_cv, X_valid_cv = X_train_main[train_idx], X_train_main[valid_idx]\n",
    "        y_train_cv, y_valid_cv = y_train_main.iloc[train_idx], y_train_main.iloc[valid_idx]\n",
    "        \n",
    "        stacking_model.fit(X_train_cv, y_train_cv)\n",
    "        y_pred_probs = stacking_model.predict_proba(X_valid_cv)\n",
    "        loss = log_loss(y_valid_cv, y_pred_probs)\n",
    "        cv_log_loss.append(loss)\n",
    "    \n",
    "    avg_log_loss = np.mean(cv_log_loss)\n",
    "    return avg_log_loss\n",
    "\n",
    "# 14. Set up Optuna study\n",
    "study = optuna.create_study(\n",
    "    direction=\"minimize\",\n",
    "    sampler=TPESampler(),\n",
    "    pruner=MedianPruner(n_startup_trials=5, n_warmup_steps=30)\n",
    ")\n",
    "\n",
    "# 15. Optimize the study\n",
    "study.optimize(objective, n_trials=50, timeout=3600)  # Adjust timeout as needed\n",
    "\n",
    "# 16. Display the best parameters and log loss\n",
    "print(\"Best Parameters:\", study.best_params)\n",
    "print(\"Best Log Loss:\", study.best_value)\n",
    "\n",
    "# 17. Train final models with optimized hyperparameters\n",
    "# Extract the best parameters\n",
    "best_params = study.best_params\n",
    "\n",
    "# Initialize models with the optimized parameters\n",
    "catboost_final = CatBoostClassifier(\n",
    "    learning_rate=best_params.get(\"catboost_learning_rate\", 0.03),\n",
    "    depth=best_params.get(\"catboost_depth\", 6),\n",
    "    iterations=best_params.get(\"catboost_iterations\", 500),\n",
    "    l2_leaf_reg=best_params.get(\"catboost_l2_leaf_reg\", 3),\n",
    "    random_state=42,\n",
    "    verbose=False,\n",
    "    task_type='GPU',        # Ensure GPU usage if available\n",
    "    thread_count=1          # Limit threads to prevent GPU conflicts\n",
    ")\n",
    "\n",
    "xgb_final = XGBClassifier(\n",
    "    learning_rate=best_params.get(\"xgb_learning_rate\", 0.03),\n",
    "    max_depth=best_params.get(\"xgb_max_depth\", 6),\n",
    "    n_estimators=best_params.get(\"xgb_n_estimators\", 500),\n",
    "    subsample=best_params.get(\"xgb_subsample\", 0.8),\n",
    "    colsample_bytree=best_params.get(\"xgb_colsample_bytree\", 0.8),\n",
    "    gamma=best_params.get(\"xgb_gamma\", 1.0),\n",
    "    random_state=42,\n",
    "    use_label_encoder=False,\n",
    "    eval_metric='mlogloss',\n",
    "    tree_method='gpu_hist',  # Ensure GPU usage if available\n",
    "    n_jobs=1                 # Limit threads to prevent GPU conflicts\n",
    ")\n",
    "\n",
    "lgb_final = LGBMClassifier(\n",
    "    learning_rate=best_params.get(\"lgb_learning_rate\", 0.05),\n",
    "    max_depth=best_params.get(\"lgb_max_depth\", 7),\n",
    "    num_leaves=best_params.get(\"lgb_num_leaves\", 31),\n",
    "    bagging_fraction=best_params.get(\"lgb_bagging_fraction\", 0.8),\n",
    "    feature_fraction=best_params.get(\"lgb_feature_fraction\", 0.8),\n",
    "    n_estimators=best_params.get(\"lgb_n_estimators\", 500),\n",
    "    random_state=42,\n",
    "    n_jobs=1  # Limit threads to prevent potential conflicts\n",
    ")\n",
    "\n",
    "# Define the final stacking model\n",
    "final_stacking_model = StackingClassifier(\n",
    "    estimators=[\n",
    "        ('catboost', catboost_final),\n",
    "        ('xgb', xgb_final),\n",
    "        ('lgb', lgb_final)\n",
    "    ],\n",
    "    final_estimator=LogisticRegression(max_iter=1000, random_state=42),\n",
    "    passthrough=False,\n",
    "    cv=5,\n",
    "    n_jobs=1  # Limit to 1 to prevent GPU conflicts\n",
    ")\n",
    "\n",
    "# 18. Train the final stacked model on the entire training data\n",
    "final_stacking_model.fit(X_train_main, y_train_main)\n",
    "print(\"Final stacked model trained successfully with optimized parameters.\")\n",
    "\n",
    "# 19. Evaluate on the validation set\n",
    "y_val_pred_probs = final_stacking_model.predict_proba(X_val)\n",
    "validation_log_loss = log_loss(y_val, y_val_pred_probs)\n",
    "print(f\"Validation Log Loss: {validation_log_loss}\")\n",
    "\n",
    "# 20. Generate predictions for the test set\n",
    "test_pred_probs = final_stacking_model.predict_proba(X_test_preprocessed)\n",
    "\n",
    "# 21. Ensure 'psu_hh_idcode' exists in test_data (already handled earlier)\n",
    "# Already ensured in step 5\n",
    "\n",
    "# 22. Prepare the submission DataFrame\n",
    "submission = pd.DataFrame(test_pred_probs, columns=[\n",
    "    \"subjective_poverty_1\", \"subjective_poverty_2\", \"subjective_poverty_3\",\n",
    "    \"subjective_poverty_4\", \"subjective_poverty_5\", \"subjective_poverty_6\",\n",
    "    \"subjective_poverty_7\", \"subjective_poverty_8\", \"subjective_poverty_9\",\n",
    "    \"subjective_poverty_10\"\n",
    "])\n",
    "\n",
    "# Include the identifier column\n",
    "submission.insert(0, \"psu_hh_idcode\", test_data[\"psu_hh_idcode\"])\n",
    "\n",
    "# Ensure probabilities sum to 1\n",
    "submission.iloc[:, 1:] = submission.iloc[:, 1:].div(submission.iloc[:, 1:].sum(axis=1), axis=0)\n",
    "\n",
    "# 23. Save the submission file\n",
    "submission.to_csv('submission.csv', index=False)\n",
    "print(\"Submission file created and saved as 'submission.csv'.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa75c38d-a451-4c74-8fbc-e4b69a7b1e68",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
