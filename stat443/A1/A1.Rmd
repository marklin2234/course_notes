---
title: "STAT443 Assignment 1"
author: "Mark Lin"
date: "`r Sys.Date()`"
output: pdf_document
editor_options: 
  markdown: 
    wrap: 72
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

# Q1

## a)

A loss function must be: 1. Real valued 2. Measure the error in
estimating $Y$ with $f(X)$

$L_{HC}(Y,f(X))$ is trivially a real-valued function, so the first
condition is satisfied. It measures the error in estimating $Y$ by
returning a real number that grows exponentially larger, as the
difference between $Y$ and our estimate increases, subtracted by 2.

Thus, it follows the properties of a loss function.

\newpage

## b)

As mentioned in $a$, it is an quadratic loss function. It grows
exponentially when $f(X)$ is further from $Y$.

```{r}
y <- 3
curve(exp(y - x) + exp(x - y) - 2, from=1, to=5, xlab="f(X)", ylab="L_{HC}")
```

If we plot squared error loss, we can see that they are similar in
shape.

```{r}
curve((y - x)^2, from=1,to=5,xlab="f(X)", ylab="L_{SE}")
```

However, note that the exponential growth of $L_{HC}$ is much faster
than $L_{SE}$.

\newpage

## c)

1.  

We want to show that the constant $c$ which minimizes $L_{HC}$ is
$\frac{\log E(e^Y))-\log E(e^{-y})}{2}$.

$$
\begin{aligned}
& E(e^{Y-c}+e^{c-Y}-2) \\
= & E(e^Ye^{-c}) + E(e^ce^{-Y}) -2 \\
= & E(e^Y)e^{-c}+E(e^{-Y})e^c -2
\end{aligned}
$$

Taking the derivative wrt $c$, we get

$$
\begin{aligned}
\frac{d}{dc}E(e^Y)e^{-c}+E(e^{-Y})e^c -2=&E(e^{-Y})e^c - E(e^Y)e^{-c} \\
\text{setting this equation to 0, we get} \\
E(e^Y)e^{-c}=&E(e^{-Y})e^c \\
\log E(e^Y)-c= & \log E(e^{-Y})+c \\
c =& \frac{\log E(e^Y))-\log E(e^{-y})}{2}
\end{aligned}
$$

2.

Now, that we've shown unconditionally, that this is the case, we can
set $c=f(X)|X=x$ since this is also a constant.
$$
f(x)=\frac{\log E(e^Y)|X=x)-\log E(e^{-y}|X=x)}{2}
$$
3.  

Recall that an inequality between $Z:=z(X), W:=w(X)$ is defined to
be $Z\geq X$ iff $z(x)\geq w(x)\forall x$.

Using the result from point 2, which applies to any $x\in X$, we get

$$
\begin{aligned}
& E_Y(e^{Y-g(X)}+e^{Y-g(X)}-2|X=x)\geq E_Y(e^{Y-f(X)}+e^{Y-f(X)}-2|X=x) \\
& E_Y(e^{Y-g(X)}+e^{Y-g(X)}-2|X)\geq E_Y(e^{Y-f(X)}+e^{Y-f(X)}-2|X) \\
& E_X(E_Y(e^{Y-g(X)}+e^{Y-g(X)}-2|X))\geq E_X(E_Y(e^{Y-f(X)}+e^{Y-f(X)}-2|X)) \\
& E_{Y,X}(e^{Y-g(X)}+e^{Y-g(X)}-2)\geq E_{Y,X}(e^{Y-f(X)}+e^{Y-f(X)}-2)
\end{aligned}
$$

In other words, we have that $L_{HC}$ is minimized when
$f(x)=\frac{\log E(e^Y)|X)-\log E(e^{-y}|X)}{2}$

\newpage

## d)

```{r,setup=TRUE}
data <- read.csv("Q1.csv")
```

i)

```{r}
predict <- c(2,5,9)
x <- data$X
y <- data$Y

model_i <- lm(y ~ x+I(x^2))
coefs_i <- coefficients(model_i)

f <- function(beta_0, beta_1, beta_2, x) {
  return ((beta_0) + (beta_1 * x) + (beta_2 * (x^2)))
}

for (i in predict) {
  print(f(coefs_i[1], coefs_i[2], coefs_i[3], i))
}
```

So, when X=2, Y=11.29905, when X=5, Y=14.89439 and when X=9, Y=8.38563.

ii)

```{r}
EV <- function(data, target) {
  split_data <- split(data$Y, data$X)
  ev <- sapply(split_data, mean)
  
  if (target %in% names(ev)) {
    return (ev[target])
  } else {
    return(1)
  }
}

for (i in predict) {
  print(EV(data, as.character(i)))
}
```
So, when X=2, Y=11.538, when X=5, Y=15.441 and when X=9, Y=8.628.

iii)

```{r}
lhc_par <- function(params, x, y) {
  predictions <- (params[1] + (params[2]* x) + (params[3] * (x^2)))
  sum(exp(y-predictions) + exp(predictions - y) - 2)
}

theta <- optim(par=c(1,1,1), fn=lhc_par, x=x, y=y)

for (i in predict) {
  print(f(theta$par[1], theta$par[2], theta$par[3], x=i))
}
```
So, when X=2, Y=4.87, when X=5, Y=15.27 and when X=9, Y=38.47.

Perhaps `optim` is finding only a local minima,
and not the global minima using these initial values.

iv)

```{r}
EV_exp <- function(data, target) {
  split_data <- split(exp(data$Y), data$X)
  ev <- sapply(split_data, mean)
  
  if (target %in% names(ev)) {
    return (ev[target])
  } else {
    return(1)
  }
}

EV_neg_exp <- function(data, target) {
  split_data <- split(exp(-data$Y), data$X)
  ev <- sapply(split_data, mean)
  
  if (target %in% names(ev)) {
    return (ev[target])
  } else {
    return(1)
  }
}

fx <- function(data, target) {
  return ((log(EV_exp(data, as.character(target))) - log(EV_neg_exp(data,as.character(target)))) / 2)
}

for (i in predict) {
  print(fx(data, i))
}
```

So, when X=2, Y=11.57089, when X=4.87, Y=15.48 and when X=9, Y=8.655921.

\newpage

## e)

```{r}
plot(x, y,main="Plot of Y vs X",xlab="X",ylab="Y")
x_values <- seq(0, 10, by=0.5)
lse_y_values <- f(coefs_i[1], coefs_i[2], coefs_i[3], x_values)
lines(x_values, lse_y_values,col="blue")
lhc_y_values <- f(theta$par[1], theta$par[2], theta$par[3], x_values)
lines(x_values, lhc_y_values, col="red")
legend("topright", legend=c("LHC", "LSE"), col=c("red", "blue"),lwd=1)
```

\newpage

# Q2

```{r, setup=TRUE}
data <- read.csv("Q2.csv", header=FALSE)
data <- ts(data, start=2010+7/17, frequency=12)
```

## a)

```{r}
plot(data, main="Time Series Plot of Q2 Data", xlab="Year", ylab="Value")
```

From this plot, we can see it is heteroscedastic, with both seasonality
and a positive linear trend.

\newpage

## b)

The variance of the data is not constant, and is increasing as a function
of time.

```{r}
library(MASS)

tim <- time(data)
month <- as.factor(cycle(data))
model <- lm(data ~ tim + month)
boxcox.model <- boxcox(model)
optimal.lambda <- boxcox.model$x[which.max(boxcox.model$y)]

par(mfrow=c(1,2))
plot((data)^optimal.lambda, main="Time Series Plot of Transformed Q2 Data", xlab="Year", ylab="Value")
plot(data, main="Time Series Plot of Q2 Data", xlab="Year", ylab="Value")
```

\newpage

### c)

```{r}
training_end <- time(data)[length(data) -24]
training <-window(data, end=training_end)
test <- window(data, start=training_end+1/12)
plot(training, col="blue", main="Time Series Plot of Q2 Data",xlab="Year", ylab="Value")
lines(test,col="red")
legend("topleft", legend=c("training", "test"), col=c("blue", "red"), lwd=1)
```

\newpage

## d)

```{r}
MSE <- function(y, p) {
  return( (1/24) * sum((y - p)^2) )
}

lhc <- function(y, fx) {
  return ( sum(exp(y - fx) + exp(fx - y) - 2) )
}

MHC <- function(y, p) {
  return ((1/24) * lhc(y, p))
}

results <- data.frame(degree=1:5, mse=NA, lhc=NA)

for (degree in 1:5) {
  model.ortho <- lm(training ~ poly(time(training), degree), data=training)
  pred <- predict(model.ortho, newdata=data.frame(training=time(test)))
  results$mse[degree] <- MSE(test, pred)
  results$lhc[degree] <- MHC(test, pred)
}

par(mfrow=c(1,2))

plab <- seq(from=1, to=5, by=1)
plot(y=results$mse, x=plab, main="MSE vs degrees", xlab="degrees", ylab="MSE")
plot(y=results$lhc, x=plab, main="MHC vs degrees", xlab="degrees", ylab="MHC")
```

Taking a look at the graphs, it appears that MHC is converging, while MSE
is diverging from 0. For both loss functions, a 2 degree polynomial trend
minimizes the MSE. We can confirm this with R.

```{r}
which.min(results$mse)
which.min(results$lhc)
```

Thus, they both choose the same model. We now fit it on the whole data set.

```{r}
plot(data, main="Time Series Plot of Q2 Data", ylab="Value", xlab="Year")
best_model <- lm(data ~ poly(time(data), 2), data=data)

y_hat <- predict(best_model)
lines(ts(y_hat, start=2010+7/17, frequency=12), col="blue")
legend("topleft", legend=c("time series", "fitted model"), col=c("black", "blue"),lwd=1)
```
\newpage

## e)

```{r}
new_time <- ts(seq(from=end(data)[1] + 1/12, by=1/12,length.out=12), start=end(data)[1] + 1/12, frequency=12)
pred <- predict(best_model, newdata=data.frame(data=new_time), interval="prediction", level=0.95)
print(pred)

plot(data, main="Time Series Plot of Q2 Data", ylab="Value", xlab="Year")
lines(ts(y_hat, start=2010+7/17, frequency=12), col="blue")
lines(ts(pred[,"fit"],start=end(data)[1] + 1/12, frequency=12),col="red")
lines(ts(pred[,"lwr"],start=end(data)[1] + 1/12, frequency=12),col="green")
lines(ts(pred[,"upr"],start=end(data)[1] + 1/12, frequency=12),col="green")
legend("topleft", legend=c("time series", "fitted model", "prediction", "prediction interval"), col=c("black", "blue", "red", "green"),lwd=1)
```
\newpage

## f)

```{r}
par(mfrow = c(2, 2))
plot(best_model)
```

### Normality

Taking a look at the graphs above, the error terms may not fit
the normal assumption. In the Q-Q plot, terms fit along the line
near the middle, but differ drastically at the extremes, especially
at the upper quantiles. This leads me to believe our error terms
are follow a t-distribution instead.

```{r}
shapiro.test(best_model$residuals)
```

To confirm, using the shapiro-wilk normality test, we have strong 
evidence against normality of residuals.

### Constant Variance

The model also does not have constant variance. Taking a look at
the fitted vs residuals graph, we see a fanning shape as we go to
the right. Using the Flinger-Killeen test, 

```{r}
fligner.test(best_model$residuals, rep(1:4,each=39))
```
Small $p$-value tells us that our residuals are heteroscedastic.
Our residuals do not have constant variance either.

### Uncorrelatedness

```{r}
acf(best_model$residuals, main = "Sample ACF of Residuals")
```

The graph tells us residuals are correlated.

### Randomness

```{r}
library(randtests)
randtests::difference.sign.test(best_model$residuals)
randtests::runs.test(best_model$residuals,plot=TRUE)
```

We have an extremely small p-value for our runs test,
indicating evidence against randomness of the residuals.

However, we have a large p-value for our difference-sign test,
which conflicts with our runs test conclusion. Regardless,
this does not affect our conclusion.

### Conclusion

After a complete residual diagnostic, our model may not be adequate
for our data since residuals are correlated, non-normal, non-random
and heteroscedastic.

This conclusion leads us to the conclusion that our inference for
individual parameters and prediction intervals based on the model from
part (d) are not valid due to the violation of residual assumptions.
