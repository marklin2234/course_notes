# A1

## Q1

### a)

$$
\begin{aligned}
Var(S) & = Var(E(S|N)) + E(Var(S|N)) \\
& = Var(E(\sum_{i\leq N}Y_i|N)) + E(Var(\sum_{i\leq N}Y_i|N)) \\
& = Var(E(NY_1|N)) + E(Var(NY_1|N)) \\
& = Var(N)E(Y_1)^2+E(N)Var(Y_1)
\end{aligned}
$$

### b)

$$
\begin{aligned}
E(e(X)g(X)) & = E[E[e(X)g(X)|P(X=x)\neq0] + E[e(X)g(X)|P(X=x)=0]] \\
& = E[E(\frac{E(Y|X=x)}{P(X=x)}g(X)|P(X=x)\neq0)] + E[E(123456g(X)|P(X=x)=0)] \\
& = E[\frac{E(Y|X=x)}{P(X=x)}g(X)P(X=x)] + E[123456g(X)P(X=x)] \\
& = E[E(Y|X)g(X)] \\
& = LHS \\\\
RHS & = E[Yg(X)] = E[E(Yg(X)|X)] = E[E(Y|X)g(X)] = LHS
\end{aligned}
$$

Thus, $e(X)$ is a version of $E[Y|X]$.

<div style="page-break-after: always"></div>

## Q2

### a)

$$
\begin{aligned}
E[S_T] &= E[E(S_T|T)] \\
&= E\Big[E\Big(\sum_{i\leq T}Z_i|T\Big)\Big] \\
&= E[T\mu] \\
&= E(T)\mu
\end{aligned}
$$

### b)

$$
\begin{aligned}
E[S_T^2]&=E[E(S_T^2|T)] \\
&= E\Big[E\Big(\Big(\sum_{i\leq T}Z_i\Big)^2|T\Big)\Big] \\
&= E[E(\sum_{i\leq T}Z_i^2|T) + E[\sum_{i\leq T}\sum_{j\leq T,j\neq i}Z_iZ_j]] \\
&= E(T\sigma^2) + \sum_{i\leq T}\sum_{j\leq T,j\neq i}E(Z_i)E(Z_j) \\
&= E(T)\sigma^2
\end{aligned}
$$

### c)

Let $(Y_n)_{n\geq 0}$ be a submartingale. Let $k\in\N, S, T$ be stopping times and
$P(T\leq S\leq k) = 1$. Then,  
$E[Y_0]\leq E[Y_T]\leq E[Y_k]$  
$E[Y_0]\leq E[Y_T]\leq E[Y_k]$

since $P(T\leq k) = P(S\leq k) = 1$. Since $P(T\leq S)=1$ as well, then we have
$E[Y_0]\leq E[Y_T]\leq E[Y_S]\leq E[Y_k]$ by definition.

<div style="page-break-after: always"></div>

## Q3

### a)

i)

$$
\begin{aligned}
E(Y_{n+1}|F_n) &= E(Y_n+\xi_{n+1}|F_n) \\
&= E(Y_n|F_n) + E(\xi_{n+1}|F_n) \\
&= Y_n + [((n+1)^2+\frac{1}{(n+1)^2})+(-\frac{(n+1)^2}{(n+1)^2-1}(1-\frac{1}{(n+1)^2}))] \\
&= Y_n + [1 + (-\frac{(n+1)^2}{(n+1)^2-1} + \frac{1}{(n+1)^2-1})] \\
&= Y_n + 1 - \frac{(n+1)^2+1}{(n+1)^2-1} = Y_n+1-\frac{-((n+1)^2)-1)}{(n+1)^2-1} \\
&= Y_n + 1 - 1 = Y_n
\end{aligned}
$$

Thus, by definition, $Y_n$ is a martingale.

ii)

$$
\begin{aligned}
\sum_{i=1}^\infin P(\xi_i)&=\sum_{i=1}^\infin P(\xi_i=i^2) \\
&= \sum_{i=1}^\infin \frac{1}{i^2}<\infin
\end{aligned}
$$

Since this converges, by Borel=Cantelli Lemma, $P(\xi_i \text{ i.o.}) = 0$.  
Thus, $P(\xi_i=-\frac{i^2}{i^2-1}) = 1$. As $n\to\infin,-\frac{i^2}{i^2-1}=-1$.  
Thus, $Y_n\to-\infin$ with probability 1.

iii)

$Y_n$ is not a submartingale because it is not bounded in $L^1$.

### b)

Let $M_n=Y_ne^{-\sum_{i=1}^{n-1}Z_i}$. Then,

$$
\begin{aligned}
E[M_{n+1}|F_n]&=E[Y_{n+1}e^{-\sum_{i=1}^{n-1} Z_i}|F_n] \\
&= E[Y_{n+1}|F_n]e^{-\sum_{i=1}^{n-1}Z_i} \\
&= Y_ne^{Z_n}e^{-\sum_{i=1}^{n-1}Z_i} \\
&= Y_ne^{\sum_{i=1}^{n-1}Z_i} \\
&= M_n
\end{aligned}
$$

Thus, $M_n$ is a martingale that converges to $M_\infin$. Since $\sum_{k\geq 0}Z_k<\infin$,
we know $\sum_{i=1}^{n-1}Z_i\to c$, where $c$ is a finite constant.  
Rearranging our formula, we get $Y_n=\frac{M_n}{e^{-\sum_{i=1}^{n-1}Z_i}}\to\frac{M_\infin}{c}$.

Thus, $Y_n$ converges with probability 1.

### c)

$$
\begin{aligned}
E(Y_n^2)&=E[(Y_0+\sum_{i=1}^n\xi_i)^2] \\
E(Y_n^2)&=E(Y_0^2) + 2E(Y_0\sum_{i=1}^n\xi_i) + E((\sum_{i=1}^n\xi_i)^2) \\
E(Y_n^2)&=E(Y_0^2) + 2E(Y_0)\sum_{i=1}^nE(\xi_i) + (\sum_{i=1}^nE(\xi_i^2)) + \sum_{i=n+1}^\infin\sum_{j=n+1,j\neq i}^\infin E(\xi_i\xi_j) \\
E(Y_n^2)&=E(Y_0^2) + 2E(Y_0)\sum_{i=1}^nE(\xi_i) + (\sum_{i=1}^nE(\xi_i^2)) \\
&<\infin\implies E(Y_n)<\infin
\end{aligned}
$$

Where the increments are orthogonal, so $E[\xi_i\xi_j]=0$.  

The first inequality holds because both expectations in $2E(Y_0)E(\sum_{i=1}^n\xi_i)$ must
converge if their squares also converge by martingale convergence theorem.

Thus, $Y_n$ converges to $Y_\infin$ with probability 1.

$$
\begin{aligned}
E[(Y_n-Y_\infin)^2] &= E[(\sum_{i=n+1}^\infin\xi_i)^2] \\
&= \sum_{i=n+1}^\infin E(\xi_i^2) + \sum_{i=n+1}^\infin\sum_{j=n+1,j\neq i}^\infin E(\xi_i\xi_j) \\
&= \sum_{i=n+1}^\infin E(\xi_i^2)
<\infin
\end{aligned}
$$

Where the increments are orthogonal, so $E[\xi_i\xi_j]=0$.  
Thus, we have convergence in $L^2$.

<div style="page-break-after: always"></div>

## Q4

### a)

First, we prove that it is a martingale, then apply martingale convergence.

$$
\begin{aligned}
E[M_{t+1}|F_n] &= E[\sum_{s\leq t+1}s^{-1}X_s|F_n] \\
&=E[(t+1)^{-1}X_{t+1}+\sum_{s\leq t}s^{-1}X_s|F_n] \\
&= M_t + E[(t+1)^{-1}X_{t+1}|F_n] = M_t
\end{aligned}
$$

Thus, $M_t$ is a martingale. $E(M_t)=E(\sum_{s\leq t}s^{-1}X_s)$. As $t\to\infin$, this
converges. Thus, we know $M_t\to M_\infin$ with probability 1 by martingale convergence
theorem.

### b)

$$
\begin{aligned}
M_t-t^{-1}\sum_{s=1}^{t-1}M_s&=\sum_{s=1}^ts^{-1}X_s-t^{-1}\sum_{s=1}^{t-1}\sum_{k=1}^sk^{-1}X_k \\
&=\sum_{s=1}^ts^{-1}X_s-t^{-1}\sum_{k=1}^{t-1}\sum_{s=k}^{t-1}k^{-1}X_k \\
&=\sum_{s=1}^ts^{-1}X_s-t^{-1}\sum_{k=1}^{t-1}k^{-1}(t-k)X_k \\
&=\sum_{s=1}^ts^{-1}X_s-t^{-1}\sum_{s=1}^t\frac{(t-s)}{s}X_s \\
&=\sum_{s=1}^t(s^{-1}-t^{-1}\frac{(t-s)}{s})X_s \\
&=\frac{S_t}{t}
\end{aligned}
$$

### c)

By the SLLN, $\frac{S_t}{t}\stackrel{\text{a.s}}{\to}0$ as $t\to\infin$.

### d)

To use the dominating convergence theorem, we must prove:

1. Pointwise Convergence

Since $Y_t=X_t\bold{1}_{|X_t|\leq t}$, as $t\to\infin$, we have $\bold{1}_{|X_t|\leq t}=1$.
Thus, $Y_t\to X_t$ as $t\to\infin$. Since $X_t$ is IID, we have $Y_t\to X_1$ as $t\to\infin$.

2. Dominating Function

$$
\begin{aligned}
|Y_t|&=|X_t|\bold{1}_{|X_t|\leq t} \\
&\leq |X_t|
\end{aligned}
$$

Let $g(X_t)=|X_t|$ be our dominating function. Then, we have $|Y_t|<g(X)$ and $E[|X|]<\infin$ (assumption) so it is integrable, then by dominating convergence theorem, $E[Y_t]\to E[X_1]$.

### e)

$$
E[|X_1|]=\sum_{t=1}^\infin P(|X_1|>t)=\sum_{t=1}^\infin P(|X_t|>t)<\infin
$$

By Borel-Cantelli Lemma, we have $P(|X_t|>t\text{ i.o.})=0$.

Using this result, and the definition of $Y_t=X_t\bold{1}_{|X_t|\leq t}$, we know that
there must be some $N<\infin$ s.t. $\bold{1}_{|X_t|\leq t}=1$.

Thus, $\exists N<\infin\;(\text{ in other words }P(N<\infin)=1)$ such that $\forall n\geq N,\; P(Y_n = X_n)$ trivially.

### f)

$$
\begin{aligned}
& \sum_{t\geq 1}\frac{Var(Y_t)}{t^2} \\
< & \sum_{t\geq 1}\frac{Var(X_t\bold{1}_{\{|X_t|\leq t\}})}{t^2} \\
=& \sum_{t\geq 1}\frac{Var(X_t\sum_{s\leq t}\bold1_{|X_t|\in(s-1, s]})}{t^2} \\
<&\sum_{t\geq1}\frac{Var(X_t)}{t^2} \\
=&\sum_{t\geq1}\frac{E[X_t^2]-E[X_t]^2}{t^2} \\
<&\sum_{t\geq1}\frac{E[|X_1|^2] - E[|X_1|]^2}{t^2} \\
<&\infin
\end{aligned}
$$

### e)

<div style="page-break-after: always"></div>

## Q5

### a)

### b)

Using the ABRACADABRA example, we can see that the expected waiting time until we see
12121 in consecutive rolls of a 6 sided die is $E[X_t] = 6^1+6^2+6^5=7818$. Since the
prefix-suffix pairs are 1, 12 and 12121.

